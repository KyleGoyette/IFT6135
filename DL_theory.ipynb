{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_theory.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "nMgtXQFS4rAX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Deep Learning assignment 1 - theory part\n",
        "\n",
        "##Giancarlo Kerg: 20109271 (matricule UdeM)"
      ]
    },
    {
      "metadata": {
        "id": "DCCyfein5148",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Question1: \n",
        "a) Assuming that the labels are all equal to either $0$ or $1$, an appropriate activation function would the sigmoid function $g(x) = \\frac{1}{1+e^{-x}}$.\n",
        "\n",
        "b) $g(x)$ could be interpreted as the probability that $x$ belongs to the class with label $1$. It should be noted that this is just a pure interpretation, not an actual probability measure.  \n",
        "\n",
        "c) $L_{CE}(f(\\pmb{x},\\pmb{\\theta}),y)= -y\\cdot\\log f(\\pmb{x},\\pmb{\\theta}) - (1-y)\\cdot \\log (1-f(\\pmb{x},\\pmb{\\theta}))$. (Note that this expresion is well-defined as the sigmoid function never actually reaches the output values of $1$ and $0$.) \n",
        "\n",
        "d) First note that $g'(x) = \\frac{e^{-x}}{(1+e^{-x})^2}=g(x)(1-g(x))$ and $\\frac{\\partial f(\\pmb{x},\\pmb{\\theta})}{\\partial a(\\pmb{x},\\pmb{\\theta})} = g'(a(\\pmb{x},\\pmb{\\theta}))=g(a(\\pmb{x},\\pmb{\\theta}))(1-g(a(\\pmb{x},\\pmb{\\theta})))= f(\\pmb{x},\\pmb{\\theta})(1-f(\\pmb{x},\\pmb{\\theta}))$. Since $y$ does not depend on $a(\\pmb{x},\\pmb{\\theta})$, we get: \n",
        "\n",
        "$$\\frac{\\partial L_{CE}(f(\\pmb{x},\\pmb{\\theta}),y)}{\\partial a(\\pmb{x},\\pmb{\\theta})} = - y \\cdot \\frac{1}{f(\\pmb{x},\\pmb{\\theta})}\\cdot\\frac{\\partial f(\\pmb{x},\\pmb{\\theta})}{\\partial a(\\pmb{x},\\pmb{\\theta})} - (1-y) \\cdot \\frac{1}{1-f(\\pmb{x},\\pmb{\\theta})}\\cdot\\frac{\\partial (1-f(\\pmb{x},\\pmb{\\theta}))}{\\partial a(\\pmb{x},\\pmb{\\theta})}\\\\ = - \\frac{y}{f(\\pmb{x},\\pmb{\\theta})} \\cdot f(\\pmb{x},\\pmb{\\theta})(1-f(\\pmb{x},\\pmb{\\theta})) + \\frac{1-y}{1-f(\\pmb{x},\\pmb{\\theta})} \\cdot f(\\pmb{x},\\pmb{\\theta})(1-f(\\pmb{x},\\pmb{\\theta})) = f(\\pmb{x},\\pmb{\\theta})-y$$\n",
        "\n",
        "e) $L_{MSE}(f(\\pmb{x},\\pmb{\\theta}),y) = \\left(f(\\pmb{x},\\pmb{\\theta})-y\\right)^2$\n",
        "\n",
        "f) $$\\frac{\\partial L_{MSE}(f(\\pmb{x},\\pmb{\\theta}),y)}{\\partial a(\\pmb{x},\\pmb{\\theta})} = 2\\cdot \\left( f(\\pmb{x},\\pmb{\\theta}) - y\\right)\\cdot \\frac{\\partial f(\\pmb{x},\\pmb{\\theta})}{\\partial a(\\pmb{x},\\pmb{\\theta})} = 2\\cdot \\left( f(\\pmb{x},\\pmb{\\theta}) - y\\right)\\cdot f(\\pmb{x},\\pmb{\\theta})(1-f(\\pmb{x},\\pmb{\\theta}))$$\n",
        "\n",
        "g) Observe that whenever our prediction $f(\\pmb{x},\\pmb{\\theta})$ is either close to $0$ or $1$, the expression $f(\\pmb{x},\\pmb{\\theta})(1-f(\\pmb{x},\\pmb{\\theta}))$ is close to $0$. Thus if our prediction $f(\\pmb{x},\\pmb{\\theta})$ is close to $0$ or $1$ while misclassifying $\\pmb{x}$, the gradient $\\frac{\\partial L_{MSE}(f(\\pmb{x},\\pmb{\\theta}),y)}{\\partial a(\\pmb{x},\\pmb{\\theta})}$ is close $0$, and therefore no correction of the weights will happen during backpropagation. $\\frac{\\partial L_{CE}(f(\\pmb{x},\\pmb{\\theta}),y)}{\\partial a(\\pmb{x},\\pmb{\\theta})}$ however will be close to $1$, and thus a strong correction of the weights will happens during backpropagation.  "
      ]
    },
    {
      "metadata": {
        "id": "eR8k4NZmoU5V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Question 2: \n",
        "\n",
        "For ease of notation let us work with coordinates $x$ and $y$ instead of $x_1$ and $x_2$. Just to clarify we denote $\\mathbf{x} = (x,y)$.\n",
        "\n",
        "Let us first try to formally write down $f$ in terms of all the coefficients we should determine: \n",
        "\n",
        "$$f(\\mathbf{x}) = H(u_1 g_1(\\mathbf{x}) + u_2 g_2(\\mathbf{x}) + u_3 g_3(\\mathbf{x}) + c)$$\n",
        "\n",
        "where $g_i(\\mathbf{x}) = H(w_{1i}x + w_{2i}y + b_i)$, for $i = 1,2,3$. \n",
        "\n",
        "By looking at the graph, we observe that the region containing the triangles can approximated by the intersection of three (closed) half-planes, say $\\pi_1$, $\\pi_2$ and $\\pi_3$, each of which is determined by a line $d_1$, $d_2$ and $d_3$ respecitvely. \n",
        "\n",
        "\n",
        "A first separating line $d_1$ we can spot goes through $(0,3)$ and $(3,0)$, and thus has equation $d_1 \\equiv y= -x+3$, and thus the half-plan containing the triangles satisfies $$\\pi_1 \\equiv y \\leq -x+3 \\Leftrightarrow -x -y +3  \\geq 0 \\Leftrightarrow H(-x-y+3) = 1 $$\n",
        "\n",
        "A second separating line $d_2$ we can spot goes through $(3,0)$ and $(0,-5)$, and thus has equation $d_2 \\equiv y= \\frac{5}{3}x-5$, and thus the half-plan containing the triangles satisfies $$\\pi_2 \\equiv y \\geq \\frac{5}{3} x-5 \\Leftrightarrow -5x +3y +15 \\geq 0 \\Leftrightarrow H(-5x +3y +15) = 1 $$\n",
        "\n",
        "A third separating line $d_3$ we can spot goes through $(0,-5)$ and $(-4,0)$, and thus has equation $d_3 \\equiv y= -\\frac{5}{4}x-5$, and thus the half-plan containing the triangles satisfies $$\\pi_3 \\equiv y \\geq -\\frac{5}{4} x-5 \\Leftrightarrow 5x +4y +20 \\geq 0 \\Leftrightarrow H(5x +4y +20) = 1 $$\n",
        "\n",
        "The triangles lie in the intersection of all three half-planes so we want all three conditions to be verified, which is equivalent to saying $$H(-x-y+3) + H(-5x +3y +15) + H(5x +4y +20)\\geq 3 $$ which in turn can be rewritten as $$ H(H(-x-y+3) + H(-5x +3y +15) + H(5x +4y +20) -3 )=1$$ \n",
        "\n",
        "which should correspond exactly to the above expression of our function $f$, thus we can take $$c = -3$$\n",
        "$$u_1 = u_2 = u_3 =1$$\n",
        "$$w_{11} = -1, w_{21} = -1, b_1= 3$$\n",
        "$$w_{12} = -5, w_{22} = 3,  b_2 = 15$$\n",
        "$$w_{13} =5 , w_{23} = 4, b_3 = 20$$"
      ]
    },
    {
      "metadata": {
        "id": "veWbXx0FqGLd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Question 3: \n",
        "\n",
        "a) For notational simplicity, denote $g(x) = \\textrm{ReLU}(x)$ for all $x \\in \\mathbb{R}$.\n",
        "\n",
        "*   if $x>0$ and for sufficiently small $\\epsilon$, we have $\\frac{g(x+\\epsilon)-g(x)}{\\epsilon} = \\frac{(x+\\epsilon)-x}{\\epsilon} = 1$, and thus $$g'(x) = \\lim_{\\epsilon \\rightarrow 0} \\frac{g(x+\\epsilon)-g(x)}{\\epsilon} = \\lim_{\\epsilon \\rightarrow 0} 1 = 1$$\n",
        "\n",
        "*   if $x<0$ and for sufficiently small $\\epsilon$, we have $\\frac{g(x+\\epsilon)-g(x)}{\\epsilon} = \\frac{0-0}{\\epsilon} = 0$, and thus $$g'(x) = \\lim_{\\epsilon \\rightarrow 0} \\frac{g(x+\\epsilon)-g(x)}{\\epsilon} = \\lim_{\\epsilon \\rightarrow 0} 0 = 0$$\n",
        "\n",
        "*   if $x=0$ and for $\\epsilon>0$, we have $\\frac{g(x+\\epsilon)-g(x)}{\\epsilon} = \\frac{g(\\epsilon)-g(0)}{\\epsilon} = \\frac{\\epsilon-0}{\\epsilon} = 1$, and thus \n",
        "$$g_R'(0) = \\lim_{\\epsilon \\rightarrow 0^+} \\frac{g(\\epsilon)-g(0)}{\\epsilon} = \\lim_{\\epsilon \\rightarrow 0^+} 1 = 1$$\n",
        "\n",
        "*   if $x=0$ and for $\\epsilon<0$, we have $\\frac{g(x+\\epsilon)-g(x)}{\\epsilon} = \\frac{g(\\epsilon)-g(0)}{\\epsilon} = \\frac{0-0}{\\epsilon} = 0$, and thus \n",
        "$$g_L'(0) = \\lim_{\\epsilon \\rightarrow 0^-} \\frac{g(\\epsilon)-g(0)}{\\epsilon} = \\lim_{\\epsilon \\rightarrow 0^-} 0 = 0$$\n",
        "\n",
        "Since $g_R'(0) \\neq g_L'(0)$, $g$ is not differentiable at $0$. However we have shown that $g$ is differentiable on $\\mathbb{R}_0$, and that for all $x \\in \\mathbb{R}_0$, we have $g'(x) = H(x)$. \n",
        "\n",
        "---\n",
        "\n",
        "b) First observe that $H(x) + H(-x) = 1$ for all $x \\in \\mathbb{R}$, thus we can give the following two alternative definitions $$g(x) = x\\cdot H(x)$$ and $$g(x) = x\\cdot \\left(1-H(-x)\\right)$$\n",
        "\n",
        "---\n",
        "\n",
        "c) Observe that for all $x>0$, we have \n",
        "\n",
        "$$\\lim_{k\\rightarrow \\infty} \\sigma(kx) = \\lim_{k\\rightarrow \\infty} \\frac{1}{1+e^{-kx}} = \\frac{1}{1+0} = 1 = H(x)$$\n",
        "and for all $x<0$, we have \n",
        "\n",
        "$$\\lim_{k\\rightarrow \\infty} \\sigma(kx) = \\lim_{k\\rightarrow \\infty} \\frac{1}{1+e^{-kx}} = 0 = H(x)$$\n",
        "\n",
        "finally for $x=0$, we have \n",
        "$$\\lim_{k\\rightarrow \\infty} \\sigma(kx) = \\lim_{k\\rightarrow \\infty} \\sigma(0) = \\sigma(0) = \\frac{1}{2} = H(0)$$\n",
        "\n",
        "thus we have proven that for all $x \\in \\mathbb{R}$, $$H(x) = \\lim_{k\\rightarrow \\infty} \\sigma(kx)$$\n",
        "\n",
        "---\n",
        "\n",
        "d) Technically $\\delta$ is not a function but can be defined either as a distribution or a probability measure. When opting for the former, one should first rigorously define the space of distributions, the notion of convergence in this space as well as the distributional derivative and then show that the distributional derivative of $H$ equals $\\delta$. However, by the way the question is asked as well as the reference mentioned therein, I assume that one is perfectly happy with an informal defintion of $\\delta$, which is:\n",
        "\n",
        "*  (C1)  $\\delta(x) = 0$ for all $x \\neq 0$\n",
        "*  (C2)  $\\delta(0)=+\\infty $\n",
        "*  (C3)  $\\int \\delta(x) dx = 1$ \n",
        "\n",
        "We will thus prove that the derivative of $H$ satisfies these three conditions.\n",
        "\n",
        "*   if $x>0$ and for sufficiently small $\\epsilon$, we have $\\frac{H(x+\\epsilon)-H(x)}{\\epsilon} = \\frac{1-1}{\\epsilon} = 0$, and thus \n",
        "\n",
        "$$H'(x) = \\lim_{\\epsilon \\rightarrow 0} \\frac{H(x+\\epsilon)-H(x)}{\\epsilon} = \\lim_{\\epsilon \\rightarrow 0} 0 = 0$$\n",
        "\n",
        "*   if $x<0$ and for sufficiently small $\\epsilon$, we have $\\frac{H(x+\\epsilon)-H(x)}{\\epsilon} = \\frac{0-0}{\\epsilon} = 0$, and thus \n",
        "\n",
        "$$H'(x) = \\lim_{\\epsilon \\rightarrow 0} \\frac{H(x+\\epsilon)-H(x)}{\\epsilon} = \\lim_{\\epsilon \\rightarrow 0} 0 = 0$$\n",
        "\n",
        "*   if $x=0$ and for $\\epsilon>0$, we have $\\frac{H(x+\\epsilon)-H(x)}{\\epsilon} = \\frac{H(\\epsilon)-H(0)}{\\epsilon} = \\frac{1-\\frac{1}{2}}{\\epsilon} = \\frac{1}{2\\epsilon} $, and thus \n",
        "\n",
        "$$H_R'(0) = \\lim_{\\epsilon \\rightarrow 0^+} \\frac{H(\\epsilon)-H(0)}{\\epsilon} = \\lim_{\\epsilon \\rightarrow 0^+} \\frac{1}{2\\epsilon} = +\\infty$$\n",
        "\n",
        "*   if $x=0$ and for $\\epsilon<0$, we have $\\frac{H(x+\\epsilon)-H(x)}{\\epsilon} = \\frac{H(\\epsilon)-H(0)}{\\epsilon} = \\frac{0-\\frac{1}{2}}{\\epsilon} = -\\frac{1}{2\\epsilon}$, and thus \n",
        "\n",
        "$$H_L'(0) = \\lim_{\\epsilon \\rightarrow 0^-} \\frac{H(\\epsilon)-H(0)}{\\epsilon} = \\lim_{\\epsilon \\rightarrow 0^-} -\\frac{1}{2\\epsilon} = + \\infty$$\n",
        "\n",
        "Since $H_R'(0) = H_L'(0)= +\\infty$, we could informally state that $H'(0) = + \\infty$. Thus we have verified that $H'(x)$ verifies the first two conditions (C1) and (C2).\n",
        "\n",
        "Let us finally prove that $H'(x)$ verifies condition (C3), namely $\\int H'(x) dx = 1 $.\n",
        "\n",
        "As we want to use the definition of the derviative, in the hope of exchanging the integral and the limit in our calculations, we can't proceed as above, where we first $x$ and then let $\\epsilon$ vary to fit our needs. Here we need to do the exact opposite, namely fix $\\epsilon$ first and then vary $x$, because we want to calculate the integral before we calculate the limit. Let us therefore consider two cases, $\\epsilon >0$ and $\\epsilon <0$. \n",
        "\n",
        "If $\\epsilon > 0$, then \n",
        "\n",
        "$$\\frac{H(x+\\epsilon)-H(x)}{\\epsilon} = \\frac{1}{\\epsilon}\\mathbb{1}_{]-\\epsilon,0[}(x) + \\frac{1}{2\\epsilon}\\mathbb{1}_{\\{-\\epsilon,0\\}}(x)  = \\frac{1}{\\epsilon}\\mathbb{1}_{]-1,0[}\\left(\\frac{x}{\\epsilon}\\right) + \\frac{1}{2\\epsilon}\\mathbb{1}_{\\{-1,0\\}}\\left(\\frac{x}{\\epsilon}\\right)$$\n",
        "\n",
        "where $\\mathbb{1}$ is the indicator function ($\\mathbb{1}_A(x) = 1$ if $x\\in A$ and $\\mathbb{1}_A(x) = 0$ if $x \\notin A$). We then have \n",
        "\n",
        "$$\\int \\lim_{\\epsilon\\rightarrow 0^+} \\frac{H(x+\\epsilon)-H(x)}{\\epsilon}  dx =  \\lim_{\\epsilon\\rightarrow 0^+} \\int \\frac{H(x+\\epsilon)-H(x)}{\\epsilon}  dx = \\lim_{\\epsilon\\rightarrow 0^+} \\int\\left(\\frac{1}{\\epsilon}\\mathbb{1}_{]-1,0[}\\left(\\frac{x}{\\epsilon}\\right) + \\frac{1}{2\\epsilon}\\mathbb{1}_{\\{-1,0\\}}\\left(\\frac{x}{\\epsilon}\\right) \\right) dx = \\lim_{\\epsilon\\rightarrow 0^+} \\int\\frac{1}{\\epsilon}\\mathbb{1}_{]-1,0[}\\left(\\frac{x}{\\epsilon}\\right)  dx $$\n",
        "\n",
        "where in the last equality we used the fact that the set $\\{-1,0\\}$ has Lebesgue measure $0$ and that $\\mathbb{1}_{\\{-1,0\\}}$ is bounded on that set. We then perform a change of variables $x = \\epsilon y$, and thus\n",
        "\n",
        "$$\\lim_{\\epsilon\\rightarrow 0^+} \\int\\frac{1}{\\epsilon}\\mathbb{1}_{]-1,0[}\\left( \\frac{x}{\\epsilon}\\right)  dx = \\lim_{\\epsilon\\rightarrow 0^+} \\int \\mathbb{1}_{]-1,0[}(y) dy = \\int \\mathbb{1}_{]-1,0[}(y) dy = \\int_{-1}^0 1\\cdot dy = [y]_{-1}^{0}= 0-(-1) =1$$\n",
        "\n",
        "In other words, we have just proved that $\\int H_R'(x)dx =1$.\n",
        "\n",
        "If $\\epsilon < 0$, then \n",
        "\n",
        "$$\\frac{H(x+\\epsilon)-H(x)}{\\epsilon} = \\frac{1}{-\\epsilon}\\mathbb{1}_{]0,-\\epsilon[}(x) + \\frac{1}{-2\\epsilon}\\mathbb{1}_{\\{0,-\\epsilon\\}}(x) = \\frac{1}{-\\epsilon}\\mathbb{1}_{]0,1[}\\left(\\frac{x}{-\\epsilon}\\right) + \\frac{1}{-2\\epsilon}\\mathbb{1}_{\\{0,1\\}}\\left(\\frac{x}{-\\epsilon}\\right)$$\n",
        "\n",
        "We then have, \n",
        "\n",
        "$$\\int \\lim_{\\epsilon\\rightarrow 0^-} \\frac{H(x+\\epsilon)-H(x)}{\\epsilon}  dx =  \\lim_{\\epsilon\\rightarrow 0^-} \\int \\frac{H(x+\\epsilon)-H(x)}{\\epsilon}  dx = \\lim_{\\epsilon\\rightarrow 0^-}\\int \\left(\\frac{1}{-\\epsilon}\\mathbb{1}_{]0,1[}\\left(\\frac{x}{-\\epsilon}\\right) + \\frac{1}{-2\\epsilon}\\mathbb{1}_{\\{0,1\\}}\\left(\\frac{x}{-\\epsilon}\\right) \\right) dx=\\lim_{\\epsilon\\rightarrow 0^-} \\int \\frac{1}{-\\epsilon}\\mathbb{1}_{]0,1[}\\left(\\frac{x}{-\\epsilon}\\right)  dx $$\n",
        "\n",
        "where in the last equality we used the fact that th set $\\{0,1\\}$ has Lebesgue measure $0$ and that $\\mathbb{1}_{\\{0,1\\}}$ is bounded on that set. We then perform a change of variables $x = -\\epsilon y$, and thus\n",
        "\n",
        "$$\\lim_{\\epsilon\\rightarrow 0^-} \\int \\frac{1}{-\\epsilon}\\cdot\\mathbb{1}_{]-1,0[}\\left(\\frac{x}{-\\epsilon}\\right)  dx = \\lim_{\\epsilon\\rightarrow 0^-} \\int \\mathbb{1}_{]0,1[}(y) dy = \\int \\mathbb{1}_{]0,1[}(y) dy = \\int_{0}^1 1\\cdot dy = [y]_{0}^{1}= 1-0 =1$$\n",
        "\n",
        "In other words, we have just proved that $\\int H_L'(x)dx =1 = \\int H_R'(x)dx$, and thus $$\\boxed{\\int H'(x)dx =1}$$ verifying condition (C3).\n",
        "\n",
        "---\n",
        "**Remark** Take any reasonable function $\\phi :\\mathbb{R} \\rightarrow \\mathbb{R}$ continuous at $0$. Then by the same arguments as in point d), we have \n",
        "\n",
        "$$\\int \\phi(x)H_R'(x)dx = \\int \\phi(x)\\lim_{\\epsilon\\rightarrow 0^+} \\frac{H(x+\\epsilon)-H(x)}{\\epsilon}  dx = \\lim_{\\epsilon\\rightarrow 0^+} \\int \\phi(x)\\frac{1}{\\epsilon}\\mathbb{1}_{[-\\epsilon,0]}(x) dx = \\lim_{\\epsilon\\rightarrow 0^+} \\int \\phi(\\epsilon y)\\mathbb{1}_{[-1,0]}(y) dy $$\n",
        "\n",
        "where in the last equality we performed the change of variables $x= \\epsilon y$, \n",
        "\n",
        "$$\\lim_{\\epsilon\\rightarrow 0^+} \\int \\phi(\\epsilon y)\\mathbb{1}_{[-1,0]}(y) dy = \\lim_{\\epsilon\\rightarrow 0^+} \\int_{-1}^{0} \\phi(\\epsilon y) dy =  \\int_{-1}^{0} \\lim_{\\epsilon\\rightarrow 0^+} \\phi(\\epsilon y) dy = \\int_{-1}^{0} \\phi(0) dy = \\phi(0)$$\n",
        "\n",
        "Thus we have proven that $\\int \\phi(x)H_R'(x)dx = \\phi(0)$.\n",
        "\n",
        "In a similar fashion, we get \n",
        "\n",
        "$$\\int \\phi(x)H_L'(x)dx = \\int \\phi(x)\\lim_{\\epsilon\\rightarrow 0^-} \\frac{H(x+\\epsilon)-H(x)}{\\epsilon}  dx = \\lim_{\\epsilon\\rightarrow 0^-} \\int \\phi(x)\\frac{1}{-\\epsilon}\\mathbb{1}_{[0, -\\epsilon]}(x) dx = \\lim_{\\epsilon\\rightarrow 0^-} \\int \\phi(-\\epsilon y)\\mathbb{1}_{[0,1]}(y) dy $$\n",
        "\n",
        "where in the last equality we performed the change of variables $x= -\\epsilon y$, \n",
        "\n",
        "$$\\lim_{\\epsilon\\rightarrow 0^-} \\int \\phi(-\\epsilon y)\\mathbb{1}_{[0,1]}(y) dy = \\lim_{\\epsilon\\rightarrow 0^-} \\int_{0}^{1} \\phi(-\\epsilon y) dy =  \\int_{0}^{1} \\lim_{\\epsilon\\rightarrow 0^-} \\phi(-\\epsilon y) dy = \\int_{0}^{1} \\phi(0) dy = \\phi(0)$$\n",
        "\n",
        "Thus we have proven that  $\\int \\phi(x)H_L'(x)dx = \\phi(0)= \\int \\phi(x)H_R'(x)dx$, which can be informally stated as $$ \\boxed{\\int \\phi(x)H'(x)dx = \\phi(0)}$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "buL1u_215Ooj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Question 4:\n",
        "\n",
        "a) Let us first start off by clarifying the notation a bit. For every $i \\in \\{1,2,\\ldots,n\\}$, define \n",
        "\n",
        "$$S_i(\\mathbf{x}) = S_i(x_1,x_2,\\ldots,x_n) = \\frac{e^{x_i}}{\\sum_{k}e^{x_k}}$$\n",
        "\n",
        "and further define $$S: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n: \\mathbf{x}\\longmapsto (S_1(\\mathbf{x}), S_2(\\mathbf{x}),\\ldots,S_n(\\mathbf{x}))$$\n",
        "\n",
        "Let us now calculate the Jacobian matrix $J_S$ of $S$. \n",
        "\n",
        "We know that $J_S$ is an $n\\times n$ matrix with entries $\\left(J_S\\right)_{ij} = \\frac{\\partial S_i}{\\partial x_j}$. For the diagonal elements (where $i=j$), we get \n",
        "\n",
        "$$\\frac{\\partial}{\\partial x_i}S_i(\\mathbf{x}) = \\frac{\\partial}{\\partial x_i} \\frac{e^{x_i}}{\\sum_{k}e^{x_k}} = \\frac{\\left(\\frac{\\partial}{\\partial x_i}e^{x_i}\\right) \\sum_{k}e^{x_k} - e^{x_i} \\left(\\frac{\\partial}{\\partial x_i}\\sum_{k}e^{x_k}\\right)}{\\left(\\sum_{k}e^{x_k}\\right)^2} = \\frac{e^{x_i}}{\\sum_{k}e^{x_k}}\\cdot \\left(1- \\frac{e^{x_i}}{\\sum_{k}e^{x_k}}\\right) = S_i(\\mathbf{x})\\cdot \\left(1- S_i(\\mathbf{x}) \\right)$$\n",
        "\n",
        "and for the off-diagonal elements (where $i\\neq j$), we get\n",
        "\n",
        "$$\\frac{\\partial}{\\partial x_j}S_i(\\mathbf{x}) = \\frac{\\partial}{\\partial x_j} \\frac{e^{x_i}}{\\sum_{k}e^{x_k}} = -e^{x_i}\\cdot \\frac{\\frac{\\partial}{\\partial x_j} \\sum_{k}e^{x_k}}{\\left( \\sum_{k}e^{x_k}\\right)^2} = -e^{x_i}\\cdot \\frac{e^{x_j}}{\\left( \\sum_{k}e^{x_k}\\right)^2} = -S_i(\\mathbf{x})S_j(\\mathbf{x})$$\n",
        "\n",
        "Thus the diagonal elements $(J_S)_{ii}$ are equal to $S_i(\\mathbf{x})\\cdot \\left(1- S_i(\\mathbf{x}) \\right)$, while the off-diagonal elements $(J_S)_{ij}$ are equal to $-S_i(\\mathbf{x})S_j(\\mathbf{x})$. In other words, \n",
        "\n",
        "$$\\boxed{J_S (\\mathbf{x})= \\begin{bmatrix}\n",
        "    S_1(\\mathbf{x})\\cdot \\left(1- S_1(\\mathbf{x}) \\right) & -S_1(\\mathbf{x})S_2(\\mathbf{x}) & \\dots  & -S_1(\\mathbf{x})S_n(\\mathbf{x}) \\\\\n",
        "    -S_2(\\mathbf{x})S_1(\\mathbf{x}) & S_2(\\mathbf{x})\\cdot \\left(1- S_2(\\mathbf{x}) \\right)  & \\dots  & -S_2(\\mathbf{x})S_n(\\mathbf{x}) \\\\\n",
        "    \\vdots & \\vdots  & \\ddots & \\vdots \\\\\n",
        "    -S_n(\\mathbf{x})S_1(\\mathbf{x}) & -S_n(\\mathbf{x})S_2(\\mathbf{x}) &  \\dots  & S_n(\\mathbf{x})\\cdot \\left(1- S_n(\\mathbf{x}) \\right)\n",
        "\\end{bmatrix}} $$ \n",
        "\n",
        "---\n",
        "\n",
        "b) First observe that,\n",
        "\n",
        "$$J_S (\\mathbf{x})= \\begin{bmatrix}\n",
        "    S_1(\\mathbf{x})& 0 & \\dots  & 0 \\\\\n",
        "    0 & S_2(\\mathbf{x}) & \\dots  & 0 \\\\\n",
        "    \\vdots & \\vdots  & \\ddots & \\vdots \\\\\n",
        "    0 & 0 &  \\dots  & S_n(\\mathbf{x})\n",
        "\\end{bmatrix} - \\begin{bmatrix}\n",
        "    S_1(\\mathbf{x})S_1(\\mathbf{x}) & S_1(\\mathbf{x})S_2(\\mathbf{x}) & \\dots  & S_1(\\mathbf{x})S_n(\\mathbf{x}) \\\\\n",
        "    S_2(\\mathbf{x})S_1(\\mathbf{x}) & S_2(\\mathbf{x})S_2(\\mathbf{x})  & \\dots  & S_2(\\mathbf{x})S_n(\\mathbf{x}) \\\\\n",
        "    \\vdots & \\vdots  & \\ddots & \\vdots \\\\\n",
        "    S_n(\\mathbf{x})S_1(\\mathbf{x}) & S_n(\\mathbf{x})S_2(\\mathbf{x}) &  \\dots  & S_n(\\mathbf{x})S_n(\\mathbf{x}) \n",
        "\\end{bmatrix}$$\n",
        "\n",
        "Now considering $S(\\mathbf{x}) = (S_1(\\mathbf{x}), S_2(\\mathbf{x}),\\ldots,S_n(\\mathbf{x}))$ and $\\mathbb{1}_n = (1,1,\\ldots,1)$ as both $1\\times n$ row vectors, and $\\textrm{Id}_n$ the $n\\times n$ identity matrix, then we have\n",
        "\n",
        "$$\\begin{bmatrix}\n",
        "    S_1(\\mathbf{x})& 0 & \\dots  & 0 \\\\\n",
        "    0 & S_2(\\mathbf{x}) & \\dots  & 0 \\\\\n",
        "    \\vdots & \\vdots  & \\ddots & \\vdots \\\\\n",
        "    0 & 0 &  \\dots  & S_n(\\mathbf{x})\n",
        "\\end{bmatrix} = \\textrm{Id}_n \\odot \\left(\\mathbb{1}_n^T S(\\mathbf{x}) \\right) $$\n",
        "\n",
        "where $\\odot$ is the Hadamard product. Further, \n",
        "\n",
        "$$\\begin{bmatrix}\n",
        "    S_1(\\mathbf{x})S_1(\\mathbf{x}) & S_1(\\mathbf{x})S_2(\\mathbf{x}) & \\dots  & S_1(\\mathbf{x})S_n(\\mathbf{x}) \\\\\n",
        "    S_2(\\mathbf{x})S_1(\\mathbf{x}) & S_2(\\mathbf{x})S_2(\\mathbf{x})  & \\dots  & S_2(\\mathbf{x})S_n(\\mathbf{x}) \\\\\n",
        "    \\vdots & \\vdots  & \\ddots & \\vdots \\\\\n",
        "    S_n(\\mathbf{x})S_1(\\mathbf{x}) & S_n(\\mathbf{x})S_2(\\mathbf{x}) &  \\dots  & S_n(\\mathbf{x})S_n(\\mathbf{x}) \n",
        "\\end{bmatrix} = S(\\mathbf{x})^T S(\\mathbf{x})$$\n",
        "\n",
        "thus we conclude that $$\\boxed{J_S(\\mathbf{x}) = \\textrm{Id}_n \\odot \\left(\\mathbb{1}_n^T S(\\mathbf{x}) \\right) - S(\\mathbf{x})^T S(\\mathbf{x}) }$$\n",
        "\n",
        "---\n",
        "\n",
        "c) Let us again start by clarifying some notations. Define for all $i \\in \\{ 1, 2,\\ldots, n \\}$, \n",
        "\n",
        "$$\\sigma_i: \\mathbb{R}^n \\rightarrow \\mathbb{R} : \\mathbf{x} \\longmapsto \\sigma(x_i)$$\n",
        "\n",
        "and \n",
        "\n",
        "$$\\overline{\\sigma}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n : \\mathbf{x} \\longmapsto (\\sigma_1(\\mathbf{x}),\\sigma_2(\\mathbf{x}), \\ldots, \\sigma_n(\\mathbf{x})) = (\\sigma(x_1),\\sigma(x_2),\\ldots, \\sigma(x_n))$$\n",
        "\n",
        "Let us now calculate the Jacobian matrix $J_{\\overline{\\sigma}}$ of $\\overline{\\sigma}$.\n",
        "\n",
        "We know that $J_{\\overline{\\sigma}}$ is an $n\\times n$ matrix with entries $\\left(J_{\\overline{\\sigma}}\\right)_{ij} = \\frac{\\partial \\sigma_i}{\\partial x_j}$. We immediately see that the off-diagonal elements are all zero, in fact for $i\\neq j$, we have $$\\frac{\\partial }{\\partial x_j} \\sigma_i (\\mathbf{x})= \\frac{\\partial }{\\partial x_j}\\sigma (x_i) = 0 $$\n",
        "\n",
        "For the diagonal elements, we have \n",
        "\n",
        "$$\\frac{\\partial}{\\partial x_i} \\sigma_i(\\mathbf{x}) = \\frac{\\partial }{\\partial x_i}\\sigma (x_i) = \\frac{\\partial }{\\partial x_i} \\frac{1}{1+e^{-x_i}} = - \\frac{\\frac{\\partial }{\\partial x_i}\\left(1+e^{-x_i}\\right)}{\\left(1+e^{-x_i}\\right)^2} = \\frac{e^{-x_i}}{\\left(1+e^{-x_i}\\right)^2} = \\sigma(x_i) \\cdot \\left( 1- \\sigma(x_i) \\right) = \\sigma'(x_i)$$\n",
        "\n",
        "and thus  \n",
        "\n",
        "$$\\boxed{ J_{\\overline{\\sigma}}(\\mathbf{x}) = \\begin{bmatrix}\n",
        "    \\sigma(x_1) \\cdot \\left( 1- \\sigma(x_1) \\right)& 0 & \\dots  & 0 \\\\\n",
        "    0 & \\sigma(x_2) \\cdot \\left( 1- \\sigma(x_2) \\right) & \\dots  & 0 \\\\\n",
        "    \\vdots & \\vdots  & \\ddots & \\vdots \\\\\n",
        "    0 & 0 &  \\dots  & \\sigma(x_n) \\cdot \\left( 1- \\sigma(x_n) \\right)\n",
        "\\end{bmatrix} \n",
        "=\\begin{bmatrix}\n",
        "    \\sigma'(x_1) & 0 & \\dots  & 0 \\\\\n",
        "    0 & \\sigma'(x_2) & \\dots  & 0 \\\\\n",
        "    \\vdots & \\vdots  & \\ddots & \\vdots \\\\\n",
        "    0 & 0 &  \\dots  & \\sigma'(x_n) \n",
        "\\end{bmatrix} }$$\n",
        "\n",
        "---\n",
        "\n",
        "d) Let us again start by clarifying some notations. Assuming that $\\mathbf{x},\\mathbf{y} \\in \\mathbb{R}^n$, let us define for every $i\\in \\{1,2,\\ldots,n\\}$$$f_i: \\mathbb{R}^n \\rightarrow \\mathbb{R}: \\mathbf{x}=(x_1,x_2,\\ldots,x_n) \\longmapsto f_i(x)=\\mathbf{y}_i$$\n",
        "as well as \n",
        "\n",
        "$$f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n: \\mathbf{x}=(x_1,x_2,\\ldots,x_n) \\longmapsto (f_1(\\mathbf{x}),f_2(\\mathbf{x}),\\ldots,f_n(\\mathbf{x}))$$\n",
        "\n",
        "We know that $J_{\\mathbf{y}}(\\mathbf{x})$ is an $n\\times n$ matrix with entries $\\left(J_{\\mathbf{y}}(\\mathbf{x})\\right)_{ij} = \\frac{\\partial }{\\partial x_j}f_i (\\mathbf{x})$. \n",
        "\n",
        "If $f \\equiv \\overline{\\sigma}$, then for all $ \\mathbf{x}\\in \\mathbb{R}^n$, we have $$J_\\mathbf{y}(\\mathbf{x}) = J_{\\overline{\\sigma}}(\\mathbf{x}) = \\begin{bmatrix}\n",
        "    \\sigma'(x_1) & 0 & \\dots  & 0 \\\\\n",
        "    0 & \\sigma'(x_2) & \\dots  & 0 \\\\\n",
        "    \\vdots & \\vdots  & \\ddots & \\vdots \\\\\n",
        "    0 & 0 &  \\dots  & \\sigma'(x_n) \n",
        "\\end{bmatrix}$$\n",
        "\n",
        "Let us consider the two $n \\times 1$ column vectors $\\overline{\\sigma}'(\\mathbf{x})= (\\sigma'(x_1), \\sigma'(x_2),\\ldots,\\sigma'(x_n))^T$ and $g_\\mathbf{y} = (g_\\mathbf{y}^1,g_\\mathbf{y}^2, \\ldots, g_\\mathbf{y}^n )^T$, then \n",
        "\n",
        "$$ \\boxed{g_\\mathbf{x} = J_\\mathbf{y}(\\mathbf{x}) \\cdot g_\\mathbf{y} = \\begin{bmatrix}\n",
        "    \\sigma'(x_1) & 0 & \\dots  & 0 \\\\\n",
        "    0 & \\sigma'(x_2) & \\dots  & 0 \\\\\n",
        "    \\vdots & \\vdots  & \\ddots & \\vdots \\\\\n",
        "    0 & 0 &  \\dots  & \\sigma'(x_n) \n",
        "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
        "    g_\\mathbf{y}^1  \\\\\n",
        "    g_\\mathbf{y}^2  \\\\\n",
        "    \\vdots  \\\\\n",
        "    g_\\mathbf{y}^n  \n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "    \\sigma'(x_1)\\cdot g_\\mathbf{y}^1  \\\\\n",
        "     \\sigma'(x_2)\\cdot g_\\mathbf{y}^2  \\\\\n",
        "    \\vdots  \\\\\n",
        "     \\sigma'(x_n)\\cdot g_\\mathbf{y}^n  \n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "    g_\\mathbf{y}^1 \\\\\n",
        "    g_\\mathbf{y}^2  \\\\\n",
        "    \\vdots  \\\\\n",
        "    g_\\mathbf{y}^n  \n",
        "\\end{bmatrix} \\odot \\begin{bmatrix}\n",
        "    \\sigma'(x_1)  \\\\\n",
        "     \\sigma'(x_2)  \\\\\n",
        "    \\vdots  \\\\\n",
        "     \\sigma'(x_n)  \n",
        "\\end{bmatrix} = g_{\\mathbf{y}}\\odot \\overline{\\sigma}'(\\mathbf{x})}$$\n",
        "\n",
        "where again $\\odot$ is the Hadamard product. \n",
        "\n",
        "If $f\\equiv S$, then from what we have established in point a), we have $$J_{\\mathbf{y}}(\\mathbf{x}) = J_S(\\mathbf{x}) = \\textrm{Id}_n \\odot \\left(\\mathbb{1}_n^T S(\\mathbf{x}) \\right) - S(\\mathbf{x})^T S(\\mathbf{x}) $$\n",
        "\n",
        "Now observe that \n",
        "\n",
        "$$\\left(\\textrm{Id}_n \\odot \\left(\\mathbb{1}_n^T S(\\mathbf{x}) \\right)\\right) \\cdot g_{\\mathbf{y}} =\\begin{bmatrix}\n",
        "    S_1(\\mathbf{x})& 0 & \\dots  & 0 \\\\\n",
        "    0 & S_2(\\mathbf{x}) & \\dots  & 0 \\\\\n",
        "    \\vdots & \\vdots  & \\ddots & \\vdots \\\\\n",
        "    0 & 0 &  \\dots  & S_n(\\mathbf{x})\n",
        "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
        "    g_\\mathbf{y}^1 \\\\\n",
        "    g_\\mathbf{y}^2  \\\\\n",
        "    \\vdots  \\\\\n",
        "    g_\\mathbf{y}^n  \n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "    S_1(\\mathbf{x})\\cdot g_\\mathbf{y}^1  \\\\\n",
        "     S_2(\\mathbf{x})\\cdot g_\\mathbf{y}^2  \\\\\n",
        "    \\vdots  \\\\\n",
        "     S_n(\\mathbf{x})\\cdot g_\\mathbf{y}^n  \n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "    g_\\mathbf{y}^1 \\\\\n",
        "    g_\\mathbf{y}^2  \\\\\n",
        "    \\vdots  \\\\\n",
        "    g_\\mathbf{y}^n  \n",
        "\\end{bmatrix} \\odot \\begin{bmatrix}\n",
        "    S_1(\\mathbf{x})  \\\\\n",
        "     S_2(\\mathbf{x})  \\\\\n",
        "    \\vdots  \\\\\n",
        "     S_n(\\mathbf{x})  \n",
        "\\end{bmatrix} = g_{\\mathbf{y}}\\odot S(\\mathbf{x})^T$$\n",
        "\n",
        "thus we get the following equation,\n",
        "\n",
        "$$g_\\mathbf{x} = g_\\mathbf{y}\\odot S(\\mathbf{x})^T-(S(\\mathbf{x})^T S(\\mathbf{x}))\\cdot g_{\\mathbf{y}}$$\n",
        "keeping in mind that $S(\\mathbf{x})$ was initially defined as a $1 \\times n $ row vector, whereas $g_\\mathbf{y}$ was defined as a $n \\times 1$ column vector. \n",
        "\n",
        "So we already see that we don't the same relation as with the case $f \\equiv \\overline{\\sigma}$, but let's further analyse the term $(S(\\mathbf{x})^T S(\\mathbf{x}))\\cdot g_{\\mathbf{y}}$, \n",
        "\n",
        "$$(S(\\mathbf{x})^T S(\\mathbf{x}))\\cdot g_{\\mathbf{y}} = S(\\mathbf{x})^T (S(\\mathbf{x})\\cdot g_{\\mathbf{y}})$$\n",
        "\n",
        "where $S(\\mathbf{x})\\cdot g_{\\mathbf{y}}$ is a scalar, thus we can write\n",
        "\n",
        "$$S(\\mathbf{x})^T (S(\\mathbf{x})\\cdot g_{\\mathbf{y}}) = (S(\\mathbf{x})\\cdot g_{\\mathbf{y}}) S(\\mathbf{x})^T = \\left((S(\\mathbf{x})\\cdot g_{\\mathbf{y}})\\mathbb{1}_n^T \\right) \\odot S(\\mathbf{x})^T$$\n",
        "\n",
        "where $\\mathbb{1}_n$ is the $1 \\times n$ row vector filled with $1$'s and thus, \n",
        "\n",
        "$$\\boxed{g_{\\mathbf{x}} = g_\\mathbf{y}\\odot S(\\mathbf{x})^T - \\left((S(\\mathbf{x})\\cdot g_{\\mathbf{y}})\\mathbb{1}_n^T \\right) \\odot S(\\mathbf{x})^T =  \\left( g_\\mathbf{y}- (S(\\mathbf{x})\\cdot g_{\\mathbf{y}})\\mathbb{1}_n^T \\right) \\odot S(\\mathbf{x})^T }$$\n",
        "\n",
        "and hence $g_{\\mathbf{x}}$ cannot be defined in the same way as in $f \\equiv \\overline{\\sigma}$."
      ]
    },
    {
      "metadata": {
        "id": "IC47vZlzrE1F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Question 5:\n",
        "\n",
        "a) For any real constant $c$, $$\\textrm{softmax}(\\mathbf{x}+c) = \\left(\\frac{e^{x_i+c}}{\\sum_{k=1}^{|x|}e^{x_k+c}}\\right)_{i=1,\\ldots,|x|} = \\left(\\frac{e^c\\cdot e^{x_i}}{e^c\\cdot \\sum_{k=1}^{|x|}e^{x_k}}\\right)_{i=1,\\ldots,|x|} = \\left(\\frac{e^{x_i}}{\\sum_{k=1}^{|x|}e^{x_k}}\\right)_{i=1,\\ldots,|x|} = \\textrm{softmax}(\\mathbf{x})$$\n",
        "\n",
        "---\n",
        "\n",
        "b) Consider $f(\\theta) = (f_1(\\theta),f_2(\\theta),\\ldots,f_{|x|}(\\theta))= \\textrm{softmax}(\\theta \\cdot\\mathbf{x}) = \\left(\\frac{e^{\\theta \\cdot x_i}}{\\sum_{k=1}^{|x|}e^{\\theta \\cdot x_k}}\\right)_{i=1,\\ldots,|x|}$ and let's look at the sign of derivative for each component: \n",
        "\n",
        "$$f_i'(\\theta) = \\frac{\\sum_{k=1}^{|x|} (x_i-x_k)e^{\\theta(x_k-x_i)}}{\\left(\\sum_{k=1}^{|x|}e^{\\theta(x_k-x_i)}\\right)^2}$$\n",
        "\n",
        "Let us assume that there is at least two distinct elements among $x_i$'s. Let us take $M$ and $m$ to be the indices of the maximal and minimal element among $\\{x_1,x_2,...,x_{|x|}\\}$ respectively, then $f_M'(\\theta) > 0 $ and $f_m'(\\theta) < 0 $ so we know that $f_M$ is strictly increasing and $f_m$ is strictly decreasing in $\\theta$. \n",
        "\n",
        "\n",
        "If $\\theta > 1$, we see that $f_M(\\theta) > f_M(1)$ and $f_m(\\theta) < f_m(1)$, in order words the maximum increases and the minimum decreases, while $\\sum_{k=1}^{|x|}f_k(\\theta) = 1$. The output of the softmax becomes more \"polarizing\", increasing the certitude of the network in its prediction.\n",
        "\n",
        "If $\\theta < 1$, we see that $f_M(\\theta) < f_M(1)$ and $f_m(\\theta) > f_m(1)$, in order words the maximum decreases and the minimum increases, while $\\sum_{k=1}^{|x|}f_k(\\theta) = 1$. The output of the softmax becomes less \"polarizing\", decreasing the certitude of the network in its prediction.\n",
        "\n",
        "---\n",
        "\n",
        "c) $$\\textrm{softmax}(\\mathbf{x}) = \\begin{matrix}\\Big(\\frac{e^{x_1}}{e^{x_1}+e^{x_2}} & \\frac{e^{x_2}}{e^{x_1}+e^{x_2}} \\Big)^T\\end{matrix} = \\begin{matrix}\\Big(\\frac{1}{1+e^{x_2-x_1}} & \\frac{e^{x_2-x_1}}{1+e^{x_2-x_1}} \\Big)^T\\end{matrix} = \\begin{matrix}\\Big(\\frac{1}{1+e^{x_2-x_1}} & 1-\\frac{1}{1+e^{x_2-x_1}} \\Big)^T\\end{matrix} = \\begin{matrix}\\Big( \\sigma (x_1-x_2) & 1- \\sigma(x_1-x_2)\\Big)^T\\end{matrix}= \\begin{matrix}\\Big( \\sigma (z) & 1-\\sigma(z)\\Big)^T\\end{matrix}$$ where $z=x_1-x_2$\n",
        "\n",
        "---\n",
        "\n",
        "d) Define for each positive integer $k$, and for each $i\\in \\{1,\\ldots,k\\}$, the following functions\n",
        "\n",
        "$$\\sigma_i^k: \\mathbb{R}^k \\rightarrow \\mathbb{R}: \\xi = (\\xi_1,\\xi_2,\\ldots,\\xi_k ) \\longmapsto \\frac{e^{\\xi_i}}{\\sum_{j=1}^k e^{\\xi_j}}$$\n",
        "\n",
        "and $$\\overline{\\sigma}^k: \\mathbb{R}^k \\rightarrow \\mathbb{R}^k: \\xi = (\\xi_1,\\xi_2,\\ldots,\\xi_k ) \\longmapsto (\\sigma_1^k(\\xi), \\sigma_2^k(\\xi), \\ldots, \\sigma_k^k(\\xi)) $$\n",
        "\n",
        "Similarly define, for each positive integer $k$, and for each $i\\in \\{1,\\ldots,k\\}$, the functions\n",
        "\n",
        "$$\\Lambda_i^k: \\mathbb{R}^k \\rightarrow \\mathbb{R}: \\xi = (\\xi_1,\\xi_2,\\ldots,\\xi_k ) \\longmapsto \\frac{e^{\\xi_i}}{1+\\sum_{j=1}^k e^{\\xi_j}}$$\n",
        "\n",
        "Now fix a positive integer $n\\geq 2$, then for each vector $x=(x_1,x_2,\\ldots, x_n) \\in \\mathbb{R}^n$ and each index $i \\in \\{1,\\ldots,n-1\\}$, we have \n",
        "$$\\sigma_i^n(x_1,x_2,\\ldots, x_n)= \\frac{e^{x_i}}{\\sum_{j=1}^n e^{x_j}} = \\frac{e^{x_i-x_n}}{1+\\sum_{j=1}^{n-1} e^{x_j-x_n}} = \\Lambda_{i}^{n-1}(x_1-x_n,x_2-x_n,\\ldots, x_{n-1}-x_n)$$\n",
        "\n",
        "and \n",
        "\n",
        "$$\\sigma_n^n(x_1,x_2,\\ldots, x_n) = \\frac{e^{x_n}}{\\sum_{j=1}^n e^{x_j}} = \\frac{1}{1+\\sum_{j=1}^{n-1} e^{x_j-x_n}}= 1- \\frac{\\sum_{j=1}^{n-1} e^{x_j-x_n}}{1+\\sum_{j=1}^{n-1} e^{x_j-x_n}} = 1- \\sum_{i=1}^{n-1} \\Lambda_{i}^{n-1}(x_1-x_n,x_2-x_n,\\ldots, x_{n-1}-x_n)$$\n",
        "\n",
        "We can continue rewriting the equations a bit further, by defining for each $i\\in \\{1,\\ldots,n-1\\}$, \n",
        "\n",
        "$$y_i = x_i-x_n$$\n",
        "\n",
        "and thus for each $i\\in \\{1,\\ldots,n-1\\}$, we get \n",
        "\n",
        "$$\\sigma_i^n(x_1,x_2,\\ldots, x_n) = \\Lambda_{i}^{n-1}(y_1,y_2,\\ldots, y_{n-1})$$\n",
        "\n",
        "and \n",
        "\n",
        "$$\\sigma_n^n(x_1,x_2,\\ldots, x_n) = 1- \\sum_{i=1}^{n-1} \\Lambda_{i}^{n-1}(y_1,y_2,\\ldots, y_{n-1})$$\n",
        "\n",
        "or more compactly, for each $x=(x_1,x_2,\\ldots,x_n) \\in \\mathbb{R}^n$, we can rewrite \n",
        "\n",
        "$$\\boxed{\\overline{\\sigma}^n(x) = \\left( \\Lambda_{1}^{n-1}(\\Delta_n(x)), \\Lambda_{2}^{n-1}(\\Delta_n(x)), \\ldots, \\Lambda_{n-1}^{n-1}(\\Delta_n(x)), 1- \\sum_{i=1}^{n-1} \\Lambda_{i}^{n-1}(\\Delta_n(x)) \\right)}$$\n",
        "\n",
        "where $$\\Delta_n :\\mathbb{R}^n \\rightarrow \\mathbb{R}^{n-1}: (x_1,x_2,\\ldots, x_n) \\longmapsto (x_1-x_n,x_2-x_n,\\ldots, x_{n-1}-x_n)$$\n"
      ]
    },
    {
      "metadata": {
        "id": "Gszc8VV39vjw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Question 6:\n",
        "\n",
        "a) the MLE principle works as follows: \n",
        "\n",
        "Assume we observe $n$ datapoints $x_1,...,x_n$ that we assumed to be i.i.d. according to some family of distributions $\\mathcal{D}(\\theta)$ parametrized by $\\theta$. We then define the likelihood to be the density function of the joint probability distribution\n",
        "$$L(\\theta) = f_{\\mathcal{D}}(x_1,\\ldots,x_n | \\theta) = \\prod_{i=1}^{n}f_{\\mathcal{D}}(x_i|\\theta)$$\n",
        "which we want to maximize with respect to $\\theta$. The $\\theta$ for which $L(\\theta)$ is maximal, is called the *maximum likelihood estimator*, and usually denoted as $\\widehat{\\theta}_{MLE}$ in the literature. Note that \n",
        "\n",
        "$$\\widehat{\\theta}_{MLE} = \\textrm{argmax}_{\\theta} L(\\theta) = \\textrm{argmax}_{\\theta} \\log L(\\theta) = \\textrm{argmin}_{\\theta} \\left(- \\log L(\\theta)\\right) = \\textrm{argmin}_{\\theta} \\left(-\\sum_{i=1}^{n}\\log f_{\\mathcal{D}}(x_i|\\theta)\\right)$$\n",
        "\n",
        "If we observed a single binary datapoint $x \\in \\{0,1\\}$ that we assumed was drawn from a Bernouilli distribution with parameter $p$, then \n",
        "\n",
        "$$\\log f_{\\mathcal{D}}(x|p) = \\log \\left( p^x\\cdot (1-p)^{(1-x)}\\right) = x\\cdot \\log p + (1-x)\\cdot \\log(1-p)$$\n",
        "\n",
        "and thus \n",
        "\n",
        "$$\\widehat{p}_{MLE} = \\textrm{argmin}_{p} \\left( -x\\cdot \\log p - (1-x)\\cdot \\log(1-p) \\right) = \\textrm{argmin}_{p}  ce(p,x) $$\n",
        "\n",
        "b) Let's start off with a little bit of background on the Kullback-Leibler divergence: \n",
        "\n",
        "In the most general case, if $P$ and $Q$ are two probability measures over a set $X$, and $P$ is absolutely continuous with respect to $Q$, then we define the *Kullback-Leibler divergence* from $Q$ to $P$ as\n",
        "\n",
        "$$D_{KL}(P//Q) = \\int_X \\log \\frac{dP}{dQ}dP $$\n",
        "\n",
        "where $\\frac{dP}{dQ}$ is the Radon-Nikodym derivative of $P$ with respect to $Q$. In the discrete setting, this translates into: \n",
        "\n",
        "$$D_{KL}(P//Q) = \\sum_{x \\in X} P(x)\\log \\frac{P(x)}{Q(x)} = - \\sum_{x \\in X} P(x)\\log Q(x) + \\sum_{x \\in X} P(x)\\log P(x) = H(P,Q)  - H(P)$$\n",
        "\n",
        "where $$H(P,Q) =  - \\sum_{x \\in X} P(x)\\log Q(x)$$ is called the cross-entropy of $P$ and $Q$, while $$H(P) = - \\sum_{x \\in X} P(x)\\log P(x)$$ is called the entropy of $P$. In Bayesian statistics, we can think of Kullback-Leibler divergence as a measure of information gain when moving from a prior distribution $Q$ to a posterior distribution $P$. It turns out that it consitutes the only measure of difference between two probability distribution satisfying some desired property, and therefore is commonly used in practice.\n",
        "\n",
        "Coming back to our example, we want to view $x \\in (0,1)$ as giving rise to posterior Bernouilli distribution $P$ of parameter $x$, while starting off from a prior Bernouilli distribution $Q$ of parameter $p$, and we want to find $p$ that minimizes the information gain when moving form $Q$ to $P$. \n",
        "\n",
        "In other words, we want to minimize $D_{KL}(P//Q)$ with respect to $p$, which in turn comes down to minimizing the cross-entropy term $H(P,Q)$ with respect to $p$, as $H(P)$ does not depend on $p$.\n",
        "\n",
        "$$H(P,Q) = -P(0)\\log Q(0) - P(1) \\log Q(1) = -x \\log p - (1-x) \\log (1-p) = ce(p,x)$$\n",
        "\n",
        "and thus $$\\widehat{p} = \\textrm{argmin}_p D_{KL}(P//Q) = \\textrm{argmin}_p H(P,Q) = \\textrm{argmin}_p ce(p,x)$$\n"
      ]
    },
    {
      "metadata": {
        "id": "tWqMxTewjFEp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Question 7: \n",
        "\n",
        "We will start this question by proving results in the most general fashion possible. We will establish a series of results valid for any activation function verfying some very mild assumptions, in order to really understand what result was due to the specific nature of the activation function, and what results come from additional assumptions made on the activation function as well as on the distribution of neurons or pre-activations in each layer. Once all the results are established, we start treating both parts of the question as immediate corollaries of our results.\n",
        "\n",
        "It should be noted that we implicitely use the hypothesis of our random variables to be independent at multiple instances which is strictly speaking mathematically incorrect, but it seems to work out in practice pretty as well, as shown in Glorot's paper. One could however try to deal with these dependencies and derive the most general form of result possible, and see for which instances, which dependencies effect initializiation the most. Due to immediate time constraints, we are not tackling this problem. \n",
        "\n",
        "Throught the answer we will use $f$ to denote the activation function, which is assumed to be the same for each layer of the network. Vectors will be thought of as row vectors. We will denote:\n",
        "*  $d_l$ the number of neurons in layer $l$, \n",
        "*  $a^{l}$ is the pre-activation of layer $l$ of dimension $1 \\times d_{l}$,\n",
        "*  $b^l$ the bias row vector between layer $l-1$ and $l$ of dimension $1 \\times d_{l}$,\n",
        "*  $x^l$ the vector of neurons (post-activations) of layer $l-1$ of dimensions $1 \\times d_{l-1}$, \n",
        "*  $W^{l}$ the $d_{l-1}\\times d_l$ weight matrix between layer $l-1$ and $l$. \n",
        "\n",
        "We thus have the following equations for each layer $l$,\n",
        "\n",
        "$$a^l = x^l W^l + b^l \\textrm{ and } x^{l+1}=f(a^l)$$\n",
        "\n",
        "We assume throught the paper that for each layer the weights are initialized accroding some distribution $\\mathbf{W}^l$ with mean $0$, which we want to determine. \n",
        "\n",
        "Let us first point out a lemma that we will use quite frequently throughout this question.\n",
        "\n",
        "---\n",
        "\n",
        "**Magical Lemma** Let $X$ and $Y$ be two independent random variables, with $\\mathbb{E}[X]=0$. Then \n",
        "\n",
        "$$\\textrm{Var}(XY) = \\textrm{Var}[X]\\mathbb{E}[Y^2]$$\n",
        "\n",
        "**Proof**\n",
        "\n",
        "$$\\textrm{Var}(XY) = \\mathbb{E}[(XY)^2]- \\mathbb{E}[XY]^2 = \\mathbb{E}[X^2]\\mathbb{E}[Y^2] -  \\mathbb{E}[X]^2 \\mathbb{E}[Y]^2 = \\mathbb{E}[X^2]\\mathbb{E}[Y^2] = \\textrm{Var}[X]\\mathbb{E}[Y^2]$$\n",
        "\n",
        "----\n",
        "\n",
        "###BACKWARD PROPAGATION\n",
        "\n",
        "**Proposition 1** Let $L$ be the loss function. Then, for each layer $l$, we have\n",
        "\n",
        "$$\\textrm{Var}\\left[\\frac{\\partial L}{\\partial a_{k}^{l}}\\right] = \\left(d_{l+1} \\textrm{Var}\\left[W^{l+1}_{k,s}\\right]\\mathbb{E}\\left[f'(a_k^l)^2\\right]\\right) \\textrm{Var}\\left[\\frac{\\partial L}{\\partial a_{s}^{l+1}}\\right]$$\n",
        "\n",
        "**Proof** By the chain rule applied to $L$, we obtain: \n",
        "\n",
        "$$\\frac{\\partial L}{\\partial a_{k}^{l}} = \\frac{\\partial x_k^{l+1} }{\\partial a_{k}^{l}}\\frac{\\partial L}{\\partial x_k^{l+1}}=f'(a_k^l)\\frac{\\partial L}{\\partial x_k^{l+1}}$$\n",
        "\n",
        "as well as \n",
        "\n",
        "$$\\frac{\\partial L}{\\partial x_k^{l+1}} = \\sum_{s=1}^{d_{l+1}} \\frac{\\partial a_{s}^{l+1}}{\\partial x_k^{l+1}} \\frac{\\partial L}{\\partial a_{s}^{l+1}} =\\sum_{s=1}^{d_{l+1}} W^{l+1}_{k,s} \\frac{\\partial L}{\\partial a_{s}^{l+1}}$$\n",
        "\n",
        "Since $\\mathbb{E}[W^{l+1}]=0$, the last equation gives $\\mathbb{E}\\left[\\frac{\\partial L}{\\partial x_k^{l+1}}\\right]=0$, which combined with the former equation gives $\\mathbb{E}\\left[\\frac{\\partial L}{\\partial a_k^{l}}\\right]=0$. \n",
        "\n",
        "Using the Magical Lemma on the last equation, we get\n",
        "\n",
        "$$\\textrm{Var}\\left[\\frac{\\partial L}{\\partial x_k^{l+1}}\\right] = d_{l+1} \\textrm{Var}\\left[W^{l+1}_{k,s} \\frac{\\partial L}{\\partial a_{s}^{l+1}}\\right] = d_{l+1} \\textrm{Var}\\left[W^{l+1}_{k,s}\\right] \\textrm{Var}\\left[\\frac{\\partial L}{\\partial a_{s}^{l+1}}\\right]  $$\n",
        "\n",
        "Applying the Magical Lemma on the former equation, we get \n",
        "\n",
        "$$\\textrm{Var}\\left[\\frac{\\partial L}{\\partial a_{k}^{l}}\\right] = \\textrm{Var}\\left[\\frac{\\partial L}{\\partial x_k^{l+1}}\\right]\\mathbb{E}\\left[f'(a_k^l)^2\\right] $$\n",
        "\n",
        "which, by combining the two last equations, gives \n",
        "\n",
        "$$\\textrm{Var}\\left[\\frac{\\partial L}{\\partial a_{k}^{l}}\\right] = d_{l+1} \\textrm{Var}\\left[W^{l+1}_{k,s}\\right] \\textrm{Var}\\left[\\frac{\\partial L}{\\partial a_{s}^{l+1}}\\right]\\mathbb{E}\\left[f'(a_k^l)^2\\right] = \\left(d_{l+1} \\textrm{Var}\\left[W^{l+1}_{k,s}\\right]\\mathbb{E}\\left[f'(a_k^l)^2\\right]\\right) \\textrm{Var}\\left[\\frac{\\partial L}{\\partial a_{s}^{l+1}}\\right]$$\n",
        "\n",
        "---\n",
        "\n",
        "**Corollary 1** If further, for each layer $l$, we assume that $\\textrm{Var}\\left[\\frac{\\partial L}{\\partial a_{k}^{l}}\\right] = \\textrm{Var}\\left[\\frac{\\partial L}{\\partial a_{s}^{l+1}}\\right]$, then we have \n",
        "\n",
        "$$\\textrm{Var}\\left[W^{l+1}_{k,s}\\right] = \\frac{1}{d_{l+1} \\mathbb{E}\\left[f'(a_k^l)^2\\right]}$$\n",
        "\n",
        "---\n",
        "\n",
        "Notice how we didn't use any assumptions on the activation function, nor on distributions of the neurons (post-activations) nor pre-activations beside the fact that $\\textrm{Var}\\left[\\frac{\\partial L}{\\partial a_{k}^{l}}\\right] = \\textrm{Var}\\left[\\frac{\\partial L}{\\partial a_{s}^{l+1}}\\right]$ for each layer $l$. \n",
        "\n",
        "The critical value to compute when it comes to making use of backpropagation is thus $\\mathbb{E}\\left[f'(a_k^l)^2\\right]$. \n",
        "\n",
        "Since we assume $W^l$ to be symmetrically distributed, for each layer $l$ (which is something we can do), we have that $a^l$ is symmetrically distributed, and thus computing $\\mathbb{E}\\left[f'(a_k^l)^2\\right]$ boils down to computing the key quantity \n",
        "\n",
        "$$\\boxed{\\mathcal{B}(f,X)=\\mathbb{E}\\left[f'(X)^2\\right]}$$ \n",
        "\n",
        "for $X$ symmetrically distributed, and seeing how various assumptions on $f$ and potentially $X$ influence this key quantity. \n",
        "\n",
        "---\n",
        "\n",
        "###FORWARD PROPAGATION\n",
        "\n",
        "**Proposition 2** For each layer $l$, we have \n",
        "\n",
        "$$\\textrm{Var}\\left[W^l_{s,k}\\right] = \\frac{\\textrm{Var}(a_k^l)}{d_{l-1}\\mathbb{E}[(x_s^l)^2]}$$\n",
        "\n",
        "**Proof** Writing out $a^l = x^lW^l+b^l$ elementwise, we get \n",
        "\n",
        "$$a_k^l = \\sum_{s=1}^{d_{l-1}}x_s^lW_{s,k}^l +b_k^l$$\n",
        "\n",
        "and thus by applying the variance both sides, followed by an application of the Magical Lemma, \n",
        "\n",
        "$$\\textrm{Var}(a_k^l)=d_{l-1}\\textrm{Var}(x_s^lW_{s,k}^l)=d_{l-1}\\textrm{Var}(W_{s,k}^l)\\mathbb{E}[(x_s^l)^2]$$ which gives\n",
        "\n",
        "$$\\textrm{Var}(W_{s,k}^l) = \\frac{\\textrm{Var}(a_k^l)}{d_{l-1}\\mathbb{E}[(x_s^l)^2]}$$\n",
        "\n",
        "---\n",
        "\n",
        "**Corollary 2** \n",
        "\n",
        "If further, for each layer $l$, we assume that $\\textrm{Var}\\left[a_{k}^{l}\\right] = \\textrm{Var}\\left[ a_{s}^{l+1}\\right]$, then we have\n",
        "\n",
        "$$\\textrm{Var}(W_{s,k}^l) = \\frac{\\textrm{Var}(a_k^{l-1})}{d_{l-1}\\mathbb{E}[f(a_k^{l-1})^2]}$$\n",
        "\n",
        "---\n",
        "\n",
        "Notice how we didn't use any assumptions on the activation function, nor on distributions of the neurons (post-activations) nor pre-activations beside the fact that $\\textrm{Var}\\left[a_{k}^{l}\\right] = \\textrm{Var}\\left[ a_{s}^{l+1}\\right]$ for each layer $l$. \n",
        "\n",
        "The critical value to compute when it comes to making use of forward propagation is thus $\\frac{\\textrm{Var}(a_k^{l-1})}{\\mathbb{E}[f(a_k^{l-1})^2]}$.\n",
        "\n",
        "Since we assume $W^l$ to be symmetrically distributed, for each layer $l$, we have that $a^l$ is symmetrically distributed, and thus computing $\\frac{\\textrm{Var}(a_k^{l-1})}{\\mathbb{E}[f(a_k^{l-1})^2]}$ boils down to computing the key quantity \n",
        "\n",
        "$$\\boxed{\\mathcal{F}(f,X)= \\frac{\\mathbb{E}[f(X)^2]}{\\textrm{Var}(X)}}$$\n",
        "\n",
        "for $X$ symmetrically distributed, and seeing how various assumptions on $f$ and potentially $X$ influence this key quantity. \n",
        "\n",
        "---\n",
        "\n",
        "### FINAL PRELIMINARY REMARKS\n",
        "\n",
        "\n",
        "Finally let us make a remark about uniform distributions,\n",
        "\n",
        "**Lemma 3** Let $X \\sim U(a,b)$, then \n",
        "\n",
        "$$\\textrm{Var}[X]= \\frac{(a-b)^2}{12}$$\n",
        "\n",
        "**Proof** We know that the probability density function $f_X$ of $X$ satisfies $f_X(x)=\\frac{1}{b-a}$ for all $x\\in [a,b]$, and that $\\mathbb{E}[X]=\\frac{a+b}{2}$, thus we get \n",
        "\n",
        "$$\\textrm{Var}[X] = \\mathbb{E}[(x-\\mathbb{E}[X])^2] =  \\int_{a}^b \\left(x-\\frac{a+b}{2}\\right)^2 \\frac{1}{b-a} dx = \\left[ \\frac{1}{3}\\left(x-\\frac{a+b}{2}\\right)^3\\frac{1}{b-a}\\right]_{a}^{b} = \\frac{2}{3}\\left(\\frac{b-a}{2}\\right)^3\\frac{1}{b-a} = \\frac{(b-a)^{2}}{12}$$\n",
        "\n",
        "---\n",
        "\n",
        "**Corollary 3** The only uniform variable $X$ with $\\mathbb{E}[X]=0$ and $\\textrm{Var}[X]=r$, is \n",
        "\n",
        "$$X\\sim U(-\\sqrt{3r}, \\sqrt{3r})$$ \n",
        "\n",
        "**Proof** Let $X \\sim U(a,b)$. Since $\\mathbb{E}[X]=0$, we must have $a=-b$, and thus by Lemma 3, we have \n",
        "\n",
        "$$\\textrm{Var}[X] = \\frac{(2b)^2}{12}= \\frac{b^2}{3}$$\n",
        "\n",
        "Since we want $\\textrm{Var}[X] = r$, we must have \n",
        "\n",
        "$$\\frac{b^2}{3} = r \\Leftrightarrow b^2 = 3r \\Leftrightarrow b = \\pm \\sqrt{3r}$$\n",
        "\n",
        "---\n",
        "\n",
        "In what follows in the question, all we do is computing $\\mathcal{B}(f,X)$ and $\\mathcal{F}(f,X)$ for various situations: one where $f$ is assumed to be analytical and linear around 0, and another one $f$ is assumed to be ReLU. \n",
        "\n",
        "Our general goal is to derive constants $\\alpha_f$ and $\\beta_f$, that only depend on $f$, such that $\\alpha_f \\approx \\mathcal{B}(f,X)$ and $\\beta_f \\approx \\mathcal{F}(f,X)$, for any symmetrically distributed $X$. The more $X$ is concentrated around its mean (in this case 0), the better the approximations are going to be. And hence we can choose \n",
        "\n",
        "$$\\boxed{\\textrm{Var}(W^l) = \\frac{2}{d_l\\mathcal{B}(f,a_k^l) + d_{l-1}\\mathcal{F}(f,a_k^l)}= \\frac{2}{d_l\\alpha_f + d_{l-1}\\beta_f}}$$\n",
        "\n",
        "and we want $W^l$ to be distributed symmetrically, thus we can take, by the previous Corollary 3, \n",
        "\n",
        "$$\\boxed{W^l \\sim U\\left(-\\sqrt{\\frac{6}{d_l\\alpha_f + d_{l-1}\\beta_f}},\\sqrt{\\frac{6}{d_l\\alpha_f + d_{l-1}\\beta_f}}\\right)}$$\n",
        "\n",
        "Further we want to analyse what the minimal set of assumptions are we need to make on $f$, and see what we can derive from it. \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### BACK TO THE QUESTION\n",
        "\n",
        "a) As in Glorot's paper, we assume $f$ to be linear in a neighbourhood around $0$, and let us establish an important lemma for smooth function that is linear around $\\mu$.\n",
        "\n",
        "---\n",
        "\n",
        "**Lemma a1** Let $X$ be a random variable with $\\mathbb{E}[X] =\\mu$ and sufficiently concentrated around it's mean. Let $f:\\mathbb{R} \\rightarrow \\mathbb{R}$ be a real-valued function that is analytic around $\\mu$ satisfying $f^{(k)'}(\\mu)=0$ for all $k\\geq 2$.\n",
        "Then we have the following approximations, \n",
        "\n",
        "*   $\\mathbb{E}[f(X)] \\approx f(\\mu)$\n",
        "\n",
        "*   $\\textrm{Var}[f(X)] \\approx f'(\\mu)^2 \\textrm{Var}[X]$\n",
        "\n",
        "*   $\\mathbb{E}[f'(X)^2] \\approx f'(\\mu)^2$\n",
        "\n",
        "\n",
        "\n",
        "**Proof** Since $f$ is analytic, we can consider it's Taylor expansion in an open neighbourhood $D$ around $\\mu$. Thus for all $x\\in D$, \n",
        "\n",
        "$$f(x) = f(\\mu) + \\sum_{k=1}^{\\infty} \\frac{f^{(k)'}(\\mu)}{k!}(x-\\mu)^k = f(\\mu) + f'(\\mu)(x-\\mu)$$\n",
        "\n",
        "where the last equality follows from the fact that $f^{(k)'}(\\mu)=0$ for all $k\\geq 2$. \n",
        "\n",
        "Applying the expectation both sides, we get the following approximations \n",
        "\n",
        "$$\\mathbb{E}[f(X)] \\approx \\mathbb{E}[f(\\mu)] + \\mathbb{E}[f'(\\mu)(X-\\mu)] = f(\\mu) + f'(\\mu)(\\mathbb{E}[X]-\\mu)= f(\\mu) $$\n",
        "\n",
        "Applying the variance both sides, we get\n",
        "\n",
        "$$\\textrm{Var}[f(X)] \\approx \\textrm{Var}[f(\\mu)] + \\textrm{Var}[f'(\\mu)(X-\\mu)] = f'(\\mu)^2 \\textrm{Var}[X-\\mu] = f'(\\mu)^2 \\textrm{Var}[X]$$\n",
        "\n",
        "Finally observe that for all $x \\in D$, we have $f'(x) = f'(\\mu)$, and thus \n",
        "\n",
        "$$\\mathbb{E}[f'(X)^2] \\approx f'(\\mu)^2 $$\n",
        "\n",
        "---\n",
        "\n",
        "**Corollary a1.1** For every layer $l$, we have \n",
        "\n",
        "$$\\mathcal{B}(f,a_k^l) = f'(0)^2$$\n",
        "\n",
        "**Proof** By the previous lemma, we have \n",
        "\n",
        "$$\\mathcal{B}(f,a_k^l) = \\mathbb{E}[f'(a_k^l)^2] = f'(\\mathbb{E}[a_k^l])^2 =f'(0)^2 $$\n",
        "\n",
        "where in the last equation we used $\\mathbb{E}[a_k^l]=0$, which comes from the fact that $\\mathbb{E}[W^l] =0$. \n",
        " \n",
        "---\n",
        "**Corollary a1.2** If further we assume that for each layer $l$, we have $x_k^l \\sim \\mathcal{N}(0,1)$, then \n",
        "\n",
        "$$ \\mathcal{F}(f,a_k^l) = f'(0)^2$$\n",
        "\n",
        "**Proof** By the Lemma 2, we get \n",
        "\n",
        "$$\\textrm{Var}[x_k^{l+1}] = \\textrm{Var}[f(a_k^l)] = f'(\\mathbb{E}[a_k^l])^2 \\textrm{Var}[a_k^l] = f'(0)^2 \\textrm{Var}[a_k^l]$$\n",
        "\n",
        "where in the last equation we used $\\mathbb{E}[a_k^l]=0$, which comes from the fact that $\\mathbb{E}[W^l] =0$. \n",
        "\n",
        "Since $x_k^l \\sim \\mathcal{N}(0,1)$ for each layer $l$, we get \n",
        "\n",
        "$$ \\textrm{Var}[a_k^l] = \\frac{\\textrm{Var}[x_k^{l+1}]}{f'(0)^2} = \\frac{1}{f'(0)^2}$$\n",
        "\n",
        "and \n",
        "\n",
        "$$ \\mathbb{E}[f(a_k^l)^2] = \\textrm{Var}[f(a_k^l)] + \\left(\\mathbb{E}[f(a_k^l)]\\right)^2 = 1$$\n",
        "\n",
        "Thus,\n",
        "\n",
        "$$\\mathcal{F}(f,a_k^l) = \\frac{\\mathbb{E}[f(a_k^l)^2]}{\\textrm{Var}[a_k^l]} = \\frac{1}{\\frac{1}{f'(0)^2}}= f'(0)^2$$\n",
        "\n",
        "----\n",
        "\n",
        "If we continue to assume that $f'(0) = 1$ as in Glorot's paper, then $\\mathcal{F}(f,a_k^l) =1$ and $\\mathcal{B}(f,a_k^l) =1$, and thus we want \n",
        "\n",
        "$$\\textrm{Var}[W^l] = \\frac{2}{d_{l-1}+d_l}$$\n",
        "\n",
        "by corollary 3, we can pick\n",
        "\n",
        "$$W^l \\sim U\\left(-\\sqrt{\\frac{6}{d_{l-1}+d_l}},\\sqrt{\\frac{6}{d_{l-1}+d_l}}\\right)$$\n",
        "\n",
        "----\n",
        "\n",
        "**Remark a2** Notice how we did not use the assumption that the neurons are $\\mathcal{N}(0,1)$-distributed in corollary a1.1 but only in corollary a1.2. If we did not make the assumption that $x_k^l \\sim \\mathcal{N}(0,1)$, then we could write, by merely using Lemma a1,\n",
        "\n",
        "$$\\mathbb{E}[f(a_k^l)^2] = \\textrm{Var}(f(a_k^l)) + \\mathbb{E}[f(a_k^l)]^2 \\approx f'(\\mathbb{E}[a_k^l])^2\\textrm{Var}[a_k^l] + f(\\mathbb{E}[a_k^l])^2 = f'(0)^2 \\textrm{Var}[a_k^l] + f(0)^2$$\n",
        "\n",
        "where in the last equation we used $\\mathbb{E}[a_k^l]=0$, and thus,\n",
        "\n",
        "$$\\mathcal{F}(f,a_k^l) = \\frac{\\mathbb{E}[f(a_k^l)^2]}{\\textrm{Var}[a_k^l]} = f'(0)^2 + \\frac{f(0)^2}{\\textrm{Var}[a_k^l]}$$\n",
        "\n",
        "Finally note that for all $l \\geq 2$, $\\textrm{Var}[a_k^l] = \\textrm{Var}[a_k^1] = d_0 \\textrm{Var}[W^1] \\textrm{Var}[x]$\n",
        "\n",
        "where $x$ is the network's input. Define $\\eta := d_0 \\textrm{Var}[W^1] \\textrm{Var}[x]$. Thus, \n",
        "\n",
        "$$\\mathcal{F}(f,a_k^l) = f'(0)^2 + \\frac{f(0)^2}{\\eta}$$\n",
        "\n",
        "As corollary a1.1 is valid without any assumptions on the distributions distribution, we have \n",
        "\n",
        "$$\\mathcal{B}(f,a_k^l) = f'(0)^2  $$\n",
        "\n",
        "And thus for all $l \\geq 2$, we want \n",
        "\n",
        "$$\\textrm{Var}[W^l] = \\frac{2}{f'(0)^2d_l + \\left(f'(0)^2 + \\frac{f(0)^2}{\\eta}\\right)d_{l-1}}$$\n",
        "\n",
        "and thus we can pick, by corollary 3, for all $l\\geq 2$\n",
        "\n",
        "$$W^l \\sim U\\left(-\\sqrt{\\frac{6}{f'(0)^2d_l + \\left(f'(0)^2 + \\frac{f(0)^2}{\\eta}\\right)d_{l-1}}},\\sqrt{\\frac{6}{f'(0)^2d_l + \\left(f'(0)^2 + \\frac{f(0)^2}{\\eta}\\right)d_{l-1}}}\\right)$$\n",
        "\n",
        "---\n",
        "\n",
        "**Remark a3** We have hence derived the most general initialization scheme under the hypothesis that $f$ is sufficiently linear around $0$, the natural question is thus: \n",
        "\n",
        "*  what would happen if $f$ had non-vanishing higher order derivatives at $0$?\n",
        "*  what if $f$ wasn't even analytic around $0$ to begin with? \n",
        "\n",
        "\n",
        "For the former case, one keeps adding as many terms in the taylor expansion as necessary in Lemma a1. One eventually needs to deal with higher moment statistics of $a_k^l$, thus we can express $\\mathcal{B}(f,a_k^l)$ and $\\mathcal{F}(f,a_k^l)$ purely in terms of:\n",
        "*  all non-vansihing higher order derivatives of $f$ at $0$\n",
        "*  higher moment statistics $\\mathbb{E}[(a_k^l)^s]$, which if we further assume that all pre-activations are identically distributed, can be purely expressed in terms of higher moment statistics of $W^1$ and $x$.\n",
        "\n",
        "The case where $f$ isn't analytic around $0$ is dealt with in point b) with an example where $f$ is the ReLU function.\n",
        "\n",
        "---\n",
        "\n",
        "b) Let us start off by proving two lemmas about symmetric distributions and ReLU functions. \n",
        "\n",
        "\n",
        "**Lemma b1** Let $X$ be a symmetric distribution (i.e. satisfying $X\\overset{D}{=}-X$) and $f(x) = \\max(x,0)$, then \n",
        "\n",
        "$$\\mathbb{E}[f'(X)^2] = \\frac{1}{2}$$\n",
        "\n",
        "**Proof**\n",
        "\n",
        "$$\\mathbb{E}[f'(X)^2] = \\int_{-\\infty}^{+\\infty} f'(x)^2 \\cdot p_X(x) dx = \\int_{0}^{+\\infty} 1\\cdot p_X(x) dx = \\frac{1}{2}\\int_{-\\infty}^{+\\infty} p_X(x) dx = \\frac{1}{2}$$\n",
        "\n",
        "where in the last equality we used the fact that $X$ is symmetrically distributed.\n",
        "\n",
        "---\n",
        "\n",
        "**Corollary b1.1** For each layer $l$, \n",
        "\n",
        "$$\\mathcal{B}(f,a_k^l) = \\frac{1}{2}$$\n",
        "\n",
        "**Proof** Follows directly from the previous lemma and the fact that $a^{l}$ is symmetrically distributed which in turn follows directly from $W^{l}$ being symmetrically distributed.\n",
        "\n",
        "---\n",
        "\n",
        "**Lemma b2** Let $X$ be a symmetric distribution (i.e. satisfying $X\\overset{D}{=}-X$) and $f(x) = \\max(x,0)$, then \n",
        "\n",
        "$$\\mathbb{E}[f(X)^2]=\\frac{1}{2}\\textrm{Var}[X]$$\n",
        "\n",
        "**Proof** Note that $X$ being symmetric implies $\\mathbb{E}[X]=0$. Then \n",
        "\n",
        "$$\\mathbb{E}[f(X)^2] = \\int_{-\\infty}^{+\\infty} f(x)^2 \\cdot p_X(x) dx = \\int_{0}^{+\\infty} x^2\\cdot p_X(x) dx = \\frac{1}{2}\\int_{-\\infty}^{+\\infty} x^2\\cdot p_X(x) dx = \\frac{1}{2}\\mathbb{E}[X^2]=\\frac{1}{2} \\left(\\textrm{Var}[X]+\\left(\\mathbb{E}[X]\\right)^2\\right)=\\frac{1}{2} \\textrm{Var}[X]$$\n",
        "\n",
        "---\n",
        "\n",
        "**Corollary b2.1** For each layer $l$, \n",
        "\n",
        "$$\\mathcal{F}(f,a_k^l) = \\frac{1}{2}$$\n",
        "\n",
        "**Proof** **Proof** Follows directly from the previous lemma, and the fact that $a^{l}$ is symmetrically distributed which in turn follows directly from $W^{l}$ being symmetrically distributed.\n",
        "\n",
        "---\n",
        "\n",
        "Notice how so far we did not use the hypothesis that $a_l$ should be $\\mathcal{N}(0,1)$-distributed.\n",
        "\n",
        "From Corollary b1.1 and Corollary b2.1, we deduce that, \n",
        "\n",
        "$$\\textrm{Var}\\left[W^l_{s,k}\\right] = \\frac{4}{d_{l-1}+d_l}$$\n",
        "\n",
        "Using Corollary 3, we can choose \n",
        "\n",
        "$$W^l \\sim U\\left(-\\sqrt{\\frac{12}{d_{l-1}+d_l}},\\sqrt{\\frac{12}{d_{l-1}+d_l}}\\right)$$\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "mvcOvwHewtYc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Question 8:\n",
        "\n",
        "a) Let us choose the column representation, as it will turn out to be more convenient for point b), \n",
        "\n",
        "$$y(x) = W^{(2)}\\phi(W^{(1)}x+b^{(1)})+b^{(2)}$$\n",
        "\n",
        "---\n",
        "\n",
        "b) Let us first rewrite $y(x)$ in terms of $\\widetilde{W}^{(2)}$:\n",
        "\n",
        "$$y(x) = W^{(2)}\\phi(W^{(1)}x+b^{(1)})+b^{(2)} = \\left[\\phi(W^{(1)}x+b^{(1)})^T{W^{(2)}}^T+{b^{(2)}}^T \\right]^T = \\left[\\phi(W^{(1)}x+b^{(1)})^T,1\\right]\\widetilde{W}^{(2)}$$\n",
        "\n",
        "Now considering the fact that $W^{(1)} = [w,w,\\ldots,w]^T$ with $w\\in \\mathbb{R}^n$, we get \n",
        "\n",
        "$$W^{(1)}x = \\left[\\langle w,x \\rangle, \\langle w,x \\rangle, \\ldots, \\langle w,x \\rangle\\right]^T = \\langle w,x \\rangle \\mathbb{1}_{N-1}$$\n",
        "\n",
        "where $\\mathbb{1}_{N-1}$ is the $N-1$ dimensional column vector filled with $1$'s, and thus \n",
        "\n",
        "$$y(x) = \\left[\\phi(\\langle w,x \\rangle \\mathbb{1}_{N-1}+b^{(1)})^T,1\\right]\\widetilde{W}^{(2)} $$\n",
        "\n",
        "and thus we have \n",
        "\n",
        "$$M = \\begin{bmatrix}\n",
        "    \\left[\\phi(\\langle w,x^{(1)} \\rangle \\mathbb{1}_{N-1}+b^{(1)})^T,1\\right] \\\\\n",
        "    \\left[\\phi(\\langle w,x^{(2)} \\rangle \\mathbb{1}_{N-1}+b^{(1)})^T,1\\right]  \\\\\n",
        "    \\vdots  \\\\\n",
        "    \\left[\\phi(\\langle w,x^{(N)} \\rangle \\mathbb{1}_{N-1}+b^{(1)})^T,1\\right]  \n",
        "\\end{bmatrix}$$\n",
        "\n",
        "---\n",
        "\n",
        "c) First note that since the last column of $M$ consists of $1$'s, we can't get a lower-triangular matrix, and thus we need to get an upper-triangular matrix. In order to get an upper-triangular matrix we need to assume,\n",
        "\n",
        "$$\\langle w,x^{(1)} \\rangle > \\langle w,x^{(2)} \\rangle > \\ldots > \\langle w,x^{(N)} \\rangle$$\n",
        "\n",
        "We know that for all $i \\in \\{1,2,\\ldots,N\\}$, we have $M_{iN} = 1$, and for all $j \\in \\{1,2,\\ldots,N-1\\}$, we have\n",
        "\n",
        "$$M_{ij} = \\phi(\\langle w,x^{(i)} \\rangle +b_j^{(1)}) = \\phi(\\langle w,x^{(i)} \\rangle -\\langle w,x^{(j)} \\rangle + \\epsilon)$$\n",
        "\n",
        "Let us define \n",
        "\n",
        "$$\\Delta_N = \\min_{1\\leq i,j \\leq N, i\\neq j}\\{\\left|\\langle w,x^{(i)} \\rangle -\\langle w,x^{(j)} \\rangle \\right| \\} = \\min_{1\\leq i\\leq N-1} \\{\\langle w,x^{(i)} \\rangle -\\langle w,x^{(i+1)} \\rangle \\}$$\n",
        "\n",
        "which we know satisfies $\\Delta_N >0$, since all $\\langle w,x^{(i)} \\rangle$ are distinct. \n",
        "\n",
        "If we pick $\\epsilon >0$ such that $\\epsilon < \\Delta_N$, then for all $i,j \\in \\{1,2,\\ldots,N\\}$ such that $i>j$, we have \n",
        "\n",
        "$$\\langle w,x^{(i)} \\rangle -\\langle w,x^{(j)} \\rangle + \\epsilon <0$$ \n",
        "\n",
        "and thus \n",
        "\n",
        "$$M_{ij}= \\phi(\\langle w,x^{(i)} \\rangle -\\langle w,x^{(j)} \\rangle + \\epsilon) = 0$$\n",
        "\n",
        "and hence our matrix $M$ is upper-triangular. For the diagonal elements, we have for all $i \\in \\{1,2,\\ldots,N-1\\}$, \n",
        "\n",
        "$$M_{ii} = \\phi(\\langle w,x^{(i)} \\rangle -\\langle w,x^{(i)} \\rangle + \\epsilon) = \\phi(\\epsilon) = \\epsilon >0$$ \n",
        "\n",
        "and $M_{NN} = 1$. Thus all diagonal elements of $M$ are non-zero. \n",
        "\n",
        "---\n",
        "\n",
        "d) Again we assume\n",
        "\n",
        "$$\\langle w,x^{(1)} \\rangle > \\langle w,x^{(2)} \\rangle > \\ldots > \\langle w,x^{(N)} \\rangle$$\n",
        "\n",
        "We know that for all $i \\in \\{1,2,\\ldots,N\\}$, we have $M_{iN} = 1$, and for all $j \\in \\{1,2,\\ldots,N-1\\}$, we have\n",
        "\n",
        "$$M_{ij} = \\phi(\\langle w,x^{(i)} \\rangle +b_j^{(1)}) = \\phi(\\langle \\lambda u,x^{(i)} \\rangle -\\langle \\lambda u,x^{(j)} \\rangle ) = \\phi\\left(\\lambda \\left[\\langle  u,x^{(i)} \\rangle -\\langle u,x^{(j)} \\rangle \\right] \\right)$$\n",
        "\n",
        "for all $i,j \\in \\{1,2,\\ldots,N\\}$ such that $i>j$, we have $\\langle  u,x^{(i)} \\rangle -\\langle u,x^{(j)} \\rangle < 0$, and thus \n",
        "\n",
        "$$\\lim_{\\lambda \\rightarrow \\infty } M_{ij} = \\lim_{\\lambda \\rightarrow \\infty } \\phi\\left(\\lambda \\left[\\langle  u,x^{(i)} \\rangle -\\langle u,x^{(j)} \\rangle \\right] \\right) = \\phi(-\\infty) = 0$$\n",
        "\n",
        "Thus $M$ is upper-triangular. For the diagonal elements, we have for all $i \\in \\{1,2,\\ldots,N-1\\}$, \n",
        "\n",
        "$$M_{ii} = \\phi(\\langle w,x^{(i)} \\rangle -\\langle w,x^{(i)} \\rangle ) = \\phi(0) >0$$ \n",
        "\n",
        "and $M_{NN} = 1$. Thus all diagonal elements of $M$ are non-zero. \n",
        "\n",
        "Now let us double check that all upper-triangular elements are indeed bounded. For all $i \\in \\{1,2,\\ldots,N\\}$, we clearly have $M_{iN} = 1$, and for all $j \\in \\{1,2,\\ldots,N-1\\}$, $\\langle  u,x^{(i)} \\rangle -\\langle u,x^{(j)} \\rangle > 0$, and thus\n",
        "\n",
        "$$\\lim_{\\lambda \\rightarrow \\infty } M_{ij} = \\lim_{\\lambda \\rightarrow \\infty } \\phi\\left(\\lambda \\left[\\langle  u,x^{(i)} \\rangle -\\langle u,x^{(j)} \\rangle \\right] \\right) = \\phi(\\infty)$$\n",
        "\n",
        "which we know is bounded since we can assume $\\phi$ to be bounded. "
      ]
    },
    {
      "metadata": {
        "id": "6WkhvKia0NKJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Question 9:\n",
        "\n",
        "a) $$\\mathbb{E}[y|x] = \\int y \\cdot p(y|x) dy =  \\int y \\cdot\\sum_{i=1}^n p(c=i|x)\\cdot p(y|x,\\theta_i) dy = \\sum_{i=1}^n p(c=i|x)\\cdot \\int y \\cdot p(y|x,\\theta_i) dy = \\sum_{i=1}^n p(c=i|x)\\cdot \\mathbb{E}[y|x,\\theta_i]$$\n",
        "\n",
        "Since $y|x,\\theta_i \\sim \\mathcal{N}(y|x^T\\beta_i,\\sigma_i^2) $, we know that $\\mathbb{E}[y|x,\\theta_i] = x^T\\beta_i$, and thus \n",
        "\n",
        "$$\\mathbb{E}[y|x] = \\sum_{i=1}^n \\frac{\\exp(\\zeta_i^Tx)}{\\sum_{j} \\exp(\\zeta_{j}^Tx)}\\cdot x^T\\beta_i$$\n",
        "\n",
        "Using the definition of the variance, we get $\\textrm{Var}[y|x] = \\mathbb{E}[y^2|x] -\\mathbb{E}[y|x]^2$. As we have just calculated $\\mathbb{E}[y|x]$, we are left to calculate $\\mathbb{E}[y^2|x] $.\n",
        "\n",
        "$$\\mathbb{E}[y^2|x] = \\int y^2 \\cdot p(y|x) dy = \\int y^2\\cdot \\sum_{i=1}^n p(c=i|x)\\cdot p(y|x,\\theta_i) dy = \\sum_{i=1}^n p(c=i|x)\\cdot \\int y^2\\cdot p(y|x,\\theta_i) dy = \\sum_{i=1}^n p(c=i|x)\\cdot \\mathbb{E}[y^2|x,\\theta_i]$$\n",
        "\n",
        "Again by using the definition of the variance, we get \n",
        "\n",
        "$$\\textrm{Var}[y|x,\\theta_i] = \\mathbb{E}[y^2|x,\\theta_i] -\\mathbb{E}[y|x,\\theta_i]^2 \\Leftrightarrow \\mathbb{E}[y^2|x,\\theta_i] = \\textrm{Var}[y|x,\\theta_i] +\\mathbb{E}[y|x,\\theta_i]^2$$\n",
        "\n",
        "Since $y|x,\\theta_i \\sim \\mathcal{N}(y|x^T\\beta_i,\\sigma_i^2)$, we know that $\\textrm{Var}[y|x,\\theta_i] = \\sigma_i^2$ and $\\mathbb{E}[y|x,\\theta_i] = x^T\\beta_i$, and thus $\\mathbb{E}[y^2|x,\\theta_i] = \\sigma_i^2 + (x^T\\beta_i)^2 $, from which we conclude \n",
        "\n",
        "$$\\mathbb{E}[y^2|x]  = \\sum_{i=1}^n p(c=i|x)\\cdot \\mathbb{E}[y^2|x,\\theta_i] = \\sum_{i=1}^n \\frac{\\exp(\\zeta_i^Tx) \\cdot (\\sigma_i^2 + (x^T\\beta_i)^2)}{\\sum_{j} \\exp(\\zeta_{j}^Tx)} $$\n",
        "\n",
        "and finally, \n",
        "\n",
        "$$\\textrm{Var}[y|x] = \\mathbb{E}[y^2|x] -\\mathbb{E}[y|x]^2 =\\sum_{i=1}^n \\frac{\\exp(\\zeta_i^Tx) \\cdot (\\sigma_i^2 + (x^T\\beta_i)^2)}{\\sum_{j} \\exp(\\zeta_{j}^Tx)} - \\left[ \\sum_{i=1}^n \\frac{\\exp(\\zeta_i^Tx)}{\\sum_{j} \\exp(\\zeta_{j}^Tx)}\\cdot x^T\\beta_i\\right]^2$$\n",
        "\n",
        "---\n",
        "\n",
        "b) Let us write $p(y|x,\\zeta)$ rather than $p(y|x)$, in order to indicate the dependcency of $p(y|x)$ wrt $\\zeta$. \n",
        "\n",
        "According to the maximum likelihood principle, we want to find $\\zeta_k$ that maximizes $p(y|x,\\zeta) = \\sum_i s_i(x,\\zeta) p_i(x)$, where $$p_i(x) = p(y|x,\\theta_i,c=i)= \\frac{1}{\\sigma_i\\sqrt{2\\pi}}\\cdot \\exp \\left(-\\frac{1}{2}\\cdot \\left(\\frac{y-x^T\\beta_i}{\\sigma_i}\\right)^2\\right)$$ \n",
        "\n",
        "In order to maximize $p(y|x,\\zeta)$ wrt $\\zeta_k$, we use gradient ascent and thus we want to calculate\n",
        "\n",
        "$$\\nabla_{\\zeta_k} p(y|x,\\zeta) = \\nabla_{\\zeta_k} \\sum_i s_i(x,\\zeta) p_i(x) = \\sum_i p_i(x) \\nabla_{\\zeta_k} s_i(x,\\zeta)$$\n",
        "\n",
        "Let us therefore calculate the gradients $\\nabla_{\\zeta_i} s_i(x,\\zeta)$ and $\\nabla_{\\zeta_k} s_i(x,\\zeta)$ for $k\\neq i$,\n",
        "\n",
        "$$\\frac{\\partial }{\\partial \\zeta_{il}} s_i (x,\\zeta) = \\frac{\\partial}{\\partial\\zeta_{il}} \\frac{\\exp (\\zeta_i^T x)}{\\sum_{i'} \\exp (\\zeta_{i'}^T x)} = \\frac{\\left(\\sum_{i'} \\exp (\\zeta_{i'}^T x)\\right) \\cdot\\frac{\\partial}{\\partial\\zeta_{il}} \\exp (\\zeta_i^T x) - \\exp (\\zeta_i^T x) \\cdot \\frac{\\partial}{\\partial\\zeta_{il}} \\left(\\sum_{i'} \\exp (\\zeta_{i'}^T x)\\right)}{\\left(\\sum_{i'} \\exp (\\zeta_{i'}^T x)\\right)^2}$$\n",
        "\n",
        "Now observe that $\\frac{\\partial}{\\partial\\zeta_{il}} \\exp (\\zeta_i^T x) = x_l \\exp (\\zeta_i^T x)$ and that $\\frac{\\partial}{\\partial\\zeta_{il}} \\left(\\sum_{i'} \\exp (\\zeta_{i'}^T x)\\right) = \\frac{\\partial}{\\partial\\zeta_{il}} \\exp (\\zeta_i^T x) = x_l \\exp (\\zeta_i^T x)$, and thus \n",
        "\n",
        "\n",
        "$$\\frac{\\partial }{\\partial \\zeta_{il}} s_i (x,\\zeta) = \\frac{\\left(\\sum_{i'} \\exp (\\zeta_{i'}^T x)\\right) \\cdot x_l \\exp (\\zeta_i^T x) - \\exp (\\zeta_i^T x) \\cdot x_l \\exp (\\zeta_i^T x)}{\\left(\\sum_{i'} \\exp (\\zeta_{i'}^T x)\\right)^2} = x_l s_i(x,\\zeta) - x_l s_i(x,\\zeta)^2 = x_l s_i(x,\\zeta) \\left( 1-s_i(x,\\zeta)\\right)$$\n",
        "\n",
        "or in a more compact fashion, \n",
        "\n",
        "$$\\boxed{\\nabla_{\\zeta_i} s_i(x,\\zeta) = x \\cdot s_i(x,\\zeta) \\left( 1-s_i(x,\\zeta)\\right) }$$\n",
        "\n",
        "Now for the $k \\neq i$ case, we have \n",
        "\n",
        "$$\\frac{\\partial }{\\partial \\zeta_{kl}} s_i (x,\\zeta) = \\frac{\\partial}{\\partial\\zeta_{kl}} \\frac{\\exp (\\zeta_i^T x)}{\\sum_{i'} \\exp (\\zeta_{i'}^T x)} = -\\exp (\\zeta_i^T x) \\cdot \\frac{\\frac{\\partial}{\\partial\\zeta_{kl}}\\left(\\sum_{i'} \\exp (\\zeta_{i'}^T x)\\right) }{\\left(\\sum_{i'} \\exp (\\zeta_{i'}^T x)\\right)^2} = -\\exp (\\zeta_i^T x) \\cdot \\frac{x_l \\exp (\\zeta_k^T x)}{\\left(\\sum_{i'} \\exp (\\zeta_{i'}^T x)\\right)^2} = -x_l s_i(x,\\zeta) s_k(x,\\zeta)$$\n",
        "\n",
        "or more compactly, \n",
        "\n",
        "$$\\boxed{\\nabla_{\\zeta_k} s_i(x,\\zeta) = -x \\cdot s_i(x,\\zeta) s_k(x,\\zeta)}$$\n",
        "\n",
        "Thus \n",
        "\n",
        "$$\\nabla_{\\zeta_k} p(y|x,\\zeta) = \\sum_i p_i(x)\\cdot \\nabla_{\\zeta_k}s_i(x,\\zeta) = p_k(x)\\cdot x \\cdot s_k(x,\\zeta)\\left(1-s_k(x,\\zeta)\\right) - \\sum_{i\\neq k} p_i(x) \\cdot x\\cdot s_i(x,\\zeta)s_k(x,\\zeta) = x\\cdot s_k(x,\\zeta)\\cdot (p_k(x)- p(y|x,\\zeta))$$\n",
        "\n",
        "So \n",
        "\n",
        "$$\\boxed{\\nabla_{\\zeta_k} p(y|x,\\zeta) = x\\cdot s_k(x,\\zeta)\\cdot (p_k(x)- p(y|x,\\zeta))}$$\n",
        "\n",
        "Finally, gradient ascent for $\\zeta_k$, writes as\n",
        "\n",
        "$$\\boxed{\\zeta_k^{(m+1)} = \\zeta_k^{(m)} + \\gamma_m \\cdot \\left[ x\\cdot s_k(x,\\zeta^{(m)})\\cdot (p_k(x)- p(y|x,\\zeta^{(m)}))\\right]}$$\n",
        "\n",
        "where $\\gamma_m$ is the step size at each iteration $m$.\n",
        "\n",
        "---\n",
        "\n",
        "c) Again let us write $h(x,\\zeta)$ rather than $h(x)$, in order to indicate the dependcency of $h$ wrt $\\zeta$. \n",
        "\n",
        "We have $l(y,x)= (y-h(x,\\zeta))^2$. Thus,\n",
        "\n",
        "$$\\nabla_{\\zeta_k} l(y,x) = 2\\left(y-h(x,\\zeta)\\right)\\cdot\\nabla_{\\zeta_k}\\left(y-h(x,\\zeta)\\right) =2\\left(y-h(x)\\right)\\cdot \\left(-\\sum_i \\mu_i(x)\\cdot \\nabla_{\\zeta_k}s_i(x,\\zeta)\\right) $$ \n",
        "\n",
        "Now observe that $$-\\sum_i \\mu_i(x)\\cdot \\nabla_{\\zeta_k}s_i(x,\\zeta) = -\\mu_k(x)\\cdot x \\cdot s_k(x,\\zeta)\\left(1-s_k(x,\\zeta)\\right) + \\sum_{i\\neq k} \\mu_i(x) \\cdot x\\cdot s_i(x,\\zeta)s_k(x,\\zeta) = x s_k(x,\\zeta)\\cdot (h(x,\\zeta)- \\mu_k(x))$$\n",
        "\n",
        "and consequently, \n",
        "\n",
        "$$\\boxed{\\nabla_{\\zeta_k} l(y,x) = 2x \\cdot s_k(x,\\zeta) \\left(y-h(x,\\zeta)\\right)(h(x,\\zeta)- \\mu_k(x)) }$$\n",
        "\n",
        "Finally, gradient descent for $\\zeta_k$, writes as\n",
        "\n",
        "$$\\boxed{\\zeta_k^{(m+1)} = \\zeta_k^{(m)} - \\gamma_m \\cdot \\left[ 2x \\cdot s_k(x,\\zeta^{(m)}) \\left(y-h(x,\\zeta^{(m)})\\right)(h(x,\\zeta^{(m)})- \\mu_k(x))\\right]}$$\n",
        "\n",
        "where $\\gamma_m$ is the step size at each iteration $m$.\n",
        "\n",
        "Note that $h(x,\\zeta)$ corresponds exactly to $\\mathbb{E}[y|x]$, or let's rather write $\\mathbb{E}[y|x,\\zeta]$ to mark the dependency upon $\\zeta$. And $\\mu_k(x) = x^T\\beta_k = \\mathbb{E}[y|x,\\theta_k,c=k]$\n",
        "\n",
        "---\n",
        "\n",
        "d) Let us rewrite $$l(y,x) = (y-\\mathbb{E}[y|x,\\zeta])^2$$ and $$\\nabla_{\\zeta_k} l(y,x) = 2x \\cdot s_k(x,\\zeta) \\left(y-\\mathbb{E}[y|x,\\zeta]\\right)(\\mathbb{E}[y|x,\\zeta]- \\mathbb{E}[y|x,\\zeta,c=k]) $$\n",
        "\n",
        "While $$\\nabla_{\\zeta_k} p(y|x,\\zeta) = x\\cdot s_k(x,\\zeta)\\cdot (p_k(x)- p(y|x,\\zeta))$$\n",
        "\n",
        "For point b) we tried to maximize $p(y|x)$ wrt $\\zeta_k$, and we see that the gradient becomes smaller, the more the overall distribution describes the distribution of the gaussian of class $k$. Intuitively updating $\\zeta_k$ tries put as much weight on class $k$ as possible, thus trying to make he overall distribution match distribution of the gaussian of class $k$. Note that the distribution of the gaussian of class $k$ does not depend on $\\zeta$, while $p(y|x)$ does.\n",
        "\n",
        "For point c) we tried to minimize $(y-\\mathbb{E}[y|x,\\zeta])^2$ wrt $\\zeta_k$, and thus the gradient becomes smaller, if the overall mean $\\mathbb{E}[y|x,\\zeta]$ gets closer to $y$, or if the overall mean $\\mathbb{E}[y|x,\\zeta]$ gets closer to the class mean $\\mathbb{E}[y|x,\\zeta,c=k]$. In this case we are not as interested in matching the overall distribution with the distribution of the model of class $k$, but we just want the overall mean to be closer to the value $y$, while at the same time we want the overall mean to be closer to the mean of class $k$. Note that mean of class $k$ does not depend on $\\zeta$, while $\\mathbb{E}[y|x,\\zeta]$ does. \n",
        "\n",
        "The problem with this last approach is that, if the class mean $\\mathbb{E}[y|x,\\zeta,c=k]$ already matches the overall mean $\\mathbb{E}[y|x,\\zeta]$, even if $\\mathbb{E}[y|x,\\zeta]$ is not close to $y$, the gradient completely vanishes, and thus no more learning is happening."
      ]
    },
    {
      "metadata": {
        "id": "UpQk2H7gB2sl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Question 10:\n",
        "\n",
        "First observe that for all $a \\in \\mathbb{R}$,\n",
        "\n",
        "$$\\tanh(a) +1 = \\frac{e^{a}-e^{-a}}{e^{a}+e^{-a}}+1 = \\frac{2e^{a}}{e^{a}+e^{-a}}= \\frac{2}{1+e^{-2a}} = 2\\sigma(2a)$$\n",
        "\n",
        "or equivalently for all $a \\in \\mathbb{R}$,\n",
        "\n",
        "$$\\sigma(a) = \\frac{1}{2}\\left[\\tanh(\\frac{a}{2}) +1\\right]$$\n",
        "\n",
        "and thus \n",
        "\n",
        "$$y_k(x,w) = \\sigma \\left( \\sum_{j=1}^M w_{kj}^{(2)}\\cdot \\sigma \\left( \\sum_{i=1}^D w_{ji}^{(1)}x_i +w_{j0}^{(1)} \\right)+w_{k0}^{(2)}\\right) = \\sigma \\left( \\sum_{j=1}^M \\frac{w_{kj}^{(2)}}{2} \\cdot \\tanh \\left( \\sum_{i=1}^D \\frac{w_{ji}^{(1)}}{2}x_i +\\frac{w_{j0}^{(1)}}{2} \\right)+\\left(w_{k0}^{(2)}+ \\sum_{j=1}^M \\frac{w_{kj}^{(2)}}{2}\\right) \\right)$$\n",
        "\n",
        "Let us define $\\overline{w_{k0}}^{(2)} = w_{k0}^{(2)}+ \\sum_{j=1}^M \\frac{w_{kj}^{(2)}}{2}$ and for $j\\neq 0: $  $\\overline{w_{kj}}^{(2)} = \\frac{1}{2}w_{kj}^{(2)}$ as well as for all $j,i$: $\\overline{w_{ji}}^{(1)} = \\frac{1}{2}w_{ji}^{(1)}$. We then have,\n",
        "\n",
        "$$ y_k(x,w) = \\sigma \\left( \\sum_{j=1}^M \\overline{w_{kj}}^{(2)}\\cdot \\tanh \\left( \\sum_{i=1}^D \\overline{w_{ji}}^{(1)}x_i +\\overline{w_{j0}}^{(1)} \\right)+\\overline{w_{k0}}^{(2)}\\right)$$"
      ]
    }
  ]
}