{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "data_loc = './data/mnist/mnist.pkl'\n",
    "data = np.load(data_loc)\n",
    "print arr[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_data = data[0][0]\n",
    "mnist_train_labels = data[0][1]\n",
    "\n",
    "mnist_train_tensor = torch.utils.data.TensorDataset(torch.Tensor(mnist_train_data),torch.IntTensor(mnist_train_labels))\n",
    "\n",
    "mnist_val_data = data[1][0]\n",
    "mnist_val_labels = data[1][1]\n",
    "\n",
    "mnist_val_tensor = torch.utils.data.TensorDataset(torch.Tensor(mnist_val_data),torch.IntTensor(mnist_val_labels))\n",
    "\n",
    "\n",
    "mnist_test_data = data[2][0]\n",
    "mnist_test_labels = data[2][1]\n",
    "\n",
    "mnist_test_tensor = torch.utils.data.TensorDataset(torch.Tensor(mnist_test_data),torch.IntTensor(mnist_test_labels))\n",
    "\n",
    "\n",
    "mnist_train = torch.utils.data.DataLoader(mnist_train_tensor, batch_size=64, shuffle=True, num_workers=2)\n",
    "mnist_val = torch.utils.data.DataLoader(mnist_val_tensor, batch_size=64, shuffle=True, num_workers=2)\n",
    "mnist_test = torch.utils.data.DataLoader(mnist_test_tensor, batch_size=64, shuffle=True, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mnist_transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "#mnist_train = torchvision.datasets.MNIST(root='./data', train=True, transform=mnist_transforms, download=True)\n",
    "#mnist_test = torchvision.datasets.MNIST(root='./data', train=False, transform=mnist_transforms, download=True)\n",
    "\n",
    "\n",
    "#trainloader = torch.utils.data.DataLoader(mnist_train, batch_size=64, shuffle=True, num_workers=2)\n",
    "#testloader = torch.utils.data.DataLoader(mnist_test, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "class MLP_MNIST(nn.Module):\n",
    "    \n",
    "    def __init__(self,dims,outsize,dropout=[0.0,0.0]):\n",
    "        super(MLP_MNIST,self).__init__()\n",
    "        self.fc1 = nn.Linear(dims[0],dims[1])\n",
    "        self.dropout1 = nn.Dropout(p=dropout[0])\n",
    "        self.fc2 = nn.Linear(dims[1],dims[2])\n",
    "        self.dropout2 = nn.Dropout(p=dropout[1])\n",
    "        self.fc3 = nn.Linear(dims[2],outsize)\n",
    "        \n",
    "        self.insize = dims[0]\n",
    "        \n",
    "    def forward(self,x):\n",
    "        a1 = self.fc1(x)\n",
    "        d1 = self.dropout1(a1)\n",
    "        h1 = nn.functional.relu(d1)\n",
    "        \n",
    "        a2 = self.fc2(h1)\n",
    "        d2 = self.dropout2(a2)\n",
    "        h2 = nn.functional.relu(d2)\n",
    "        \n",
    "        logits = self.fc3(h2)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def prediction(self,logits):\n",
    "        \n",
    "        values, indices = torch.max(logits.data,1)\n",
    "        \n",
    "        return values, indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_train(model,trainloader,lr,batch_size):\n",
    "    print model\n",
    "    #trainloader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    loss_crit = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(),lr=lr)\n",
    "    NUMEPOCHS=10\n",
    "    epoch_loss= [0]*NUMEPOCHS\n",
    "    for epoch in range(NUMEPOCHS):\n",
    "        losses = []\n",
    "        for batch_index, (inputs, targets) in enumerate(trainloader):\n",
    "            x, targets = Variable(inputs.view([-1,model.insize])), Variable(targets)\n",
    "\n",
    "            logits = model.forward(x)\n",
    "    \n",
    "            loss = loss_crit(logits,targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.data[0])\n",
    "        epoch_loss[epoch] = np.mean(losses)\n",
    "        print('Epoch : %d Loss : %.3f ' % (epoch+1, np.mean(losses)))\n",
    "    return epoch_loss\n",
    "\n",
    "#Initializes using Glorot Initialization\n",
    "def GlorotInitialize(model):\n",
    "    #Initialize parameters\n",
    "    params = list(model.parameters())\n",
    "\n",
    "    for p in params:\n",
    "        if len(p.size()) > 1:\n",
    "            dl = np.sqrt(6.0/(p.size()[0]+p.size()[1]))\n",
    "            nn.init.uniform(p,-dl,dl)\n",
    "        else:\n",
    "            nn.init.uniform(p,0,0)\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "For each of the following tests the model architecture has 2 hidden layers one of size 500 and one of size 200.  The non-linearity is a Rectified Linear Unit (ReLu), the output layer uses a log-softmax function and the loss uses standard cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero weight initialization\n",
      "MLP_MNIST(\n",
      "  (fc1): Linear(in_features=784, out_features=500)\n",
      "  (dropout1): Dropout(p=0.0)\n",
      "  (fc2): Linear(in_features=500, out_features=200)\n",
      "  (dropout2): Dropout(p=0.0)\n",
      "  (fc3): Linear(in_features=200, out_features=10)\n",
      ")\n",
      "Epoch : 1 Loss : 2.302 \n",
      "Epoch : 2 Loss : 2.302 \n",
      "Epoch : 3 Loss : 2.302 \n",
      "Epoch : 4 Loss : 2.302 \n",
      "Epoch : 5 Loss : 2.302 \n",
      "Epoch : 6 Loss : 2.302 \n",
      "Epoch : 7 Loss : 2.302 \n",
      "Epoch : 8 Loss : 2.302 \n",
      "Epoch : 9 Loss : 2.302 \n",
      "Epoch : 10 Loss : 2.303 \n",
      "Normal weight initialization mean of zero, variance of 1\n",
      "MLP_MNIST(\n",
      "  (fc1): Linear(in_features=784, out_features=500)\n",
      "  (dropout1): Dropout(p=0.0)\n",
      "  (fc2): Linear(in_features=500, out_features=200)\n",
      "  (dropout2): Dropout(p=0.0)\n",
      "  (fc3): Linear(in_features=200, out_features=10)\n",
      ")\n",
      "Epoch : 1 Loss : 34.032 \n",
      "Epoch : 2 Loss : 2.309 \n",
      "Epoch : 3 Loss : 2.569 \n",
      "Epoch : 4 Loss : 2.314 \n",
      "Epoch : 5 Loss : 2.313 \n",
      "Epoch : 6 Loss : 2.316 \n",
      "Epoch : 7 Loss : 2.316 \n",
      "Epoch : 8 Loss : 2.322 \n",
      "Epoch : 9 Loss : 2.325 \n",
      "Epoch : 10 Loss : 2.326 \n",
      " Glorot Initialization\n",
      "MLP_MNIST(\n",
      "  (fc1): Linear(in_features=784, out_features=500)\n",
      "  (dropout1): Dropout(p=0.0)\n",
      "  (fc2): Linear(in_features=500, out_features=200)\n",
      "  (dropout2): Dropout(p=0.0)\n",
      "  (fc3): Linear(in_features=200, out_features=10)\n",
      ")\n",
      "Epoch : 1 Loss : 1.679 \n",
      "Epoch : 2 Loss : 0.602 \n",
      "Epoch : 3 Loss : 0.681 \n",
      "Epoch : 4 Loss : 0.749 \n",
      "Epoch : 5 Loss : 0.778 \n",
      "Epoch : 6 Loss : 0.681 \n",
      "Epoch : 7 Loss : 0.532 \n",
      "Epoch : 8 Loss : 0.382 \n",
      "Epoch : 9 Loss : 0.340 \n",
      "Epoch : 10 Loss : 0.314 \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcf86ef1850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Zero Weight Initialization\n",
    "print \"Zero weight initialization\"\n",
    "lr=1e-4\n",
    "ZeroInitNet = MLP_MNIST([28*28,500,200],10)\n",
    "params = list(ZeroInitNet.parameters())\n",
    "for p in params:\n",
    "    nn.init.uniform(p,0,0)\n",
    "    \n",
    "zero_losses = init_train(ZeroInitNet,mnist_train,lr,128)\n",
    "\n",
    "print \"Normal weight initialization mean of zero, variance of 1\"\n",
    "lr=1e-3\n",
    "NormInitNet = MLP_MNIST([28*28,500,200],10)\n",
    "params = list(NormInitNet.parameters())\n",
    "for p in params:\n",
    "    if len(p.size()) > 1:\n",
    "        nn.init.normal(p,0,1)\n",
    "    else:\n",
    "        nn.init.uniform(p,0,0)\n",
    "norm_losses = init_train(NormInitNet,mnist_train,lr,128)\n",
    "\n",
    "print \" Glorot Initialization\"\n",
    "lr=1e-5\n",
    "GlorotInitNet = MLP_MNIST([28*28,500,200],10)\n",
    "GlorotInit_Net = GlorotInitialize(GlorotInitNet)\n",
    "glorot_losses = init_train(GlorotInit_Net,mnist_train,lr,128)\n",
    "\n",
    "plt.plot(zero_losses,label='Train loss: Zero initialized ')\n",
    "plt.plot(norm_losses,label='Train loss: Norm Initialized')\n",
    "plt.plot(glorot_losses,label='Train loss: Glorot Initialized')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization Comments\n",
    "# GC we're asked to comment on the different results from the short training tests above.  All networks are identical (learning rates are not) and we need to discuss why each performed differently.\n",
    "First it should be noted that the loss remains unchanged when using 0 weight intialization.  This can be attributed to the fact that each neuron will output the same value during forward propagation. When considering back propagation, each neuron will follow the same gradient and change identically. Thus the network will learn nothing.\n",
    "\n",
    "Second, when intializing values using a normal distribution, the intial loss is very high.  This is because the intialization values can be large and output values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#returns the best instance in training of the model to be trained as well as the loss and accuracy\n",
    "#for both train and val if available.\n",
    "def train(model, num_epochs,trainLoader,optimizer,valLoader=None, testLoader = None):\n",
    "    bestValAcc = 0\n",
    "    bestNetwork = 0\n",
    "    train_accuracy = []\n",
    "    val_accuracy = []\n",
    "    test_accuracy = []\n",
    "    epoch_loss=[0]*num_epochs\n",
    "    loss_crit = nn.CrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        losses = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        model.train()\n",
    "        for batch_index, (inputs, targets) in enumerate(trainLoader):\n",
    "            x, targets = Variable(inputs.view([-1,model.insize])), Variable(targets)\n",
    "\n",
    "            logits = model.forward(x)\n",
    "            _, preds = model.prediction(logits)\n",
    "            correct += preds.eq(targets.data).sum()\n",
    "            total += targets.size(0)\n",
    "            loss = loss_crit(logits,targets)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.data[0])\n",
    "        epoch_loss[epoch] = np.mean(losses)\n",
    "        train_accuracy.append(correct/float(total))\n",
    "        \n",
    "        if (valLoader != None):\n",
    "            model.eval()\n",
    "            for batch_index,(inputs,targets) in enumerate(valLoader):\n",
    "                x, targets = Variable(inputs.view([-1,model.insize])), Variable(targets)\n",
    "\n",
    "                logits = model.forward(x)\n",
    "                _, preds = model.prediction(logits)\n",
    "                val_correct += preds.eq(targets.data).sum()\n",
    "                val_total += targets.size(0)\n",
    "            val_acc = (val_correct/float(val_total))\n",
    "            val_accuracy.append(val_acc)\n",
    "                            \n",
    "            if val_acc > bestValAcc:\n",
    "                bestNetwork = model\n",
    "                bestValAcc= val_acc\n",
    "                \n",
    "                \n",
    "        if (testLoader != None):\n",
    "            model.eval()\n",
    "            for batch_index,(inputs,targets) in enumerate(testLoader):\n",
    "                x, targets = Variable(inputs.view([-1,model.insize])), Variable(targets)\n",
    "\n",
    "                logits = model.forward(x)\n",
    "                _, preds = model.prediction(logits)\n",
    "                test_correct += preds.eq(targets.data).sum()\n",
    "                test_total += targets.size(0)\n",
    "            test_acc = (test_correct/float(test_total))\n",
    "            test_accuracy.append(test_acc)\n",
    "            \n",
    "        if (valLoader != None and testLoader != None):\n",
    "            print('Epoch : %d Loss : %.3f  Train Accuracy: %.3f Validation Accuracy: %.3f Test Accuracy: %.3f' % (epoch+1,epoch_loss[epoch],\\\n",
    "                                                                                                                  train_accuracy[epoch], val_accuracy[epoch], test_accuracy[epoch]))\n",
    "        elif (valLoader != None and testLoader == None):\n",
    "            print('Epoch : %d Loss : %.3f  Train Accuracy: %.3f Validation Accuracy: %.3f' % (epoch+1,epoch_loss[epoch],\\\n",
    "                                                                                              train_accuracy[epoch], val_accuracy[epoch]))\n",
    "        else:\n",
    "            print('Epoch : %d Loss : %.3f  Train Accuracy: %.3f' %(epoch+1, epoch_loss[epoch], train_accuracy[epoch]))\n",
    "    return (bestNetwork, epoch_loss, train_accuracy, val_accuracy, test_accuracy)\n",
    "    \n",
    "\n",
    "            \n",
    "def hyperparamsearch(num_epochs, hparams, trainLoader, valLoader, testLoader = None):\n",
    "    bestNetwork = 0\n",
    "    bestValAcc = 0\n",
    "    for layer in hparams['layers']:\n",
    "        for lr in hparams['lr']:\n",
    "            for decay in hparams['lr_decay']:\n",
    "                print \"Neural Net Hyper params:\"\n",
    "                print \"Layers: \" + repr(layer)\n",
    "                print \"Learning Rate: \" + repr(lr)\n",
    "                print \"Learning rate decay: \" + repr(decay)\n",
    "                epoch_loss = [0] * num_epochs\n",
    "                NeuralNet = MLP_MNIST(layer,10)\n",
    "                NeuralNet = GlorotInitialize(NeuralNet)\n",
    "                optimizer = torch.optim.Adagrad(NeuralNet.parameters(), lr=lr,lr_decay = decay)\n",
    "\n",
    "                    \n",
    "                network, loss_history, train_accuracy, val_accuracy,test_accuracy = train(NeuralNet,num_epochs, trainLoader,optimizer,valLoader)\n",
    "                if (max(val_accuracy) > bestValAcc):\n",
    "                    bestNetwork = network\n",
    "                    bestValAcc = max(val_accuracy)\n",
    "                plt.plot(train_accuracy)\n",
    "                plt.plot(val_accuracy)\n",
    "                if (testLoader != None):\n",
    "                    plt.plot(test_accuracy)\n",
    "                plt.show()\n",
    "                    \n",
    "    return bestNetwork, bestValAcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Net Hyper params:\n",
      "Layers: [784, 500, 250]\n",
      "Learning Rate: 0.001\n",
      "Learning rate decay: 0.0001\n",
      "Epoch : 1 Loss : 0.261  Train Accuracy: 0.924 Validation Accuracy: 0.963\n",
      "Epoch : 2 Loss : 0.102  Train Accuracy: 0.968 Validation Accuracy: 0.971\n",
      "Epoch : 3 Loss : 0.063  Train Accuracy: 0.980 Validation Accuracy: 0.973\n",
      "Epoch : 4 Loss : 0.045  Train Accuracy: 0.986 Validation Accuracy: 0.973\n",
      "Epoch : 5 Loss : 0.029  Train Accuracy: 0.991 Validation Accuracy: 0.976\n",
      "Epoch : 6 Loss : 0.020  Train Accuracy: 0.993 Validation Accuracy: 0.976\n",
      "Epoch : 7 Loss : 0.015  Train Accuracy: 0.996 Validation Accuracy: 0.974\n",
      "Epoch : 8 Loss : 0.010  Train Accuracy: 0.997 Validation Accuracy: 0.977\n",
      "Epoch : 9 Loss : 0.007  Train Accuracy: 0.998 Validation Accuracy: 0.978\n",
      "Epoch : 10 Loss : 0.005  Train Accuracy: 0.998 Validation Accuracy: 0.979\n",
      "Epoch : 11 Loss : 0.004  Train Accuracy: 0.999 Validation Accuracy: 0.978\n",
      "Epoch : 12 Loss : 0.002  Train Accuracy: 0.999 Validation Accuracy: 0.977\n",
      "Epoch : 13 Loss : 0.002  Train Accuracy: 0.999 Validation Accuracy: 0.979\n",
      "Epoch : 14 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 15 Loss : 0.002  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 16 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 17 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 18 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 19 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 20 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 21 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 22 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 23 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980\n",
      "Epoch : 24 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 25 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980\n",
      "Epoch : 26 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980\n",
      "Epoch : 27 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 28 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980\n",
      "Epoch : 29 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 30 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 31 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980\n",
      "Epoch : 32 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980\n",
      "Epoch : 33 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 34 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 35 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 36 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 37 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 38 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 39 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 40 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 41 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 42 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 43 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 44 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 45 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 46 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 47 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 48 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 49 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 50 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n//OTVbnLAspVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcf7dacf8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Net Hyper params:\n",
      "Layers: [784, 500, 250]\n",
      "Learning Rate: 0.0001\n",
      "Learning rate decay: 0.0001\n",
      "Epoch : 1 Loss : 0.930  Train Accuracy: 0.790 Validation Accuracy: 0.886\n",
      "Epoch : 2 Loss : 0.405  Train Accuracy: 0.886 Validation Accuracy: 0.909\n",
      "Epoch : 3 Loss : 0.330  Train Accuracy: 0.904 Validation Accuracy: 0.920\n",
      "Epoch : 4 Loss : 0.297  Train Accuracy: 0.914 Validation Accuracy: 0.925\n",
      "Epoch : 5 Loss : 0.273  Train Accuracy: 0.921 Validation Accuracy: 0.930\n",
      "Epoch : 6 Loss : 0.256  Train Accuracy: 0.926 Validation Accuracy: 0.934\n",
      "Epoch : 7 Loss : 0.241  Train Accuracy: 0.931 Validation Accuracy: 0.937\n",
      "Epoch : 8 Loss : 0.229  Train Accuracy: 0.934 Validation Accuracy: 0.939\n",
      "Epoch : 9 Loss : 0.219  Train Accuracy: 0.937 Validation Accuracy: 0.941\n",
      "Epoch : 10 Loss : 0.210  Train Accuracy: 0.940 Validation Accuracy: 0.943\n",
      "Epoch : 11 Loss : 0.203  Train Accuracy: 0.942 Validation Accuracy: 0.945\n",
      "Epoch : 12 Loss : 0.196  Train Accuracy: 0.944 Validation Accuracy: 0.946\n",
      "Epoch : 13 Loss : 0.190  Train Accuracy: 0.946 Validation Accuracy: 0.948\n",
      "Epoch : 14 Loss : 0.184  Train Accuracy: 0.947 Validation Accuracy: 0.949\n",
      "Epoch : 15 Loss : 0.179  Train Accuracy: 0.949 Validation Accuracy: 0.950\n",
      "Epoch : 16 Loss : 0.175  Train Accuracy: 0.950 Validation Accuracy: 0.952\n",
      "Epoch : 17 Loss : 0.170  Train Accuracy: 0.952 Validation Accuracy: 0.953\n",
      "Epoch : 18 Loss : 0.166  Train Accuracy: 0.953 Validation Accuracy: 0.954\n",
      "Epoch : 19 Loss : 0.163  Train Accuracy: 0.954 Validation Accuracy: 0.953\n",
      "Epoch : 20 Loss : 0.159  Train Accuracy: 0.955 Validation Accuracy: 0.955\n",
      "Epoch : 21 Loss : 0.156  Train Accuracy: 0.956 Validation Accuracy: 0.955\n",
      "Epoch : 22 Loss : 0.153  Train Accuracy: 0.957 Validation Accuracy: 0.955\n",
      "Epoch : 23 Loss : 0.150  Train Accuracy: 0.957 Validation Accuracy: 0.956\n",
      "Epoch : 24 Loss : 0.147  Train Accuracy: 0.959 Validation Accuracy: 0.957\n",
      "Epoch : 25 Loss : 0.144  Train Accuracy: 0.959 Validation Accuracy: 0.956\n",
      "Epoch : 26 Loss : 0.142  Train Accuracy: 0.959 Validation Accuracy: 0.957\n",
      "Epoch : 27 Loss : 0.140  Train Accuracy: 0.960 Validation Accuracy: 0.958\n",
      "Epoch : 28 Loss : 0.137  Train Accuracy: 0.961 Validation Accuracy: 0.958\n",
      "Epoch : 29 Loss : 0.137  Train Accuracy: 0.961 Validation Accuracy: 0.959\n",
      "Epoch : 30 Loss : 0.133  Train Accuracy: 0.962 Validation Accuracy: 0.959\n",
      "Epoch : 31 Loss : 0.132  Train Accuracy: 0.962 Validation Accuracy: 0.959\n",
      "Epoch : 32 Loss : 0.130  Train Accuracy: 0.962 Validation Accuracy: 0.960\n",
      "Epoch : 33 Loss : 0.128  Train Accuracy: 0.963 Validation Accuracy: 0.960\n",
      "Epoch : 34 Loss : 0.127  Train Accuracy: 0.963 Validation Accuracy: 0.960\n",
      "Epoch : 35 Loss : 0.125  Train Accuracy: 0.964 Validation Accuracy: 0.960\n",
      "Epoch : 36 Loss : 0.124  Train Accuracy: 0.964 Validation Accuracy: 0.960\n",
      "Epoch : 37 Loss : 0.122  Train Accuracy: 0.965 Validation Accuracy: 0.961\n",
      "Epoch : 38 Loss : 0.121  Train Accuracy: 0.965 Validation Accuracy: 0.961\n",
      "Epoch : 39 Loss : 0.119  Train Accuracy: 0.965 Validation Accuracy: 0.961\n",
      "Epoch : 40 Loss : 0.118  Train Accuracy: 0.966 Validation Accuracy: 0.961\n",
      "Epoch : 41 Loss : 0.117  Train Accuracy: 0.966 Validation Accuracy: 0.962\n",
      "Epoch : 42 Loss : 0.116  Train Accuracy: 0.966 Validation Accuracy: 0.962\n",
      "Epoch : 43 Loss : 0.115  Train Accuracy: 0.967 Validation Accuracy: 0.962\n",
      "Epoch : 44 Loss : 0.114  Train Accuracy: 0.967 Validation Accuracy: 0.962\n",
      "Epoch : 45 Loss : 0.113  Train Accuracy: 0.967 Validation Accuracy: 0.962\n",
      "Epoch : 46 Loss : 0.112  Train Accuracy: 0.968 Validation Accuracy: 0.963\n",
      "Epoch : 47 Loss : 0.111  Train Accuracy: 0.968 Validation Accuracy: 0.963\n",
      "Epoch : 48 Loss : 0.110  Train Accuracy: 0.968 Validation Accuracy: 0.963\n",
      "Epoch : 49 Loss : 0.109  Train Accuracy: 0.969 Validation Accuracy: 0.963\n",
      "Epoch : 50 Loss : 0.108  Train Accuracy: 0.969 Validation Accuracy: 0.963\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcf73f4a0d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Net Hyper params:\n",
      "Layers: [784, 500, 250]\n",
      "Learning Rate: 1e-05\n",
      "Learning rate decay: 0.0001\n",
      "Epoch : 1 Loss : 2.125  Train Accuracy: 0.273 Validation Accuracy: 0.431\n",
      "Epoch : 2 Loss : 1.970  Train Accuracy: 0.504 Validation Accuracy: 0.595\n",
      "Epoch : 3 Loss : 1.876  Train Accuracy: 0.609 Validation Accuracy: 0.667\n",
      "Epoch : 4 Loss : 1.804  Train Accuracy: 0.662 Validation Accuracy: 0.704\n",
      "Epoch : 5 Loss : 1.745  Train Accuracy: 0.690 Validation Accuracy: 0.729\n",
      "Epoch : 6 Loss : 1.695  Train Accuracy: 0.709 Validation Accuracy: 0.744\n",
      "Epoch : 7 Loss : 1.652  Train Accuracy: 0.723 Validation Accuracy: 0.755\n",
      "Epoch : 8 Loss : 1.613  Train Accuracy: 0.734 Validation Accuracy: 0.764\n",
      "Epoch : 9 Loss : 1.579  Train Accuracy: 0.743 Validation Accuracy: 0.772\n",
      "Epoch : 10 Loss : 1.549  Train Accuracy: 0.750 Validation Accuracy: 0.777\n",
      "Epoch : 11 Loss : 1.521  Train Accuracy: 0.755 Validation Accuracy: 0.783\n",
      "Epoch : 12 Loss : 1.496  Train Accuracy: 0.760 Validation Accuracy: 0.788\n",
      "Epoch : 13 Loss : 1.473  Train Accuracy: 0.764 Validation Accuracy: 0.791\n",
      "Epoch : 14 Loss : 1.451  Train Accuracy: 0.768 Validation Accuracy: 0.795\n",
      "Epoch : 15 Loss : 1.432  Train Accuracy: 0.771 Validation Accuracy: 0.798\n",
      "Epoch : 16 Loss : 1.414  Train Accuracy: 0.773 Validation Accuracy: 0.802\n",
      "Epoch : 17 Loss : 1.397  Train Accuracy: 0.776 Validation Accuracy: 0.804\n",
      "Epoch : 18 Loss : 1.381  Train Accuracy: 0.778 Validation Accuracy: 0.807\n",
      "Epoch : 19 Loss : 1.366  Train Accuracy: 0.780 Validation Accuracy: 0.808\n",
      "Epoch : 20 Loss : 1.352  Train Accuracy: 0.782 Validation Accuracy: 0.810\n",
      "Epoch : 21 Loss : 1.339  Train Accuracy: 0.784 Validation Accuracy: 0.811\n",
      "Epoch : 22 Loss : 1.327  Train Accuracy: 0.785 Validation Accuracy: 0.813\n",
      "Epoch : 23 Loss : 1.315  Train Accuracy: 0.787 Validation Accuracy: 0.814\n",
      "Epoch : 24 Loss : 1.304  Train Accuracy: 0.788 Validation Accuracy: 0.815\n",
      "Epoch : 25 Loss : 1.293  Train Accuracy: 0.789 Validation Accuracy: 0.815\n",
      "Epoch : 26 Loss : 1.283  Train Accuracy: 0.790 Validation Accuracy: 0.815\n",
      "Epoch : 27 Loss : 1.274  Train Accuracy: 0.791 Validation Accuracy: 0.816\n",
      "Epoch : 28 Loss : 1.265  Train Accuracy: 0.792 Validation Accuracy: 0.817\n",
      "Epoch : 29 Loss : 1.256  Train Accuracy: 0.794 Validation Accuracy: 0.818\n",
      "Epoch : 30 Loss : 1.248  Train Accuracy: 0.794 Validation Accuracy: 0.819\n",
      "Epoch : 31 Loss : 1.240  Train Accuracy: 0.795 Validation Accuracy: 0.820\n",
      "Epoch : 32 Loss : 1.233  Train Accuracy: 0.796 Validation Accuracy: 0.820\n",
      "Epoch : 33 Loss : 1.225  Train Accuracy: 0.797 Validation Accuracy: 0.821\n",
      "Epoch : 34 Loss : 1.219  Train Accuracy: 0.797 Validation Accuracy: 0.822\n",
      "Epoch : 35 Loss : 1.212  Train Accuracy: 0.798 Validation Accuracy: 0.823\n",
      "Epoch : 36 Loss : 1.205  Train Accuracy: 0.799 Validation Accuracy: 0.824\n",
      "Epoch : 37 Loss : 1.199  Train Accuracy: 0.800 Validation Accuracy: 0.824\n",
      "Epoch : 38 Loss : 1.193  Train Accuracy: 0.801 Validation Accuracy: 0.824\n",
      "Epoch : 39 Loss : 1.188  Train Accuracy: 0.801 Validation Accuracy: 0.825\n",
      "Epoch : 40 Loss : 1.182  Train Accuracy: 0.802 Validation Accuracy: 0.826\n",
      "Epoch : 41 Loss : 1.177  Train Accuracy: 0.802 Validation Accuracy: 0.827\n",
      "Epoch : 42 Loss : 1.172  Train Accuracy: 0.803 Validation Accuracy: 0.828\n",
      "Epoch : 43 Loss : 1.167  Train Accuracy: 0.803 Validation Accuracy: 0.828\n",
      "Epoch : 44 Loss : 1.162  Train Accuracy: 0.804 Validation Accuracy: 0.829\n",
      "Epoch : 45 Loss : 1.157  Train Accuracy: 0.804 Validation Accuracy: 0.830\n",
      "Epoch : 46 Loss : 1.153  Train Accuracy: 0.805 Validation Accuracy: 0.830\n",
      "Epoch : 47 Loss : 1.149  Train Accuracy: 0.805 Validation Accuracy: 0.830\n",
      "Epoch : 48 Loss : 1.144  Train Accuracy: 0.805 Validation Accuracy: 0.830\n",
      "Epoch : 49 Loss : 1.140  Train Accuracy: 0.806 Validation Accuracy: 0.831\n",
      "Epoch : 50 Loss : 1.136  Train Accuracy: 0.806 Validation Accuracy: 0.831\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl4XHd97/H3V7Noty1Zsh3vexInTdLg7OklBAIJSwIU\nLklaoNsNKQ2X9naB0odCoX0e2kBbbhvwzYVcaKGE3AuBEFxSSBuymcQOpImdxInsxLst2ZKsZSTN\n9r1/nDPSaCxZY3tkeWY+r+c5z1nmaPQ7GH/y8/f8zu+YuyMiIpWlZqYbICIipadwFxGpQAp3EZEK\npHAXEalACncRkQqkcBcRqUAKdxGRCqRwFxGpQAp3EZEKFJ2pX9zW1ubLly+fqV8vIlKWnnnmmcPu\n3j7VeTMW7suXL2fLli0z9etFRMqSme0q5jyVZUREKpDCXUSkAincRUQqkMJdRKQCKdxFRCqQwl1E\npAIp3EVEKtCMjXMXEZkx2SwwwStG3cEzkElCJhUuyWDJpoP9bHr8kklBNhPu5z7PhMdTed+VHNte\nejmsunZaL1HhLiJnDndID0NyEJID4TpvSQ1BZiQ4J50Mt5PBfmoIUuE5qaGx81OJvHW4nR6e2eu8\n+g8U7iJyBshmC3qhBT3RTBiwI/1BKI8MhOv+IGRz4Vu4zoX4yAAk+4O1Z068fTUxiDVArB7iDWPb\n0TqoXxju530erYeayCTfFQm+LxKHSG4dD45HYsFnNVGIRIN1TTQ8Fhnbj4T7uZ/NfU9NLNg2O7U/\njyIo3EUqSa7nmx+WuR5wKgHJxPiebHIwCODhXhg+On5JDYfliLDUcLIsEgZrXRCqsTqI1gbbdbNg\n1kKobYZ4E9Q2hetmiDcGPxdvCrZzoR2JB6EdDdeRWqjR7cNCCneRmZAeGR+k40oIeeGbzpUgRsLe\n8UiwpBJjPeNxveSBEwvimlgQsHWzw2UOzFoUHIs1FPRco8f2ZvP3o7UTBHRTcPw09FRlPIW7CIQ3\n2AhCqDCIspmC3m64nRwYKymM9OWVFwYLesuDY8dyYX4iNV+rCXuoYYBGaoPyQm1zEKRN88Pt/N5v\n+FnuWLwp7PnWQ6wxXNcHwSwVSeEu5SubDcoJQz2Q6Iah7rH1SH9BjXcE0uGNtpFcLzev13tM2OZC\n3k6sBmw1eWWEcIk1QsNcmLM06B3XzxnfU66dFZ5XP1YXzq8ZR/TXVE6c/l8jMy+bDQJ5oDMI6qGe\nsdAe6oGh3oJjuf2jTDicLScSP7bGm+vxzl58bH0XAw+HyHk2qF/jYz3lcTfq6oOfKewxxxpUgpAz\ngsJdpk82C4kj0H8A+g+G69xyCAbylsnqxBYJe7stwdIwF+auDnq89XOgvhUaWvPWLcG6dtbkoyFE\nqkBR4W5m1wNfBCLAV9z9cwWfzwa+ASwNv/Pz7v5/StxWOdNkM9C3H3p3Fyy7gqXvQDDSolBDGzQv\nCGrF886FpnnQtCBYN7SGwR2GeW2zesJSMpmsk0imSSQzDKcyJNNZRsIl2M6QyTpZh6x78EyTB/vp\nbJZ0xklns6QyTjqTJZ310e1UJksq66TSY9vZrI/7vmDbuW7dfG66aNG0XuuU4W5mEeAu4DpgL7DZ\nzB5w9xfyTvs94AV3f4eZtQPbzeyb7p6cllbL6eMO3Tth38+h59UwuHdDzy7o23dsj7v5rKC2vOSy\nYNTFrIXBseazxgI9Gp+Za5EZ4+6MpLMMpzIMpTIMJTMMp4IwTWWcZDpLMpMJ1+F+OksynSGZCcJ0\nJAzfkfDnhvPWyXSWdDZLJuukw0DNBXEimQmXNMOp7LReZzxSQyxiRMN1pMaosXCpgYgZNTXGhYvn\nTGs7oLie+6VAh7vvBDCze4GbgPxwd6DZzAxoArqBUxgYKzNmZAD2/xz2PA17NwdL4sjY500LgvBe\nfAnM+dVgu2UZzFkWhHmsbubaLicsk3UGk2kSIxkGk2kGR9IMjgRBOJjMMBT2chPJIJATyQzD6Qyp\ndDYM3WxeIGcYSmUZSQW94uFUluF0sD2Szga3ME5RPFJDbayGuliEulgNtdFgHY/UhIFaQ13MiNYE\nwRqpMRrjURpqI8E6HqWxNkJ9PEJdNEI8WkNttIZ4uNRGg+8wDDNGQzkX0KPBXROsoxEjVjMW5HYG\n/SuzmHBfBOzJ298LXFZwzj8CDwD7gWbgfe5+zH8izew24DaApUuXnkx7pVSy2aAXfmgbdL4QrA9t\ng+4d4U1FoG0trL0BllwShHnrKoX3DMtknaFUhsRImoEwiIN1OgznzGjZIT+0E+F5/SNp+odTDAwH\nP59IFj8SqMagIR4dDdNYGIS57dpIDbPqotQ11wbhGx0L4fpYhLowUOvjkWA/DOdYZCxc45Ea4lEj\nHomMHotFbPSzMyk8z3SluqH6FuBZ4FpgFfBjM3vM3fvyT3L3u4G7AdavX1+C/45L0RLdQW98z1PB\n+sCzwRDAnJblMP98OP/dsGg9LF4f1L/lpCXTWfqGUxwdCpb+4TSJkfRoiWAwmRnbT2UYToYli9Gy\nRWZcSSGRDHrAxYpHa2iMR2iIR2mIR2iuizK7PsbiOfU010Vpqo3SWBuluW6sR5vr5TbEozTGgyDO\n/XxtVOFaTooJ933Akrz9xeGxfL8JfM7dHegws1eBc4CnS9JKOTHpkaA3fuA/g7LKnqfh8MvBZzVR\nOOtCuOjWIMznnwft5wTD+OQYqUyWnkSSnsEURwZH6BlM0T04Qvdgir68HnD/SJqB4RQDI2n6htIc\nHUoxlCquV9wQj9AQj1AXC3q0ue2WxjiLWiLUx6Kj59SH64b4WDg31kbGtnPhHIsQjeiR/GpWTLhv\nBtaY2QqCUL8ZuLXgnN3AG4HHzGw+cDaws5QNlUlk0rD/F0Gd/MBzQaB3vTh2o7O+Jbi5eeHNsORy\nWPjLwXjtKpLJeliCSNM/nObIQJLuwSTdgyMcGUzSM5ikO5GibygI7P7hNH1hT/t4Ad1UGwRsU9gL\nbq6LMn9WHU21QQ95dn2M2Q3BelZdjFn1YQ95tHcclClqatQbltKbMtzdPW1mdwAPEQyFvMfdt5nZ\n7eHnG4DPAl8zs+cBAz7m7oensd3Vrec12PHvwbLzURg5GhxvaAt65Wuug7MuCLbnLK+oSZWGU5kw\nmMeWwwMjHB5IcmQgCOsjAyP0DqUYDOvSU42QmFUXpbUxzuz6GM11Mc6aXUdzbRDGzXUxWhpitDbW\n0tIYY264bmmIE1PPWM5gRdXc3X0jsLHg2Ia87f3Am0vbNBmVzcBrj8OLP4AdDwdDEwFmLYZ1N8Kq\nN8DSK4LhhmVYEx0cSXOwb5hDR4fpygvqXGgfHhgZDfLJbgDGIsbcxlrmNsVpa6pleVsjjWHPujFX\nTw57160NcVqb4rQ2xhXSUrH0hOqZKpuB3Ztg2/3wwvdhsCt45H3Ff4FLPxRM9N+25owP82Q6y4Gj\nQ+zrGWJvb7De3zvEwb5hDh4d5mDfMP3Dx46ajdbYaFDPbaplVXsTrY3x0aWlIVjPbYrT1ljLrPqo\nbvaJ5FG4n0lSw7D7Sdj+oyDQBw4Ggb72LXDeu2DNm8+4evlwKsPeniH29Q6xtycRhHjefmf/yLjx\nzWYwr7mWBbPrWdneyFWr25g/q44Fs2uZP6uOec21zG2sZXZ9TLVokVOgcJ9J7nBkB3T8JCi3vPpY\nMHNhtC6om5/3LljzlhkfyeLudPWPsKNrkB1dA+zoGmBnuL2vd2hceEdrjIVz6lk0p55fWdPOojn1\nLG6pZ1FLPYvnNLBgdh3xqMogItNN4T4T+g/CU/8Ltn4neJAIgsmwLv4ArH4TLL8qnKXw9Mpknd3d\nCXZ0DtDRNUBHZ7Ds6BoYVzqpj0VYNa+Ri5e28J7XLWb53MYgvFvqmddcR0Q9bpEZp3A/nTpfgk3/\nAM/dFwxVXH0dXPXfYdUboXXFaWuGu3Oob4SXDvbx8qF+th8cYPuhPl45NDDuIZn25lpWtzdx00UL\nWdXexOp5Taxqb2LBrDqVTETOcAr36eYOu56AJ/4nvPJQUEO/+INwxYehdeW0//p0JsuOrkFeOHCU\nF/b3sW1/Hy8c6KM3MTZb47zmWs5e0Mz7L1/G2vnNrJ7fxKq2JmY36C09IuVK4T6ddm2CH/857H06\nGIN+zSfgkt+BxrnT9iuHkhme2dXDpp2H2bTjCFv395EMe+O10RrOWdDMDecv4NyzZnH2/GbWzm+m\npVGzNIpUGoX7dOh6GX7yadj+w2Ds+du+ABf9WvAGnxIbSWf4xe5eNu04wqYdR3h2Ty/JTJZojXHB\n4tl88IplnLdwNusWzmJlW6MeSRepEgr3Uuo/BD/9HDzz9eB1a9d+Ei7/cEmHL2ayztZ9R3liR9Az\n3/xaN8OpLDUG5y+azW9etZzLV83lkuWtNNXqj1ekWulvfymkhoKa+hNfhMwIXPLb8PqPQWNbSb5+\nT3eCR17u4tGXu/jZziOjI1fOnt/MLZcu5YqVc7ls5Vxm16tGLiIBhfup6vgJ/PCPgrcUnXsjvOnT\nMHfVKX3lcCrD069288j2Lh55uZOdXYMALGmt5+0XnMWVq9q4fOVc2ptrT739IlKRFO4nq+8APPQJ\n2PbdYIz6Bx6Ala8/pa98bm8vX39yFz98fj/DqSzxaA2Xr5zLr1+2jGvObmdFW6MesReRoijcT1Q2\nA5u/Cv/+2WDe9Df8GVz1UYieXC96JJ1h4/MH+PqTu3h2Ty+N8Qjvvngx162bz+Ur5lIfj5T4AkSk\nGijcT8SRHfD/fit4i9Gqa+Gtnz/pEsyhvmH+edMuvvX0bo4MJlnZ3shf3Hge7754Ec11qp2LyKlR\nuBfrtcfh278OGLznHjjv3Sc1I2NX/whffmQH33hqF6lMljeeM58PXrmMq1a16alPESkZhXsxfvEN\n+MHvB0+U3vrtk5oqoGcwyYZHd/BPT+5iJJ3h3Rcv5iPXrmbZ3NM/h4yIVD6F+/Fks/Dwp4Mhjivf\nAO/9GtTPOaGvODqU4quP7eSrj79KIpXhxgsX8tE3rmFlu95ZKiLTR+E+meQgfPc2eOlBWP/bcMPf\nQKT4/7kyWefezbu586Ht9CZSvPWXFvD7b1rL2vnN09hoEZGAwn0i/Qfhm++FQ1uDUL/0thOqr/98\ndw+f+v42nt93lEtXtPKpd6zjvIWzp7HBIiLjKdwLZbPBiJgjO+CWb8Pa4l8Ne3hghL/50Uvct2Uv\n82fV8sWbL+LGCxdqbLqInHYK90I/+1IwRe9NXyo62N2db/xsF3c+tJ1EMsOHXr+Sj1y7RnO7iMiM\nUfrk63wRHv4MnP02uOjWon4kkUzzx//3OX74/AGuXt3Gp288j9XzdLNURGaWwj0nnQxuoNY2wzu+\nWFSNfU93gtv++Rm2H+zjE289h//2KytVghGRM4LCPefRO+Hgc/C+b0BT+5Snb9pxhA9/8xnSWeee\n37iEa86edxoaKSJSHIU7wN4t8NgX4MJb4dx3HPdUd+efNu3iMw++wIq2Rv73B9azok0PIonImUXh\nnkzA/R8K3ph0w+eOf2o6yye/t5Vvb9nDm86dx9+97yLNAyMiZ6Si3rlmZteb2XYz6zCzj0/w+R+b\n2bPhstXMMmbWWvrmToOH/wKOdMA774K6yceiZ7LO/7jvWb69ZQ8fuXY1d79/vYJdRM5YU4a7mUWA\nu4AbgHXALWa2Lv8cd7/T3S9y94uAPwV+6u7d09Hgktr5CDy1AS67HVZeM+lp7s4nv7+VB587wJ/e\ncA5/+OazNcmXiJzRium5Xwp0uPtOd08C9wI3Hef8W4BvlaJx0yqbhY1/Aq2r4I2fOu6pf/PQdv7l\nqd18+JpVfOj1p/aWJRGR06GYcF8E7Mnb3xseO4aZNQDXA9859aZNs5d+AIe3wxs+cdwXWG/46Q6+\n/MgOfu2ypfzxW84+jQ0UETl5RdXcT8A7gCcmK8mY2W1mtsXMtnR1dZX4V58Ad3j080Gv/bx3TXra\nt57ezef+9SXeceFCPnPT+RrDLiJlo5hw3wcsydtfHB6byM0cpyTj7ne7+3p3X9/ePvVY8mnT8ZNg\nTPvVfwA1E7/G7sHn9vOJ+5/nmrPb+cJ7LySiGruIlJFiwn0zsMbMVphZnCDAHyg8ycxmA68Hvl/a\nJpaYe/DA0qzFcMH7Jjxl044j/MG3n2X9sha+/GuvIx4t9T9wRESm15Tj3N09bWZ3AA8BEeAed99m\nZreHn28IT30X8G/uPjhtrS2F1x6HPU/BDXdCNH7Mx8OpDB/7znMsaWngKx+8RC+oFpGyVNRDTO6+\nEdhYcGxDwf7XgK+VqmHT5rHPQ+M8uPj9E378pf/oYHd3gn/5ncuYXa9x7CJSnqqr3rD3mWBs+5V3\nQKz+mI93dg2w4ac7eedFC7lyddvpb5+ISIlUV7g/9nmomwPrf+uYj3IPKtXGavizt62b4IdFRMpH\n9YT7wa2wfSNc/rvBtL4FfvDcAZ7oOMKfvOVs2ptrZ6CBIiKlUz3h/tgXIN4cvA+1QN9wis8++AIX\nLJ7NrZctm4HGiYiUVnWE++EO2HY/XPLb0HDsfGZ/+28vc3hghL985/kazy4iFaE6wv3xv4NoLVzx\ne8d8tHXfUf5p02u8//JlXLB4zulvm4jINKj8cE8nYet34MKboWn825IyWefP7n+e1sZa/vDNmjdG\nRCpH5Yf7/l9AeghWv+mYj7719G7+c+9RPvn2czWmXUQqSuWH+64ngvXSK8cddnfuefxVLl46hxsv\nXDgDDRMRmT5VEO5PQvu50Dh33OGXDvaz8/Ag7754sWZ7FJGKU9nhnknD7p/BsiuP+eiHzx2gxuD6\n8xfMQMNERKZXZYf7oech2X9MuLs7G58/wOUr59LWpAeWRKTyVHa473oyWBeEe64k89ZfOmsGGiUi\nMv0qP9xbVsCs8TdMVZIRkUpXueGezQbhvuyqcYdVkhGRalC54d71Egx1w/Lx4Z4rybztApVkRKRy\nVW6458a3F9TbcyWZt5ynkoyIVK4KDvcnYdYimDM2y6NKMiJSLSoz3N3DevuVkPeAkkoyIlItKjPc\nu3fCwEGVZESkalVmuI/W268ePZQryVyxSiUZEal8FRruT0JDG7StGT2kB5dEpJpUaLg/cUy9XSUZ\nEakmlRfuvXugd/e4h5dUkhGRalN54b57U7DOu5mqkoyIVJvKC/fXHofa2TD/vNFDKsmISLUpKtzN\n7Hoz225mHWb28UnOucbMnjWzbWb209I28wTsehKWXQE1kdFDP9p2UA8uiUhVmTLczSwC3AXcAKwD\nbjGzdQXnzAG+BNzo7ucB752Gtk5toBOOvDKuJJNIpunoHOCKlXOP84MiIpWlmJ77pUCHu+909yRw\nL3BTwTm3At91990A7t5Z2mYWaXT+9rGbqTs6BwFYM79pJlokIjIjign3RcCevP294bF8a4EWM3vE\nzJ4xsw+UqoEnZNeTEGuAsy4cPfTyoX4A1sxvnpEmiYjMhGgJv+d1wBuBemCTmf3M3V/OP8nMbgNu\nA1i6dGmJfnWeXU/CkkshEhs99ErnAPFIDctaG0r/+0REzlDF9Nz3AUvy9heHx/LtBR5y90F3Pww8\nClxYcA7ufre7r3f39e3t7Sfb5okN9cKhrce8nOOVQ/2sbG8kGqm8gUEiIpMpJvE2A2vMbIWZxYGb\ngQcKzvk+cLWZRc2sAbgMeLG0TZ1Cz6uAw7xzxx1+pXOA1fNUbxeR6jJluLt7GrgDeIggsO9z921m\ndruZ3R6e8yLwI+A54GngK+6+dfqaPYG+A8E6732pQ8kMe3oSrFW9XUSqTFE1d3ffCGwsOLahYP9O\n4M7SNe0E9e8P1s1j4b6jawB3WKOeu4hUmcopRPftB4tA07zRQxopIyLVqoLC/QA0Lxj3ZOornQPE\nIsayuRopIyLVpXLCvX8/NI+fGOyVQ/2sbGsippEyIlJlKif1+vbDrIJw7xxgtZ5MFZEqVEHhfgBm\njT04O5TMsLs7wdp5qreLSPWpjHAf6Ydk/7iyzOhIGfXcRaQKVUa4TzDG/ZXOYKTMWoW7iFShCgn3\ncDaEvJ77K4dyI2UaZ6hRIiIzpzLCvf/YnvvLhwZY0daokTIiUpUqI/n6wqdT88K9o7OfNbqZKiJV\nqnLCvW4OxOoBGE5l2NWd0M1UEalalRHu/eOHQY7NKaOeu4hUp8oI94IHmF45NABopIyIVK/KCff8\nkTKd/URrNFJGRKpX+Yd7JgWDXePKMrmRMvFo+V+eiMjJKP/06z8I+LiyTEfngG6mikhVq4BwD8e4\nhy/pGE5l2HVkkNW6mSoiVaz8wz33dGo4xn1n1yBZ181UEaluFRDu459Ozc0po2GQIlLNyj/c+/dD\npBbqW4BgGGSkxljRppEyIlK9yj/cc2PczYDgvanL5zZopIyIVLXyT8CCl3R0dA6wVi/EFpEqV/7h\nnvfu1OFUhteODLJmnm6mikh1K+9wdw977kG4v3o4GCmzRj13Ealy5R3uiW7IjIyWZV4+FI6U0TBI\nEaly5R3u/eE87mFZpqNTI2VERKDcw73gJR0vH+pn2dwGaqORGWyUiMjMKyrczex6M9tuZh1m9vEJ\nPr/GzI6a2bPh8uelb+oECsL9lc4B1urhJRERolOdYGYR4C7gOmAvsNnMHnD3FwpOfczd3z4NbZxc\n/wHAoGk+I+kMu44keNsvnTXlj4mIVLpieu6XAh3uvtPdk8C9wE3T26wi9e2HpnkQibG/d5hM1lmu\nOdxFRIoK90XAnrz9veGxQlea2XNm9q9mdl5JWjeVvv2jJZnuwSQArU3x0/KrRUTOZKW6ofpzYKm7\nXwD8A/C9iU4ys9vMbIuZbenq6jr139p/YHSq395EGO4NCncRkWLCfR+wJG9/cXhslLv3uftAuL0R\niJlZW+EXufvd7r7e3de3t7efQrNDee9OzfXcWxTuIiJFhftmYI2ZrTCzOHAz8ED+CWa2wCyYucvM\nLg2/90ipGztOMgHDvaNj3HsTKQBaGmPT+mtFRMrBlKNl3D1tZncADwER4B5332Zmt4efbwDeA/yu\nmaWBIeBmd/dpbPfYG5jCp1O7E0miNUZT7ZSXJCJS8YpKwrDUsrHg2Ia87X8E/rG0TZvC6Bj3XM89\nSUtjnPAfECIiVa18n1DNhXt4Q7VnMEVLg0oyIiJQzuHeP77n3p1IMkc3U0VEgHIO974DUDsLaoPp\nBnoTSQ2DFBEJlW+4572kA6B7MKWRMiIiofIN97ynU909uKGqnruICFDW4X5gNNz7R9Kks65wFxEJ\nlWe4Z9IwcGjsAabB3ANMCncRESjXcB/sBM+MTRqWyE09oJq7iAiUa7j35Z5ODce458JdPXcREaBc\nw73g3ak9mjRMRGSc8gz3gtfr9YSThmmcu4hIoHzDvSYGDcGswj2DSWoMmus0aZiICJRruPcfCEoy\nNUHze8Ix7jU1mjRMRATKNdzzXtIBQbjP0UgZEZFRZRzuC0d3ewZTtGqkjIjIqPILd/dx706FXM9d\n4S4iklN+4T58FFKJY8oyeoBJRGRM+YV7wTBId6cnkdIDTCIiecov3PvHv4EpkcyQTGf1AJOISJ7y\nC3cH2s6G2cGLsXNTD+gBJhGRMeX31M+aNwVLqCecEVJDIUVExpRfz73AaM9dNXcRkVEVE+4aCiki\nMqb8w31QPXcRkUJlH+7diRRmMLteNXcRkZyyD/feRJLZ9TEimjRMRGRUUeFuZteb2XYz6zCzjx/n\nvEvMLG1m7yldE4+vezCpMe4iIgWmDHcziwB3ATcA64BbzGzdJOf9NfBvpW7k8fQmUpp6QESkQDE9\n90uBDnff6e5J4F7gpgnO+wjwHaCzhO2bknruIiLHKibcFwF78vb3hsdGmdki4F3Al0vXtOL0akZI\nEZFjlOqG6t8DH3P37PFOMrPbzGyLmW3p6uoqyS/uTiRpbVRZRkQkXzHTD+wDluTtLw6P5VsP3Gtm\nAG3AW80s7e7fyz/J3e8G7gZYv369n2yjc4ZTGYZTWfXcRUQKFBPum4E1ZraCINRvBm7NP8HdV+S2\nzexrwIOFwT4dNPWAiMjEpgx3d0+b2R3AQ0AEuMfdt5nZ7eHnG6a5jZPqDp9O1WgZEZHxipoV0t03\nAhsLjk0Y6u7+G6ferOL0JoIZITVaRkRkvLJ+QnW0566yjIjIOGUd7r2JXFlG4S4ikq+sw71bL+oQ\nEZlQWYd7TyJJc12UWKSsL0NEpOTKOhV7Epp6QERkImUe7indTBURmUB5h/tgUmPcRUQmUN7hnkjS\nqrKMiMgxyjvcBzUjpIjIRMo23EfSGQaTGZVlREQmULbhPjr1gG6oiogco2zDvUdPp4qITKpsw31s\nXhmVZURECpVtuGtGSBGRyZVtuOtFHSIikyvfcA/LMpo0TETkWOUb7okUjfEItdHITDdFROSMU77h\nrgeYREQmVb7hnkiq3i4iMomyDffuREr1dhGRSZRtuPeq5y4iMqmyDffuQb2oQ0RkMmUZ7qlMlv7h\ntMJdRGQSZRnuY5OGqeYuIjKRMg333ANM6rmLiEykLMM9N2mY3sIkIjKxsgz3nrAso6GQIiITKyrc\nzex6M9tuZh1m9vEJPr/JzJ4zs2fNbIuZXV36po7RpGEiIscXneoEM4sAdwHXAXuBzWb2gLu/kHfa\nw8AD7u5mdgFwH3DOdDQY9KIOEZGpFNNzvxTocPed7p4E7gVuyj/B3Qfc3cPdRsCZRj2DSepiNdTH\nNWmYiMhEign3RcCevP294bFxzOxdZvYS8EPgtyb6IjO7LSzbbOnq6jqZ9gJBzV29dhGRyZXshqq7\n3+/u5wDvBD47yTl3u/t6d1/f3t5+0r+rN6GnU0VEjqeYcN8HLMnbXxwem5C7PwqsNLO2U2zbpLoH\nk3qASUTkOIoJ983AGjNbYWZx4GbggfwTzGy1mVm4fTFQCxwpdWNzelWWERE5rilHy7h72szuAB4C\nIsA97r7NzG4PP98A/CrwATNLAUPA+/JusJZct8oyIiLHNWW4A7j7RmBjwbENedt/Dfx1aZs2sUzW\nOTqUokVj3EVEJlV2T6geHUrhDi16OlVEZFJlF+56OlVEZGrlF+6DmhFSRGQq5RfuubncVZYREZlU\n2YV7a2Nhe9u5AAADw0lEQVSMG85fwPxZdTPdFBGRM1ZRo2XOJK9b1srrlrXOdDNERM5oZddzFxGR\nqSncRUQqkMJdRKQCKdxFRCqQwl1EpAIp3EVEKpDCXUSkAincRUQqkE3jtOvH/8VmXcCuk/zxNuBw\nCZtTTqr12nXd1UXXPbll7j7le0pnLNxPhZltcff1M92OmVCt167rri667lOnsoyISAVSuIuIVKBy\nDfe7Z7oBM6har13XXV103aeoLGvuIiJyfOXacxcRkeMou3A3s+vNbLuZdZjZx2e6PdPFzO4xs04z\n25p3rNXMfmxmr4Trlpls43QwsyVm9h9m9oKZbTOzj4bHK/razazOzJ42s/8Mr/svwuMVfd05ZhYx\ns1+Y2YPhfsVft5m9ZmbPm9mzZrYlPFay6y6rcDezCHAXcAOwDrjFzNbNbKumzdeA6wuOfRx42N3X\nAA+H+5UmDfyhu68DLgd+L/wzrvRrHwGudfcLgYuA683scir/unM+CryYt18t1/0Gd78ob/hjya67\nrMIduBTocPed7p4E7gVumuE2TQt3fxToLjh8E/D1cPvrwDtPa6NOA3c/4O4/D7f7Cf7CL6LCr90D\nA+FuLFycCr9uADNbDLwN+Ere4Yq/7kmU7LrLLdwXAXvy9veGx6rFfHc/EG4fBObPZGOmm5ktB34Z\neIoquPawNPEs0An82N2r4rqBvwf+BMjmHauG63bgJ2b2jJndFh4r2XWX3TtUJeDubmYVO9TJzJqA\n7wC/7+59Zjb6WaVeu7tngIvMbA5wv5mdX/B5xV23mb0d6HT3Z8zsmonOqcTrDl3t7vvMbB7wYzN7\nKf/DU73ucuu57wOW5O0vDo9Vi0NmdhZAuO6c4fZMCzOLEQT7N939u+Hhqrh2AHfvBf6D4J5LpV/3\nVcCNZvYaQZn1WjP7BpV/3bj7vnDdCdxPUHYu2XWXW7hvBtaY2QoziwM3Aw/McJtOpweAD4bbHwS+\nP4NtmRYWdNG/Crzo7n+b91FFX7uZtYc9dsysHrgOeIkKv253/1N3X+zuywn+Pv+7u/86FX7dZtZo\nZs25beDNwFZKeN1l9xCTmb2VoEYXAe5x97+a4SZNCzP7FnANwSxxh4BPAd8D7gOWEsyo+V/dvfCm\na1kzs6uBx4DnGavBfoKg7l6x125mFxDcQIsQdLruc/fPmNlcKvi684VlmT9y97dX+nWb2UqC3joE\n5fF/cfe/KuV1l124i4jI1MqtLCMiIkVQuIuIVCCFu4hIBVK4i4hUIIW7iEgFUriLiFQghbuISAVS\nuIuIVKD/Dylq9n6kh7TyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcf7da93a10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Net Hyper params:\n",
      "Layers: [784, 500, 500]\n",
      "Learning Rate: 0.001\n",
      "Learning rate decay: 0.0001\n",
      "Epoch : 1 Loss : 0.242  Train Accuracy: 0.927 Validation Accuracy: 0.957\n",
      "Epoch : 2 Loss : 0.093  Train Accuracy: 0.972 Validation Accuracy: 0.972\n",
      "Epoch : 3 Loss : 0.052  Train Accuracy: 0.984 Validation Accuracy: 0.974\n",
      "Epoch : 4 Loss : 0.034  Train Accuracy: 0.989 Validation Accuracy: 0.975\n",
      "Epoch : 5 Loss : 0.023  Train Accuracy: 0.992 Validation Accuracy: 0.975\n",
      "Epoch : 6 Loss : 0.015  Train Accuracy: 0.995 Validation Accuracy: 0.979\n",
      "Epoch : 7 Loss : 0.011  Train Accuracy: 0.997 Validation Accuracy: 0.978\n",
      "Epoch : 8 Loss : 0.007  Train Accuracy: 0.997 Validation Accuracy: 0.978\n",
      "Epoch : 9 Loss : 0.004  Train Accuracy: 0.999 Validation Accuracy: 0.978\n",
      "Epoch : 10 Loss : 0.003  Train Accuracy: 0.999 Validation Accuracy: 0.979\n",
      "Epoch : 11 Loss : 0.002  Train Accuracy: 0.999 Validation Accuracy: 0.978\n",
      "Epoch : 12 Loss : 0.002  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 13 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 14 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 15 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 16 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 17 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980\n",
      "Epoch : 18 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980\n",
      "Epoch : 19 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 20 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 21 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 22 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 23 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 24 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 25 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 26 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 27 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 28 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 29 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 30 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 31 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 32 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 33 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 34 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 35 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 36 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977\n",
      "Epoch : 37 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977\n",
      "Epoch : 38 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977\n",
      "Epoch : 39 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977\n",
      "Epoch : 40 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977\n",
      "Epoch : 41 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977\n",
      "Epoch : 42 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977\n",
      "Epoch : 43 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977\n",
      "Epoch : 44 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 45 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 46 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 47 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977\n",
      "Epoch : 48 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977\n",
      "Epoch : 49 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977\n",
      "Epoch : 50 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XuUXGWd7vHv09W3pHOFhAC5i+ESws2JARUUYdSAjiij\nM8HrMDAsPOJBjx6HwTPLNS7Hw7icGR0XIycjKDOMoIPgoCKIguIFIQmEQCCBJgRyAdIhJJ100peq\n+p0/9k53pdOXSlKdSnY9n7V6VdXeb1W9b3fy7Lfe9629FRGYmVntqKt2BczM7OBy8JuZ1RgHv5lZ\njXHwm5nVGAe/mVmNcfCbmdUYB7+ZWY1x8JuZ1RgHv5lZjamvdgUGMmnSpJg1a1a1q2FmdthYtmzZ\n5oiYXE7ZQzL4Z82axdKlS6tdDTOzw4akF8ot66EeM7Ma4+A3M6sxDn4zsxrj4DczqzEOfjOzGjNs\n8Eu6SdImSU8Osl+S/kVSq6QVkt5Qsm+hpNXpvmsqWXEzM9s/5fT4vwssHGL/BcCc9OcK4FsAknLA\n9en+ucAlkuYeSGXNzOzADbuOPyIelDRriCIXAf8eyTUc/yBpgqRjgFlAa0SsAZB0W1r2qQOttFVP\noRh054t0F4q9t109BTp7inTmC3T2FOjqKdLZUyBfDOrrRH2uLr0VuTqRkygUg3wxyBeL5AvJ/UIx\naMjV0dxQR3NDLv2po7k+R75YZGd3If3J994v9L6HqK+r630/AT2FtH75tK75IvlicdC21dfV0Vhf\nR2MuvU3v54tBV9q2zrRtnT1FChE01IlcTjTU1ZGrEw05obR9PYViXzsLQWGI9zYDGN1Uz5VvO27E\n36cSX+CaCqwrebw+3TbQ9jMHexFJV5B8YmDGjBkVqFbtKhSDV3d08XJ7J1t39iQBuFdYF+noyrOj\nO8+Ozjw7uvLJ4648u3qSMO/K7w65JOi60yCz/SdVuwZ2KJs0pumwCf6KiIjFwGKA+fPnO1362byj\ni4eee5XXdnazq3vPHnZnT5HXOrp5ub2TV9o72bS9q+yAbqqvY0xTPS1N9eltjnHN9TSPbUp63PV1\nvT3vpvocjfV1NOzuEedEY32yvbmhjqaGHM3p/eaGHPV16u3J7+799hSCYkRv7zhXV/JpQKK7UKQz\nPfAk7UsOPvW5OkY35BjdmGN0Uz2jG3OMashRn1PJJ4YiPYXk/YoRfXXN1dGU9uB3fxroL4B8oeTg\nWHKgzNWp7xNI+vtoqk96+H2fXIJ8oUi+GBSLQX2urqSNySeCujqnvh0aKhH8G4DpJY+npdsaBtlu\nZSgWgxUbtvHAqk38avUmVmzYRvTL8vreQKpjwuhGpoxr4k3HHcnR45o5enwzU8Y1c0RL415DF0lY\n19HSVE9Dzgu7DkR9TtTnql0Ls31TieC/C7gqHcM/E9gWES9JagPmSJpNEviLgA9V4P0ybcX6rXz3\n92v59eo2Xu3oRoIzpk/gM398PG87fjJTJ47q7XnWO7TNbD8MG/ySbgXOBSZJWg98kaQ3T0TcANwN\nXAi0AjuBS9N9eUlXAfcCOeCmiFg5Am3IhGUvvMY373+WX61uY2xzPeefeBRvP/EozpkzmSNaGqtd\nPTPLkHJW9VwyzP4APjnIvrtJDgw2iIfXvMo372/lt62bOaKlkc8vPIGPnjWTsc0N1a6amWXUITO5\nW0u68gUeWLWJ7/xuLQ8/v4VJY5r4woUn8eGzZjC60X8SMxtZTpmDJCJ4bN1W7nh0PT9+/CW27erh\n6HHNfPFP5nLJghk0N3iG0MwODgf/CNu0vZPvP7KOOx7bwPObO2huqONdJx/NxW+YxluOO9ITtGZ2\n0Dn4R9Ajz2/hE7cs49WObs563RF84tzjuGDe0R6/N7OqcvCPkNseeZG//e8nmT5xNLdecRbHTxlb\n7SqZmQEO/orLF4p8+adP893fr+Wtx0/mm4vOYPxo9/DN7NDh4K+grTu7uep7j/Hb1s1cfvZsrrng\nRI/hm9khx8FfIa2btnP5zUvZuLWTr37gVP5s/vThn2RmVgUO/gp45pXtfPCGh2jI1XHrFWfyRzOP\nqHaVzMwG5eA/QOu27OSjNz5MU30dt1/5ZmYcObraVTIzG5IHoA9A2/YuPnrjw+zqLvAfl53p0Dez\nw4J7/PupvbOHv/jOI7zS3sUtl5/JCUd7uaaZHR7c498PnT0FLr95Katf3s63PvIG/mjmxGpXycys\nbO7x76N8ochV33uUJWu38PU/P51zTziq2lUyM9sn7vHvg4jgr3/4BL94ehNfeu/JXHT61GpXycxs\nnzn498Gtj6zjh4+u5+rz5/DRN82qdnXMzPaLg79ML7zawZd/+hRnv34SV58/p9rVMTPbbw7+MhSK\nwWd/8Di5OvHVD5xKXZ2qXSUzs/1WVvBLWihptaRWSdcMsH+ipDslrZD0iKR5JfuulvSkpJWSPl3J\nyh8six9cw9IXXuNLF53MsRNGVbs6ZmYHZNjgl5QDrgcuAOYCl0ia26/YtcDyiDgV+BjwjfS584C/\nAhYApwHvkfT6ylV/5D39Ujv/dN9qLjzlaN7nyVwzy4ByevwLgNaIWBMR3cBtwEX9yswF7geIiFXA\nLElTgJOAhyNiZ0TkgV8DF1es9iOsK1/gM99fzvhRjXz5facgeYjHzA5/5QT/VGBdyeP16bZSj5MG\nuqQFwExgGvAkcI6kIyWNBi4EBjxtpaQrJC2VtLStrW3fWjFC/vm+Z1n18na++oFTOKKlsdrVMTOr\niEpN7l4HTJC0HPgU8BhQiIingX8Afg7cAywHCgO9QEQsjoj5ETF/8uTJFarW/luydgv/78HnuGTB\ndM47cUq1q2NmVjHlfHN3A3v20qel23pFRDtwKYCS8ZDngTXpvhuBG9N9XyH5xHBI6+jK879+sJxp\nE0fxhXf3n84wMzu8ldPjXwLMkTRbUiOwCLirtICkCek+gMuBB9ODAZKOSm9nkAwHfa9SlR8pt/zh\nBdZt2cXXPnAaY5p8Vgszy5ZhUy0i8pKuAu4FcsBNEbFS0pXp/htIJnFvlhTASuCykpf4oaQjgR7g\nkxGxtdKNqKRCMfiPP7zAmbOP4MzXHVnt6piZVVxZ3dmIuBu4u9+2G0ruPwQcP8hzzzmQCh5s96/a\nxPrXdvGFC0+qdlXMzEaEv7nbz78/tJZjxjfzjrme0DWzbHLwl2jdtIPfPLuZD585g/qcfzVmlk1O\ntxL/8dBaGnN1LFowo9pVMTMbMQ7+1PbOHm5ftp73nHoMk8Y0Vbs6ZmYjxsGfuuPRDXR0F/j4m2dV\nuypmZiPKwU9yZa2bH1rLadMncNr0CdWujpnZiHLwA79t3cyatg7+4s0zq10VM7MR5+AHbv79C0wa\n08iFpxxT7aqYmY24mg/+dVt28stVr3DJghk01eeqXR0zsxFX88F/yx9eoE7iQ2d6CaeZ1YaaDv5d\n3QVuW7KOd508hWPG+5KKZlYbajr4f7xiI9t29fDxN82qdlXMzA6amg7+37Vu5uhxzSyYfUS1q2Jm\ndtDUdPCv3NjOvKnjfS1dM6spNRv8O7vzrGnbwcnHjqt2VczMDqqaDf6nX9pOMXDwm1nNqdngf2rj\nNgBOnjq+yjUxMzu4ajb4V25sZ8LoBo4d31ztqpiZHVRlBb+khZJWS2qVdM0A+ydKulPSCkmPSJpX\nsu8zklZKelLSrZIOiaRdubGdk48d54ldM6s5wwa/pBxwPXABMBe4RNLcfsWuBZZHxKnAx4BvpM+d\nCvxPYH5EzCO5WPuiylV///QUiqx+eTvzjvUwz4gq9MCurdC+ETa3wkuPwwsPwcbl1a6ZWU0r52Lr\nC4DWiFgDIOk24CLgqZIyc4HrACJilaRZknZftLYeGCWpBxgNbKxU5fdX66YddBeKzPXEbuVFQOsv\n4Tf/CC/+fvByC66Ad/1fyJXzT9DMKqmc/3VTgXUlj9cDZ/Yr8zhwMfAbSQuAmcC0iFgm6WvAi8Au\n4OcR8fMDr/aBWbmxHYCT3eOvnGIRVv0kCfyXlsO4qXDO52D0EdAwGhpbkp+G0fDsffCH62HzM/DB\n78KoidWuvVlNqVR36zrgG5KWA08AjwEFSRNJPh3MBrYC/yXpIxFxS/8XkHQFcAXAjBkje8K0lRu3\nMaohx+xJLSP6PjWhkIeVdySB37YKjngdvPebcOoiqG8c+DnHvR2mzIUffxq+/cdwyfdh0usrV6eI\n5HZ/52+KBdj1GuzYBB1t0DgGJs5KDmKeE7IMKCf4NwDTSx5PS7f1ioh24FIAJbOlzwNrgHcBz0dE\nW7rvDuDNwF7BHxGLgcUA8+fPj31tyL5YubGdk44ZS64u4/+Jezqh0AXNFfpks3MLbHoKXnkKNq1M\nb5+G7u0w+ST40xth7vvKG7454yNwxHHw/Q/Dt89Lev7HnTf88/Ld8MoTsH4prF8CW1+E7p3Q05He\n7oTuDogC1I+CxtHQ0JLepj8DhXdEEvYdm2DnqxDFvcvsPgBMmAkTZ8Kk42HaG+Gok6DOp/S2w0c5\nwb8EmCNpNkngLwI+VFpA0gRgZ0R0A5cDD0ZEu6QXgbMkjSYZ6jkfWFrJBuyrYjF4amM77z9jamVf\neP3S5D//MadXvlfY3QHLbobl/wnTz4S3XwstkwYvH5H0wu+5FnZtgbP+B5zzWWgeZk5j7e/gsVuS\n53R39IVo984k3He91le2eQJMORlOW5QE9vELoW4fVwfPfBP81QNw6yK45QPwrq8kr9U/xLt3JAeY\n9UuSieFCV/L8cVPhyONg1BHpUFJJyNfVp88vfZ0OyHcmw1L9SUmoT38jtEyGlqOS33HLpOR5r61N\nf16ALWvgufshvyt5bkMLTH1DchCY9kaYfALku/red/dtvgvqm/qGvBrHJHVtbIExU5J9ZgfBsMEf\nEXlJVwH3kqzKuSkiVkq6Mt1/A3AScLOkAFYCl6X7HpZ0O/AokCcZAlo8Ii0p04tbdrKjK1/Zb+y+\n/ATctBCKPUkv9pQPwikfgElz9iwXkYTGml/B87+G9peSYY8TLoRjTtv7gNG5DR75N/jDvya90KNP\ngWXfhSduh7d9Ppkg7T+c8upz8NPPwpoHkoPQ7LfC776eHDTO+9ukp13aO42A534JD6aTsc0Tkt5s\nQ0sy9j5ual9QTZyVDNEcNRfGHlOZA9zEmXDZz+GHfwX3/PXg5eqb4dgz4MwrknCdOh/GV/jgvS8i\n4LXn+z55rF8Cv/8XKOb38wUF444t+UQxK/ndjDs2PQhNTv4e+3pwNRuAIkZ0VGW/zJ8/P5YuHZkP\nBj9d8RKf/N6j/PiqszllWgWGQHo64d/Og52b4W1/DU/9CJ7/DRBJmM/7AIw9Gtb8Ogn7bek8+bip\nSXhufDQZVhg3Nek1n3ghHHUyLL0RHl4MXdvg9e+At34OZpwFbavh3i9A633JePo7v5wcOPJd8Nt/\ngt/+cxKS5/0tvPGyJOQ3LIN7/gbWPQxTToGFX4GZZ8Pqn8KDX+ubjH3L1XDGR5Ne6MFWLMDqnyU9\n8obRew/RjJ8GuYaDX6990bMrWbK6ZU3yNyid0G5sSXr0+a6+Tx+ln2jaN/Z9onhtLWwfYPGbcumn\nkPQgUPravffTTxK7t/f+Lkcnzx9Iy6Tk7+8VVoc1ScsiYn5ZZWst+L96zyoWP7iGlV96V2Uutfjz\n/wO//yZ8+HaY845kW/tLsPJOePL2JHQh6UnPPgdedy7MPjcZopCgYzM8cy+svjsZPujZmb6w4KQ/\nSYZojj197/d99hdw77WweTXMOge2rU96oKd8EN759zB2yp7lI5I63fdF2PZictDZ/hJMnA1nfwZO\nu2TwyVg7+Ho6k07C9pfSSebNyfxDR1tyf+eWfkNi6TzH/n7iqKtPDq79P3FMnJX8Gxk1cfBPeBHJ\nwatre1KuoYyLGhV60rmU6Dtw7es8SURf2/O7+ib19yyU/C57OvqGLfcaSuzYc1guinsfOAc8mLb0\nDdWVdlIG+z0VepLfU/fOwf9Odbnk77AfHPxD+PhNj7Bpexc/u/qcA3+xtb+F774H5l8K7/nngcts\nWZP8h5gyb/h/2D2dyaeCjcth7kVw1IlDly/kYdl34IGvwOgj4d1fSw4sw73HH65P6n7ah+Dk97un\nlyX57r5g6+7YM+QG+r8eRdjxCmx9Yc95jJ2b9yzXOLbvYNA8Idnfe0Bq65vv2F22ZRKMKRmi6tre\nV7Zj057zRbvlmvoOArkGYIAALeb3PNBVhPrmWxpGg+r63qN7R7JQYF9eq/RTVqGn729Q7Bn+6S1H\nwf9+dv9a4eAf4rW//AvOPWEyX/vgaQf2Qp3t8K23JKF55W+Tf6zVEuFlhlZZXduTFVOlB4Pd97va\n0yGnNNh3h3zT2HQZbFtfwHdsTnr2TeP2LLv7Phqg570TCt0D16suN8DwVksytKZB5j8amkt65CU9\n891hX9889KeZQvfAnwzK+fSQa+j33rsXHwwybNnQDPP+dJ/+VLvtS/DXVFdvU3snm3d0VWZi955r\noH09/OXPqxv64NC3ymsam6zamnJytWtSXVIyN1PflHyPIyNqaolAxb6x+/SPk1Uy53w2Wf5nZnYY\nqbHgT87Bf9IxY/f/Rba/Aj++Olkq+bYhlh+amR2iamqo58kN7cw6cjRjm4dZFrh7jXbntpLxu3Q2\nfsVtyfjdxYsP/eWFZmYDqKngX/nSNk6dOmHwAsUiPPOzZG37xkcHKaRk9czkE0akjmZmI61mgn/b\nrh7WbdnFojcOcAK4YiFZ4/6bf0zORTNxFiy8LlnPvMca3tHJ6oQMTfKYWe2pmeB/Kp3YnVd6jd1C\nDzx+W/Jt1y3PwaQT4P2Lk+VUXttuZhlVM+m2e2K3dylnvgtu+xC0/gKOPhX+7N/hxD/xuVDMLPNq\nJvif2tjOlHFNTBrTlHzj9fa/TEL/3f8I8y/zWngzqxk1E/zJxdXHJxO4P/pEcrWohdfBGy+vdtXM\nzA6qmhjX6Owp0Nq2g5OPGQs//Qw88YPk7JVnfaLaVTMzO+hqose/+uXtFIpF/nTzv8KzNydno3zr\n56pdLTOzqqiJ4H9y4zY+U/9DZj17R3LxkvO/WO0qmZlVTU0E/9gV3+HD9XcQp38YLfwHT+SaWU2r\niTH+0175EasbTkTv/aaXa5pZzSsrBSUtlLRaUqukawbYP1HSnZJWSHpE0rx0+wmSlpf8tEv6dKUb\nMaRigWPy61k76pR9v8KPmVkGDTvUIykHXA+8A1gPLJF0V0Q8VVLsWmB5RLxf0olp+fMjYjVwesnr\nbADurHAbhrb1RRrpYcvoWQf1bc3MDlXl9PgXAK0RsSYiuoHbgIv6lZkL3A8QEauAWZL6XfSV84Hn\nIuKFA6zzvtmcXMZs25jZB/VtzcwOVeUE/1RgXcnj9em2Uo8DFwNIWgDMBPpfMXgRcOv+VfMAbF4N\nwK6xxx30tzYzOxRVaqbzOmCCpOXAp4DHgN4rFEtqBN4L/NdgLyDpCklLJS1ta2urULWg2PYMm2Mc\ndS1HVuw1zcwOZ+Us59wATC95PC3d1isi2oFLASQJeB5YU1LkAuDRiHhlsDeJiMXAYkgutl5O5ctR\n3LSa5+JYxjTXxMpVM7NhldPjXwLMkTQ77bkvAu4qLSBpQroP4HLgwfRgsNslVGOYB9Crz/Jc8VjG\nNHlFj5kZlBH8EZEHrgLuBZ4GfhARKyVdKenKtNhJwJOSVpP07q/e/XxJLSQrgu6odOWH1fEquc4t\nPBfH0tLkHr+ZGZT5zd2IuBu4u9+2G0ruPwQcP8hzO4DqDLCnE7utMZWzHfxmZkDWv7m7+RmAZIzf\nwW9mBmQ9+NueoZBrZkMc6eA3M0tlO/g3P0N7yyyCOge/mVkq88H/2qhZAJ7cNTNLZTf4e3bB1hdp\na54BQIuXc5qZAVkO/ldbgeClhpk05ERTvYPfzAyyHPxtyVLO9blpHt83MyuR3eDf/CwgXuQYj++b\nmZXIcPA/AxNnsrUn5x6/mVmJbAf/pOPZ0ZV3j9/MrEQ2g79YSCZ3Jx1Ph4PfzGwP2Qz+rS9CvrO3\nx+8zc5qZ9clm8KeXW2TyCXR0FTzGb2ZWIqPBn5yczUM9ZmZ7y2jwr4bRRxKjJrKjO+8ev5lZiYwG\n/7Mw6QR2dheI8Hl6zMxKZTP421bDpDl0dOUBB7+ZWansBX/Hq7BrS++KHoCxDn4zs15lBb+khZJW\nS2qVdM0A+ydKulPSCkmPSJpXsm+CpNslrZL0tKQ3VbIBe9k9sZuu6AH3+M3MSg0b/JJywPUkF1Gf\nC1wiaW6/YtcCyyPiVOBjwDdK9n0DuCciTgROI7lg+8hJr7PLpDls7+oBfEpmM7NS5fT4FwCtEbEm\nIrqB24CL+pWZC9wPEBGrgFmSpkgaD7wVuDHd1x0RWytW+4Fsfhbqm2H8jN4ev1f1mJn1KSf4pwLr\nSh6vT7eVehy4GEDSAmAmMA2YDbQB35H0mKRvS2o54FoPpW01HDkH6uo8uWtmNoBKTe5eB0yQtBz4\nFPAYUADqgTcA34qIM4AOYK85AgBJV0haKmlpW1vb/tdk8zMwaQ6AJ3fNzAZQTvBvAKaXPJ6WbusV\nEe0RcWlEnE4yxj8ZWEPy6WB9RDycFr2d5ECwl4hYHBHzI2L+5MmT97EZqfRyi0w+AcA9fjOzAZQT\n/EuAOZJmS2oEFgF3lRZIV+40pg8vBx5MDwYvA+sknZDuOx94qkJ131t6ucXSHr8Eoxs9uWtmttuw\nXeGIyEu6CrgXyAE3RcRKSVem+28ATgJulhTASuCykpf4FPCf6YFhDXBphdvQp/ccPclxZkdXnpbG\neiSN2FuamR1uyhoDiYi7gbv7bbuh5P5DwPGDPHc5MP8A6li+tmcAwZHHAaQnaHNv38ysVLa+ubv5\nGZgwAxpGAfiUzGZmA8he8E8+ofdhchEWB7+ZWansBH/J5RZ38/V2zcz2lqFUFFz6M2ge37uloyvP\nES2jq1gnM7NDT3aCv64Opu75FQEP9ZiZ7S07Qz0D8KoeM7O9ZTz4C4xpaqh2NczMDimZDf6ufIHu\nQpEx7vGbme0hs8Hvi7CYmQ0sw8HvE7SZmQ0ks8G/+5TMXtVjZranzAZ/h4PfzGxAmQ3+HR7qMTMb\nUOaD3z1+M7M9ZTb4+yZ3vZzTzKxUZoN/R7qc0z1+M7M9ZTb4vZzTzGxgmQ7+pvo6GnKZbaKZ2X4p\nKxUlLZS0WlKrpGsG2D9R0p2SVkh6RNK8kn1rJT0habmkpZWs/FC2+8ycZmYDGjYZJeWA64F3AOuB\nJZLuioinSopdCyyPiPdLOjEtf37J/rdHxOYK1ntYHb4Ii5nZgMrp8S8AWiNiTUR0A7cBF/UrMxe4\nHyAiVgGzJE2paE33kYPfzGxg5QT/VGBdyeP16bZSjwMXA0haAMwEpqX7AviFpGWSrjiw6pYvuQiL\nl3KamfVXqZnP64AJkpYDnwIeAwrpvrMj4nTgAuCTkt460AtIukLSUklL29raDrhCybn43eM3M+uv\nnODfAEwveTwt3dYrItoj4tI04D8GTAbWpPs2pLebgDtJho72EhGLI2J+RMyfPHnyPjekP19o3cxs\nYOUE/xJgjqTZkhqBRcBdpQUkTUj3AVwOPBgR7ZJaJI1Ny7QA7wSerFz1B+fr7ZqZDWzYZIyIvKSr\ngHuBHHBTRKyUdGW6/wbgJOBmSQGsBC5Lnz4FuFPS7vf6XkTcU/lm7M2Tu2ZmAysrGSPibuDufttu\nKLn/EHD8AM9bA5x2gHXcZ8VisLO74OA3MxtAJr/W2tGdnK5hrIPfzGwvmQx+n4vfzGxwmQx+n5LZ\nzGxwmQx+n5LZzGxwmQx+n5LZzGxwmQx+X3bRzGxwmQz+Dge/mdmgMhn8XtVjZja4TAe/e/xmZnvL\nZPB3dOWpEzQ3ZLJ5ZmYHJJPJuPuUzOk5gszMrEQmg99n5jQzG1w2g7/TZ+Y0MxtMJoO/o9vBb2Y2\nmEwGv4d6zMwGl8ngTy7C4hO0mZkNJKPBX2BMU0O1q2FmdkjKZPBv7+xhjHv8ZmYDylzwRwQdvuyi\nmdmgygp+SQslrZbUKumaAfZPlHSnpBWSHpE0r9/+nKTHJP2kUhUfTFe+SKEYDn4zs0EMG/yScsD1\nwAXAXOASSXP7FbsWWB4RpwIfA77Rb//VwNMHXt3h+Tw9ZmZDK6fHvwBojYg1EdEN3AZc1K/MXOB+\ngIhYBcySNAVA0jTg3cC3K1brIfiUzGZmQysn+KcC60oer0+3lXocuBhA0gJgJjAt3fd14PNAcag3\nkXSFpKWSlra1tZVRrYFt7/Qpmc3MhlKpyd3rgAmSlgOfAh4DCpLeA2yKiGXDvUBELI6I+RExf/Lk\nyftdEff4zcyGVk46bgCmlzyelm7rFRHtwKUASk6J+TywBvhz4L2SLgSagXGSbomIj1Sg7gPq6N7d\n4/dyTjOzgZTT418CzJE0W1IjsAi4q7SApAnpPoDLgQcjoj0i/iYipkXErPR5949k6APs6CoA7vGb\nmQ1m2HSMiLykq4B7gRxwU0SslHRluv8G4CTgZkkBrAQuG8E6D6l3qKfZwW9mNpCy0jEi7gbu7rft\nhpL7DwHHD/MavwJ+tc813Ecdvt6umdmQMvfN3d5VPY0OfjOzgWQu+Du68oxqyJGr82UXzcwGkr3g\n90VYzMyGlLng39FV8Jk5zcyGkLng7+jKe0WPmdkQMhf8Ozrzntg1MxtC9oLf19s1MxtS5oLfk7tm\nZkPLXvB3OfjNzIaSueDf0ZVnrCd3zcwGlangzxeKdPYUPblrZjaETAV/R3pmTp+S2cxscJkK/h3d\nvgiLmdlwMhX8PjOnmdnwMhX8O3wufjOzYWUr+Ds91GNmNpxMBX/vUI9X9ZiZDSpTwd871OMev5nZ\noMoKfkkLJa2W1CrpmgH2T5R0p6QVkh6RNC/d3pw+flzSSkl/V+kGlOqb3PVyTjOzwQwb/JJywPXA\nBcBc4BJJc/sVuxZYHhGnAh8DvpFu7wLOi4jTgNOBhZLOqlTl++voTtbxe3LXzGxw5fT4FwCtEbEm\nIrqB24CL+pWZC9wPEBGrgFmSpkRiR1qmIf2JylR9b9s78zTkRFO9e/xmZoMpJ/inAutKHq9Pt5V6\nHLgYQNJtsx9AAAAE1UlEQVQCYCYwLX2ck7Qc2ATcFxEPD/Qmkq6QtFTS0ra2tn1rRconaDMzG16l\nJnevAyakAf8p4DGgABARhYg4neRAsGD3+H9/EbE4IuZHxPzJkyfvVyU6unwRFjOz4ZSTkhuA6SWP\np6XbekVEO3ApgCQBzwNr+pXZKukBYCHw5AHUeVC+CIuZ2fDK6fEvAeZImi2pEVgE3FVaQNKEdB/A\n5cCDEdEuabKkCWmZUcA7gFWVq/6ekouweHzfzGwow3aPIyIv6SrgXiAH3BQRKyVdme6/ATgJuFlS\nACuBy9KnH5Nuz5EcZH4QET8ZgXYAsKOrwPhRDSP18mZmmVDWuEhE3A3c3W/bDSX3HwKOH+B5K4Az\nDrCOZdvR2cPUCc0H6+3MzA5LmfrmbkdXwZO7ZmbDyFjwezmnmdlwMhX85590FKdOG1/tapiZHdIy\n1T3++qKDNp1gZnbYylSP38zMhufgNzOrMQ5+M7Ma4+A3M6sxDn4zsxrj4DczqzEOfjOzGuPgNzOr\nMYoYsSsh7jdJbcAL+/n0ScDmClbncOF21xa3u7aU0+6ZEVHWVawOyeA/EJKWRsT8atfjYHO7a4vb\nXVsq3W4P9ZiZ1RgHv5lZjcli8C+udgWqxO2uLW53balouzM3xm9mZkPLYo/fzMyGkJngl7RQ0mpJ\nrZKuqXZ9RpKkmyRtkvRkybYjJN0n6dn0dmI161hpkqZLekDSU5JWSro63Z71djdLekTS42m7/y7d\nnul27yYpJ+kxST9JH9dKu9dKekLScklL020Va3smgl9SDrgeuACYC1wiaW51azWivgss7LftGuCX\nETEH+GX6OEvywGcjYi5wFvDJ9G+c9XZ3AedFxGnA6cBCSWeR/XbvdjXwdMnjWmk3wNsj4vSSZZwV\na3smgh9YALRGxJqI6AZuAy6qcp1GTEQ8CGzpt/ki4Ob0/s3A+w5qpUZYRLwUEY+m97eThMFUst/u\niIgd6cOG9CfIeLsBJE0D3g18u2Rz5ts9hIq1PSvBPxVYV/J4fbqtlkyJiJfS+y8DU6pZmZEkaRZw\nBvAwNdDudLhjObAJuC8iaqLdwNeBzwPFkm210G5IDu6/kLRM0hXptoq1PVPX3LVERISkTC7XkjQG\n+CHw6Yhol9S7L6vtjogCcLqkCcCdkub125+5dkt6D7ApIpZJOnegMllsd4mzI2KDpKOA+yStKt15\noG3PSo9/AzC95PG0dFsteUXSMQDp7aYq16fiJDWQhP5/RsQd6ebMt3u3iNgKPEAyv5P1dr8FeK+k\ntSRDt+dJuoXstxuAiNiQ3m4C7iQZzq5Y27MS/EuAOZJmS2oEFgF3VblOB9tdwMfT+x8H/ruKdak4\nJV37G4GnI+KfSnZlvd2T054+kkYB7wBWkfF2R8TfRMS0iJhF8v/5/oj4CBlvN4CkFkljd98H3gk8\nSQXbnpkvcEm6kGRMMAfcFBF/X+UqjRhJtwLnkpyx7xXgi8CPgB8AM0jObPpnEdF/AviwJels4DfA\nE/SN+V5LMs6f5XafSjKRlyPpqP0gIr4k6Ugy3O5S6VDP5yLiPbXQbkmvI+nlQzIc/72I+PtKtj0z\nwW9mZuXJylCPmZmVycFvZlZjHPxmZjXGwW9mVmMc/GZmNcbBb2ZWYxz8ZmY1xsFvZlZj/j8HtXeP\nwkMZ6AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcf73f4a150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Net Hyper params:\n",
      "Layers: [784, 500, 500]\n",
      "Learning Rate: 0.0001\n",
      "Learning rate decay: 0.0001\n",
      "Epoch : 1 Loss : 0.793  Train Accuracy: 0.818 Validation Accuracy: 0.896\n",
      "Epoch : 2 Loss : 0.352  Train Accuracy: 0.898 Validation Accuracy: 0.915\n",
      "Epoch : 3 Loss : 0.295  Train Accuracy: 0.914 Validation Accuracy: 0.927\n",
      "Epoch : 4 Loss : 0.265  Train Accuracy: 0.924 Validation Accuracy: 0.932\n",
      "Epoch : 5 Loss : 0.243  Train Accuracy: 0.930 Validation Accuracy: 0.936\n",
      "Epoch : 6 Loss : 0.226  Train Accuracy: 0.935 Validation Accuracy: 0.941\n",
      "Epoch : 7 Loss : 0.213  Train Accuracy: 0.939 Validation Accuracy: 0.942\n",
      "Epoch : 8 Loss : 0.202  Train Accuracy: 0.942 Validation Accuracy: 0.945\n",
      "Epoch : 9 Loss : 0.191  Train Accuracy: 0.945 Validation Accuracy: 0.947\n",
      "Epoch : 10 Loss : 0.181  Train Accuracy: 0.948 Validation Accuracy: 0.951\n",
      "Epoch : 11 Loss : 0.172  Train Accuracy: 0.951 Validation Accuracy: 0.953\n",
      "Epoch : 12 Loss : 0.165  Train Accuracy: 0.952 Validation Accuracy: 0.954\n",
      "Epoch : 13 Loss : 0.158  Train Accuracy: 0.955 Validation Accuracy: 0.956\n",
      "Epoch : 14 Loss : 0.153  Train Accuracy: 0.956 Validation Accuracy: 0.956\n",
      "Epoch : 15 Loss : 0.147  Train Accuracy: 0.958 Validation Accuracy: 0.957\n",
      "Epoch : 16 Loss : 0.143  Train Accuracy: 0.960 Validation Accuracy: 0.959\n",
      "Epoch : 17 Loss : 0.138  Train Accuracy: 0.960 Validation Accuracy: 0.959\n",
      "Epoch : 18 Loss : 0.134  Train Accuracy: 0.962 Validation Accuracy: 0.960\n",
      "Epoch : 19 Loss : 0.131  Train Accuracy: 0.963 Validation Accuracy: 0.961\n",
      "Epoch : 20 Loss : 0.127  Train Accuracy: 0.964 Validation Accuracy: 0.961\n",
      "Epoch : 21 Loss : 0.124  Train Accuracy: 0.965 Validation Accuracy: 0.962\n",
      "Epoch : 22 Loss : 0.121  Train Accuracy: 0.965 Validation Accuracy: 0.962\n",
      "Epoch : 23 Loss : 0.118  Train Accuracy: 0.966 Validation Accuracy: 0.962\n",
      "Epoch : 24 Loss : 0.116  Train Accuracy: 0.967 Validation Accuracy: 0.963\n",
      "Epoch : 25 Loss : 0.113  Train Accuracy: 0.968 Validation Accuracy: 0.963\n",
      "Epoch : 26 Loss : 0.111  Train Accuracy: 0.968 Validation Accuracy: 0.963\n",
      "Epoch : 27 Loss : 0.109  Train Accuracy: 0.969 Validation Accuracy: 0.964\n",
      "Epoch : 28 Loss : 0.107  Train Accuracy: 0.970 Validation Accuracy: 0.964\n",
      "Epoch : 29 Loss : 0.105  Train Accuracy: 0.970 Validation Accuracy: 0.965\n",
      "Epoch : 30 Loss : 0.103  Train Accuracy: 0.971 Validation Accuracy: 0.966\n",
      "Epoch : 31 Loss : 0.101  Train Accuracy: 0.972 Validation Accuracy: 0.966\n",
      "Epoch : 32 Loss : 0.100  Train Accuracy: 0.972 Validation Accuracy: 0.966\n",
      "Epoch : 33 Loss : 0.098  Train Accuracy: 0.973 Validation Accuracy: 0.966\n",
      "Epoch : 34 Loss : 0.096  Train Accuracy: 0.973 Validation Accuracy: 0.966\n",
      "Epoch : 35 Loss : 0.094  Train Accuracy: 0.973 Validation Accuracy: 0.967\n",
      "Epoch : 36 Loss : 0.093  Train Accuracy: 0.974 Validation Accuracy: 0.967\n",
      "Epoch : 37 Loss : 0.092  Train Accuracy: 0.974 Validation Accuracy: 0.967\n",
      "Epoch : 38 Loss : 0.091  Train Accuracy: 0.975 Validation Accuracy: 0.967\n",
      "Epoch : 39 Loss : 0.089  Train Accuracy: 0.975 Validation Accuracy: 0.967\n",
      "Epoch : 40 Loss : 0.088  Train Accuracy: 0.975 Validation Accuracy: 0.967\n",
      "Epoch : 41 Loss : 0.087  Train Accuracy: 0.975 Validation Accuracy: 0.968\n",
      "Epoch : 42 Loss : 0.086  Train Accuracy: 0.976 Validation Accuracy: 0.969\n",
      "Epoch : 43 Loss : 0.084  Train Accuracy: 0.976 Validation Accuracy: 0.969\n",
      "Epoch : 44 Loss : 0.084  Train Accuracy: 0.976 Validation Accuracy: 0.969\n",
      "Epoch : 45 Loss : 0.082  Train Accuracy: 0.977 Validation Accuracy: 0.969\n",
      "Epoch : 46 Loss : 0.082  Train Accuracy: 0.977 Validation Accuracy: 0.969\n",
      "Epoch : 47 Loss : 0.081  Train Accuracy: 0.977 Validation Accuracy: 0.970\n",
      "Epoch : 48 Loss : 0.080  Train Accuracy: 0.977 Validation Accuracy: 0.969\n",
      "Epoch : 49 Loss : 0.079  Train Accuracy: 0.977 Validation Accuracy: 0.969\n",
      "Epoch : 50 Loss : 0.078  Train Accuracy: 0.978 Validation Accuracy: 0.970\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcf741533d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Net Hyper params:\n",
      "Layers: [784, 500, 500]\n",
      "Learning Rate: 1e-05\n",
      "Learning rate decay: 0.0001\n",
      "Epoch : 1 Loss : 2.121  Train Accuracy: 0.360 Validation Accuracy: 0.544\n",
      "Epoch : 2 Loss : 1.944  Train Accuracy: 0.589 Validation Accuracy: 0.663\n",
      "Epoch : 3 Loss : 1.836  Train Accuracy: 0.663 Validation Accuracy: 0.708\n",
      "Epoch : 4 Loss : 1.753  Train Accuracy: 0.697 Validation Accuracy: 0.734\n",
      "Epoch : 5 Loss : 1.685  Train Accuracy: 0.719 Validation Accuracy: 0.752\n",
      "Epoch : 6 Loss : 1.628  Train Accuracy: 0.734 Validation Accuracy: 0.764\n",
      "Epoch : 7 Loss : 1.578  Train Accuracy: 0.745 Validation Accuracy: 0.774\n",
      "Epoch : 8 Loss : 1.535  Train Accuracy: 0.753 Validation Accuracy: 0.781\n",
      "Epoch : 9 Loss : 1.496  Train Accuracy: 0.759 Validation Accuracy: 0.788\n",
      "Epoch : 10 Loss : 1.462  Train Accuracy: 0.764 Validation Accuracy: 0.792\n",
      "Epoch : 11 Loss : 1.431  Train Accuracy: 0.769 Validation Accuracy: 0.796\n",
      "Epoch : 12 Loss : 1.403  Train Accuracy: 0.773 Validation Accuracy: 0.800\n",
      "Epoch : 13 Loss : 1.377  Train Accuracy: 0.776 Validation Accuracy: 0.803\n",
      "Epoch : 14 Loss : 1.353  Train Accuracy: 0.779 Validation Accuracy: 0.806\n",
      "Epoch : 15 Loss : 1.331  Train Accuracy: 0.782 Validation Accuracy: 0.809\n",
      "Epoch : 16 Loss : 1.311  Train Accuracy: 0.784 Validation Accuracy: 0.810\n",
      "Epoch : 17 Loss : 1.293  Train Accuracy: 0.786 Validation Accuracy: 0.812\n",
      "Epoch : 18 Loss : 1.275  Train Accuracy: 0.789 Validation Accuracy: 0.814\n",
      "Epoch : 19 Loss : 1.259  Train Accuracy: 0.791 Validation Accuracy: 0.816\n",
      "Epoch : 20 Loss : 1.244  Train Accuracy: 0.793 Validation Accuracy: 0.817\n",
      "Epoch : 21 Loss : 1.230  Train Accuracy: 0.794 Validation Accuracy: 0.819\n",
      "Epoch : 22 Loss : 1.216  Train Accuracy: 0.796 Validation Accuracy: 0.821\n",
      "Epoch : 23 Loss : 1.203  Train Accuracy: 0.797 Validation Accuracy: 0.822\n",
      "Epoch : 24 Loss : 1.192  Train Accuracy: 0.799 Validation Accuracy: 0.824\n",
      "Epoch : 25 Loss : 1.180  Train Accuracy: 0.800 Validation Accuracy: 0.825\n",
      "Epoch : 26 Loss : 1.170  Train Accuracy: 0.802 Validation Accuracy: 0.826\n",
      "Epoch : 27 Loss : 1.160  Train Accuracy: 0.803 Validation Accuracy: 0.827\n",
      "Epoch : 28 Loss : 1.150  Train Accuracy: 0.804 Validation Accuracy: 0.828\n",
      "Epoch : 29 Loss : 1.141  Train Accuracy: 0.805 Validation Accuracy: 0.829\n",
      "Epoch : 30 Loss : 1.132  Train Accuracy: 0.806 Validation Accuracy: 0.829\n",
      "Epoch : 31 Loss : 1.124  Train Accuracy: 0.807 Validation Accuracy: 0.830\n",
      "Epoch : 32 Loss : 1.116  Train Accuracy: 0.807 Validation Accuracy: 0.832\n",
      "Epoch : 33 Loss : 1.108  Train Accuracy: 0.808 Validation Accuracy: 0.832\n",
      "Epoch : 34 Loss : 1.101  Train Accuracy: 0.809 Validation Accuracy: 0.833\n",
      "Epoch : 35 Loss : 1.094  Train Accuracy: 0.810 Validation Accuracy: 0.833\n",
      "Epoch : 36 Loss : 1.087  Train Accuracy: 0.810 Validation Accuracy: 0.834\n",
      "Epoch : 37 Loss : 1.081  Train Accuracy: 0.811 Validation Accuracy: 0.835\n",
      "Epoch : 38 Loss : 1.075  Train Accuracy: 0.812 Validation Accuracy: 0.835\n",
      "Epoch : 39 Loss : 1.069  Train Accuracy: 0.813 Validation Accuracy: 0.836\n",
      "Epoch : 40 Loss : 1.063  Train Accuracy: 0.813 Validation Accuracy: 0.836\n",
      "Epoch : 41 Loss : 1.057  Train Accuracy: 0.814 Validation Accuracy: 0.837\n",
      "Epoch : 42 Loss : 1.052  Train Accuracy: 0.814 Validation Accuracy: 0.837\n",
      "Epoch : 43 Loss : 1.047  Train Accuracy: 0.815 Validation Accuracy: 0.839\n",
      "Epoch : 44 Loss : 1.042  Train Accuracy: 0.815 Validation Accuracy: 0.839\n",
      "Epoch : 45 Loss : 1.037  Train Accuracy: 0.816 Validation Accuracy: 0.840\n",
      "Epoch : 46 Loss : 1.032  Train Accuracy: 0.816 Validation Accuracy: 0.840\n",
      "Epoch : 47 Loss : 1.028  Train Accuracy: 0.817 Validation Accuracy: 0.841\n",
      "Epoch : 48 Loss : 1.023  Train Accuracy: 0.817 Validation Accuracy: 0.841\n",
      "Epoch : 49 Loss : 1.019  Train Accuracy: 0.818 Validation Accuracy: 0.842\n",
      "Epoch : 50 Loss : 1.015  Train Accuracy: 0.818 Validation Accuracy: 0.843\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcf741b4590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Net Hyper params:\n",
      "Layers: [784, 650, 650]\n",
      "Learning Rate: 0.001\n",
      "Learning rate decay: 0.0001\n",
      "Epoch : 1 Loss : 0.227  Train Accuracy: 0.932 Validation Accuracy: 0.963\n",
      "Epoch : 2 Loss : 0.085  Train Accuracy: 0.974 Validation Accuracy: 0.971\n",
      "Epoch : 3 Loss : 0.052  Train Accuracy: 0.984 Validation Accuracy: 0.975\n",
      "Epoch : 4 Loss : 0.032  Train Accuracy: 0.990 Validation Accuracy: 0.978\n",
      "Epoch : 5 Loss : 0.021  Train Accuracy: 0.993 Validation Accuracy: 0.976\n",
      "Epoch : 6 Loss : 0.015  Train Accuracy: 0.995 Validation Accuracy: 0.978\n",
      "Epoch : 7 Loss : 0.010  Train Accuracy: 0.997 Validation Accuracy: 0.978\n",
      "Epoch : 8 Loss : 0.007  Train Accuracy: 0.997 Validation Accuracy: 0.978\n",
      "Epoch : 9 Loss : 0.005  Train Accuracy: 0.998 Validation Accuracy: 0.978\n",
      "Epoch : 10 Loss : 0.003  Train Accuracy: 0.999 Validation Accuracy: 0.979\n",
      "Epoch : 11 Loss : 0.003  Train Accuracy: 0.999 Validation Accuracy: 0.980\n",
      "Epoch : 12 Loss : 0.002  Train Accuracy: 0.999 Validation Accuracy: 0.978\n",
      "Epoch : 13 Loss : 0.002  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 14 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 15 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 16 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 17 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 18 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 19 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 20 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 21 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 22 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 23 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 24 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 25 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 26 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 27 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 28 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 29 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 30 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 31 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 32 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 33 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 34 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 35 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 36 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 37 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 38 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 39 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 40 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980\n",
      "Epoch : 41 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980\n",
      "Epoch : 42 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980\n",
      "Epoch : 43 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980\n",
      "Epoch : 44 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980\n",
      "Epoch : 45 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980\n",
      "Epoch : 46 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980\n",
      "Epoch : 47 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980\n",
      "Epoch : 48 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980\n",
      "Epoch : 49 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980\n",
      "Epoch : 50 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKTH/Hw8tcaghxxn9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcf6a210890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Net Hyper params:\n",
      "Layers: [784, 650, 650]\n",
      "Learning Rate: 0.0001\n",
      "Learning rate decay: 0.0001\n",
      "Epoch : 1 Loss : 0.672  Train Accuracy: 0.843 Validation Accuracy: 0.909\n",
      "Epoch : 2 Loss : 0.313  Train Accuracy: 0.909 Validation Accuracy: 0.927\n",
      "Epoch : 3 Loss : 0.270  Train Accuracy: 0.923 Validation Accuracy: 0.934\n",
      "Epoch : 4 Loss : 0.239  Train Accuracy: 0.933 Validation Accuracy: 0.939\n",
      "Epoch : 5 Loss : 0.216  Train Accuracy: 0.940 Validation Accuracy: 0.944\n",
      "Epoch : 6 Loss : 0.201  Train Accuracy: 0.943 Validation Accuracy: 0.946\n",
      "Epoch : 7 Loss : 0.187  Train Accuracy: 0.946 Validation Accuracy: 0.949\n",
      "Epoch : 8 Loss : 0.172  Train Accuracy: 0.951 Validation Accuracy: 0.954\n",
      "Epoch : 9 Loss : 0.160  Train Accuracy: 0.954 Validation Accuracy: 0.954\n",
      "Epoch : 10 Loss : 0.149  Train Accuracy: 0.957 Validation Accuracy: 0.958\n",
      "Epoch : 11 Loss : 0.142  Train Accuracy: 0.959 Validation Accuracy: 0.957\n",
      "Epoch : 12 Loss : 0.135  Train Accuracy: 0.962 Validation Accuracy: 0.960\n",
      "Epoch : 13 Loss : 0.127  Train Accuracy: 0.963 Validation Accuracy: 0.961\n",
      "Epoch : 14 Loss : 0.123  Train Accuracy: 0.965 Validation Accuracy: 0.962\n",
      "Epoch : 15 Loss : 0.117  Train Accuracy: 0.966 Validation Accuracy: 0.963\n",
      "Epoch : 16 Loss : 0.111  Train Accuracy: 0.968 Validation Accuracy: 0.964\n",
      "Epoch : 17 Loss : 0.107  Train Accuracy: 0.969 Validation Accuracy: 0.963\n",
      "Epoch : 18 Loss : 0.103  Train Accuracy: 0.971 Validation Accuracy: 0.965\n",
      "Epoch : 19 Loss : 0.100  Train Accuracy: 0.971 Validation Accuracy: 0.966\n",
      "Epoch : 20 Loss : 0.096  Train Accuracy: 0.972 Validation Accuracy: 0.966\n",
      "Epoch : 21 Loss : 0.093  Train Accuracy: 0.974 Validation Accuracy: 0.967\n",
      "Epoch : 22 Loss : 0.091  Train Accuracy: 0.974 Validation Accuracy: 0.967\n",
      "Epoch : 23 Loss : 0.088  Train Accuracy: 0.975 Validation Accuracy: 0.967\n",
      "Epoch : 24 Loss : 0.085  Train Accuracy: 0.975 Validation Accuracy: 0.968\n",
      "Epoch : 25 Loss : 0.083  Train Accuracy: 0.977 Validation Accuracy: 0.969\n",
      "Epoch : 26 Loss : 0.081  Train Accuracy: 0.977 Validation Accuracy: 0.970\n",
      "Epoch : 27 Loss : 0.079  Train Accuracy: 0.978 Validation Accuracy: 0.969\n",
      "Epoch : 28 Loss : 0.077  Train Accuracy: 0.978 Validation Accuracy: 0.970\n",
      "Epoch : 29 Loss : 0.075  Train Accuracy: 0.979 Validation Accuracy: 0.971\n",
      "Epoch : 30 Loss : 0.074  Train Accuracy: 0.979 Validation Accuracy: 0.971\n",
      "Epoch : 31 Loss : 0.072  Train Accuracy: 0.980 Validation Accuracy: 0.970\n",
      "Epoch : 32 Loss : 0.070  Train Accuracy: 0.980 Validation Accuracy: 0.971\n",
      "Epoch : 33 Loss : 0.069  Train Accuracy: 0.981 Validation Accuracy: 0.972\n",
      "Epoch : 34 Loss : 0.067  Train Accuracy: 0.981 Validation Accuracy: 0.973\n",
      "Epoch : 35 Loss : 0.066  Train Accuracy: 0.982 Validation Accuracy: 0.972\n",
      "Epoch : 36 Loss : 0.065  Train Accuracy: 0.982 Validation Accuracy: 0.972\n",
      "Epoch : 37 Loss : 0.063  Train Accuracy: 0.982 Validation Accuracy: 0.973\n",
      "Epoch : 38 Loss : 0.062  Train Accuracy: 0.983 Validation Accuracy: 0.973\n",
      "Epoch : 39 Loss : 0.061  Train Accuracy: 0.983 Validation Accuracy: 0.973\n",
      "Epoch : 40 Loss : 0.060  Train Accuracy: 0.983 Validation Accuracy: 0.972\n",
      "Epoch : 41 Loss : 0.059  Train Accuracy: 0.984 Validation Accuracy: 0.973\n",
      "Epoch : 42 Loss : 0.058  Train Accuracy: 0.984 Validation Accuracy: 0.973\n",
      "Epoch : 43 Loss : 0.057  Train Accuracy: 0.984 Validation Accuracy: 0.973\n",
      "Epoch : 44 Loss : 0.056  Train Accuracy: 0.984 Validation Accuracy: 0.973\n",
      "Epoch : 45 Loss : 0.055  Train Accuracy: 0.985 Validation Accuracy: 0.974\n",
      "Epoch : 46 Loss : 0.054  Train Accuracy: 0.985 Validation Accuracy: 0.974\n",
      "Epoch : 47 Loss : 0.053  Train Accuracy: 0.985 Validation Accuracy: 0.973\n",
      "Epoch : 48 Loss : 0.053  Train Accuracy: 0.985 Validation Accuracy: 0.973\n",
      "Epoch : 49 Loss : 0.052  Train Accuracy: 0.986 Validation Accuracy: 0.974\n",
      "Epoch : 50 Loss : 0.051  Train Accuracy: 0.986 Validation Accuracy: 0.974\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XucXGWd5/HPr6vv907S6aTTSTqE3JpLAjQRRQTkIipj\nkPECrjqCDoMriOPsjg6zuzO7M7PDa0d3ZRVlGGRgBoRRxDUgCCI4BEVIAgkhN9Lk1kk66U46fb9U\nVddv/zjVSae7k66QvuXU9/161auqzjl16nkifuvp5zzneczdERGR9JEx0QUQEZHxpeAXEUkzCn4R\nkTSj4BcRSTMKfhGRNKPgFxFJMwp+EZE0o+AXEUkzCn4RkTSTOdEFGM60adO8urp6ooshInLaWLt2\n7UF3L0/l2EkZ/NXV1axZs2aiiyEictows12pHquuHhGRNKPgFxFJMwp+EZE0o+AXEUkzCn4RkTSj\n4BcRSTMKfhGRNDMpx/GLiIRZIuG0dMdoau8NHh09NLX30peAL182f8y/X8EvInICfQmnqb2Xtp4Y\nhTmZFOdlUZAdwcyOOa4n1seBth4aWntoaO2mobWHw51RWrpitHTHaO2K0dId5XBXjMOdUeKJoeud\nlxflKPhFRE5FvC9BtC9BX8JJJCCeSNDnTl/C6ezto7U7Rlt3jNYBj0Mdvexv62F/Wy8HWntobO9h\ncEZnGBTnZVGUm0l+ViYHO3o51Bkd8v25WRmU5mVTmp9FaX4WZ0wrpDQ/i7KCbKYX5VBelEN5YQ7T\nkq+LcsYnkhX8IjJpuTvNnVH2t/VwoK2H1u4YkYwMsiNGViSDzEgGWREDh32tPdQ3d1F/uIs9zd3U\nH+5if1sPPrRhfUJFuZnMLMmlojiXhdOnMaMklxkluZTkZdHZG6etO05bT/CD0dYTp7M3zgXVZcws\nDo6bWZLHzNJcZhTnUjBOQX6yJmepRCR0EgmnrSfGwY4ozZ1RDnX00todo6M3TltPnPaeGO3J5yNh\n39pLtC+R8neYwYziXGaX5fPe+VOpKs0jPyeTiBmRjOCRkWFEzCjIiVCcl0VJ8lGal0VxXhZZkfCP\neVHwi8i71pdwOnrjdPTGae2KcaC9h/2tweNAW0/QZdLaw6HOIOz7hunX7leUk0lhbiZFuZmU5Wdz\n/pwyZhQHLe+ZJblUlORSmpdFwp1o3IknEsT6EkTjjuPMLMmjsjSXnMzIOP4LnJ5SCn4zuwa4G4gA\n97v7XYP2lwEPAPOBHuBmd38rue9PgS8BDmwAbnL3nlGrgYiMutauGDsPdR65ULm/9ehFy4Md0SDs\ne+J0x/qG/bwZTC3IYWZJLlVleZw3p5QpBdlMLchhamHwPKUgm5L8oJ+8MDuTjAwb9lwy+kYMfjOL\nAPcAVwF7gNVmttLdNw047E5gnbt/3MwWJ4+/wsxmAV8Faty928x+DNwAPDjK9RCR40gknMNdUSIZ\nRm5WhJzMjGNGpHT0xnlrbysb9rSyfk8LG/a2sutQ1zHnyM7MYGZJ0PI+e1YJhTlBy7wgO2ilF+ZE\nKMrNoqI4hxkleUwvykmLLpPTVSot/uVAnbtvBzCzx4AVwMDgrwHuAnD3LWZWbWYVA74jz8xiQD6w\nb7QKLyLQFY1T39zN7uYu6puDC5oNrT00tARDCg+09RwzdNAMcjIzyM2KkB3JoKmj98gF0FmleZxb\nVcKnL5zNgulFR8J+SkH2kOGLcvpKJfhnAfUD3u8B3jPomPXA9cAqM1sOzAWq3H2tmX0L2A10A8+5\n+3OnXmyR8IvGExzs6KW5MxoMF0xeFD3YGQwz3N3cxe7mbg529B7zuezMDCqTI1GWz5vCzJJcyoty\nSHgw1rw31kd3rI+eWILeeB9VZfmcU1XCObNKmFaYM0G1lfE0Whd37wLuNrN1BP34bwB9yb7/FcA8\noAX4iZl91t0fHnwCM7sFuAVgzpw5o1QskcmtKxqnsa2XXc1d7DzYyY4Bjz2Hu4aMHwfIjmRQXpTD\nnCn5XLF4OnOm5jN7Sj5zpuQzuyxPrXMZUSrBvxeYPeB9VXLbEe7eBtwEYMF/cTuA7cCHgB3u3pTc\n9wTwPmBI8Lv7fcB9ALW1tSc58lZk8ojGExzq7D16O/6R2/J7Odhx7LbO6LEXRwuyI1RPK+DcqhJW\nLKtkZkkeUwuzmdZ/QbQwm6KcTAW7nJJUgn81sMDM5hEE/g3AZwYeYGalQJe7RwlG8Lzk7m1mthu4\nyMzyCbp6rgC0mK6ERmNbDxv3tbGpoY2N+1rZuK9tyIXRfkW5mUfu1jynqjR5x2Y25YU5zJ6SzxnT\nCigvylGoy5gbMfjdPW5mtwHPEgznfMDdN5rZrcn99wJLgIfMzIGNwBeT+141s8eB14E4QRfQfWNS\nE5ExcKgj6IYZODa9oTUYn769qfOY/vW5U/OpmVnMdctmMaMkl2mFyVvyi3KYWpBNbpbGl8vkYH6y\n9zOPg9raWl+zRn8YyPhKJJx3mjpYs+swa3YeZu2uZnYOM6xxRnFwO/7sKfmcVVnMWZXFLKkspjg3\na4JKLgJmttbda1M5VnfuSlqK9SXYcbCTzQ1tbN3fzuaGNt6ob6GlKwbAlIJsLphbxo3L57CwoujI\n3aOl+VnqipHTnoJfQqU/0Oubu+jojdPeE0wn0Jl8fbgrytb97Wxv6jwyB0xmhjG/vJAP1czgguoy\naueWMW9agQJexldfDDoaoWTWmH+Vgl9OWwc7elm3u4WtB9rZur+dtw+0805TB7G+od2XZlCYk0lJ\nXhZnTi/kskXTWTyjiEUziphfXkh2pu4yTXt9MTi4Ddr2QrQTYl3Bc/9rT0DpHCirDh7FsyDjXVy3\n6T4MB+vg4NvJx7bg+fAOKKyAr28a+RynSMEvp5Wm9l5+uXE/v3hzH6/taD4yzn1WaR6LZhRx+eLp\nLKooonpaAUW5mUcm/srLGrpwhpxG3KHzIPS0QlYeZOdDdiFEhrmuEo9CrD+wu4PAHu58rfVwYCM0\nbgqem7ZCInb8MljGsefKyAx+CErnBoFdWA4F06FwOhSUQ/7UoAXfH/CHkmHf2XT0HJFsmHIGTF8M\nS/4Api0MyjbG/60q+GVSc3caWnv49eYD/GJDw5Gwn19ewG2Xn8mli8pZWFFEkS6snlhPW9CiPLxz\n0GNXED79oVVQfvR1/lTIK4Xc0qPPWXnB+boPBy3j1r3Bc9teaD8A0Y5kS7lrQPj2QGY2ZOVDdkHw\n6H+dmRN8f0ZWEOKR7OA52glt+5KPPcFz39CFTsjICn4EMnOD74l1QiJ+cv82xbNgeg2ceSVUnBW0\n5o+UsTB5/rwg9Nv2Dv03bNkVhHpHI/T1Dv8d+VODUF/0YZi6IHg9bUHwoxEZ/xjWqB6ZFDp64+xv\n7WF7UwfvNHVS19hBXVMH2xs7aO8N/o985vRCPnLOTD56zkwWVhSGvwUf6w5alcO1akcS7YLdr8D2\n38D2F2H/hmP3500JAq50ThCUHY1BS7SzKQjv44lkg0Ug3n3sdosELd2coqOhfuQ5D+K9Q7tOop3B\n9kQs6Gbpix0NzowsKJ4JxVVQXBn0exfPgryyoT8s0a6gPJl5yR+WfMga8AOTcZxuvMIZUFETnHM0\nuENv+9F/x86DwQ/ptAWQP2V0vuMENKpHJqV4X4LfvnOIl95uYn9bD01tvTS299DY3kvXoDtYK4pz\nOHN6IdefP4v50wu56IypLKwomqCSj4NoZxDO+9ZBw7rg+eBWyC6ChVfD4o8GLdKc4/wb9LRCw3qo\nfy0I+/pXgxZyRhbMfg9cdmfQnVA2D8rmQm7JCcrSBZ2N0NUMPS3Q3XLsc6IvCOHiSihJBnNhxbvr\n7x7MPTi/ZRw/sCcrM8gtDh5Tx37d3FOh4JcxlUg4r+8+zMr1+/jFmw0c6oySm5XBzJI8yotyOHtW\nCdOLcplenENFcQ7VUwuYP70wXGPi3YPW9+Yngy6SwRcOe9uD7oL+/uOC6VC5DJZcG3RxbH0GNvwk\naG3PuxQWfyToF254E/a9EfxQNG8/+n0V58DyW+CMy2Hue4OW78nIzofs6uAvgvFmNiFdH+lG/8Iy\n6nrjfWzY08qvtzSyct0+9rZ0k5OZwZU1FXxsaSWXLSo/PVZJcg8CuaV++JZvdiHMfV/Qos4rHfr5\nrmZY/yisfTC4qJeZF/zpn51/tBukuDJ4PvdTMHMZVJ4XdHEM1BcPWvBbn4YtT8FTf3p0X8ns4Edi\n2X8InmeeBwVTx/SfRU5/6uOXU9baHeP1XYd5bWcza3Y2s35PK9F4gkiGccmCaaxYVslVNTMonKQL\nTx/hHgT0rt/Czt/Crt9B+zDLR1hG0FXS25EcBWIw4xyYe3HwQ5BbDG88DJt+HnS3VF0IF9wEZ113\n8q3v4crYuBk69sOMpQp5OeJk+vgV/PKuuDv//nYT33/xHVbvasY9uBHq7FklXFhdRm31FJZXT6Gs\nIHuii3pibQ1Q9yuoez4I+66DwfbCGVB9Mcx5bzACY+DoluyioP851g17Vgc/EDtfDl7Hk6uK5pTA\n0k/D+X8EM86euPpJ2tDFXRkz7s7zmxv57gvbeHNPK5UludxxxQLeM28qy2aXkpc9jl04iQR0NydH\npDQGoyj6X8d7oWjmsRcgi2YCFgT0tueCwO8f7VJUCQuuClrscy8O+tBHGjWUlQfzPhA8IBg/vu8N\n6DgQXIjNzh/T6ou8Wwp+SUki4fxy436++0IdmxvamDMln7uuP4frz68avbteo11B2GbmDg3dRCK4\ngNmwLgjXfetg/5vQ2zb0PBmZwYXQ2ODpkZPnjncHww/nXARX/jUsuDoYx32qw0Mzs2HO4MXpRCYf\nBb8cV6wvweqdzfxmaxPPbdzPzkNdnDGtgG9/cikrllWSORqLabcfgM0rYeP/C/rW8aAPPSs5Hrt/\nLHbL7qMhH8kJuk/O+WTQDTP4jsm8siDEe1qDUTFHbjLaF2ybcxGccdnwF2RF0oCCX47R1N7Lb7Y2\n8uLWRla9fZD23jhZEeM986by9asX8dFzZhLJOMWW8XBhP20RXPL1YKTMcDf6zLkoOeplGZQvTu2m\nptyS4DF9yamVVyRkFPxCVzTOsxv389O1e/ntOwdxD26g+ui5M7l88XQuPnPaqY/IOfROMBRxy9PB\n0EQ8CPBLvxGMdlE4i4wbBX+aSiSc3+84xE/X7uWZtxroivZRVZbH7ZefydVnzeCsyuKRp0RI9AVz\nlfS0JOdYSc6zkpGcc6W1Hrb8Ingc3Bp8Zsa5cNk3oWaFwl5kgij404i789beNp7asI+n1jewt6Wb\nopxMPra0kuvPr6J2bhkZx+vG6WoOLqYe2ASNG5OzGm4ZOmfLYBYJhkVe+MVggqrSOaNfMRE5KQr+\nkHN3Nu5r4xcbGvjFmw3sbu4iM8N4/4JpfOPDi7m6pmL4tWDbGoL+913JG5mathzdlz8tmMWw9qZg\nNEzBtOQEW9Fgwq++aPDILYX5HxyXCapEJHUpBb+ZXQPcTbDY+v3ufteg/WXAA8B8oAe42d3fSu4r\nBe4HzgY8ue+VUauBDKsrGudfX9nFY6vr2XGwk0iG8b75U7nt8jO5+qwKSvOTN1Z1H4amndCcnLL3\nUF0wr0z/3C/ZRcEQxXM/BZXnB4FfOH2iqiUio2DE4DezCHAPcBWwB1htZivdfeAyMXcC69z942a2\nOHn8Fcl9dwO/dPdPmFk2oLtaxlBPrI9HXt3ND35Tx8GOKO+ZN4Vb3zeTa2Z0UNK5Aw69CE8nF4U4\nvDMY3jhQQXkwxUDtF4ObmWacq0mzREImlf9HLwfq3H07gJk9BqwABgZ/DXAXgLtvMbNqM6sgaP1/\nAPhCcl8UGGY1BTlVvfE+/m11Pfe8WMeBtl4uPaOQuxb/ipn1z8BzuwccaUE/+9QzoWr50WXkyqqD\n6XqPN+2viIRGKsE/C6gf8H4PMPj2xPXA9cAqM1sOzAWqgD6gCfhnM1sKrAXucPfOwV9iZrcAtwDM\nmaMLgKlKJJzH1+7hO8+/zb7WHi6sLuOHl/dx9pqvw1t1sOijcP7ngsUgpi0MpiLoX0VJRNLSaP0N\nfxdwt5mtAzYAbxCEfiZwPnC7u79qZncD3wT+6+ATuPt9wH0QTNI2SuUKtc0Nbfzlzzbw+u4Wls0u\n5R+uW8D7dv0Ae/YHwXS9n/95cIeqiMgAqQT/XmD2gPdVyW1HuHsbcBOABYO/dwDbCfrz97j7q8lD\nHycIfjlZfXFY9wgk4nQXzeGBjc7/XdtDQV4e3/rkUv5wWj328+ug+Z2gf/6q/65uGxEZVirBvxpY\nYGbzCAL/BuAzAw9IjtzpSvbhfwl4Kflj0GZm9Wa2yN23Elzw3YScnJ5WePzmYOpgIA/4CvDl7Ay8\nsIrIusrgbtjS2fD5lXDGpRNaXBGZ3EYMfnePm9ltwLMEwzkfcPeNZnZrcv+9wBLgITNzYCPwxQGn\nuB14JDmiZzvJvwwkRYd3wo9uwA9t41+mfo0f7J3P+6d1cNuyCNUZTcH+w7vgoi/D5X8JOYUTXWIR\nmeS0EMtktvv3JB79DL3RKH/c81XeiJzLHVcu4KaL55E1GjNjikhoaCGWEIi+/iiRJ2+nPjGNL8b+\nkvcuX87/uWIh5UU5E100ETnNKfgnk542vHUPbz//QxZtu59X+mp4bN7f8o/XLufM6bpQKyKjQ8E/\nURo3w++/D617ji4WEm3HgEXAszlXU/yHd3P3wsqJLqmIhIyCfyI0vQ0PXhusCzttAUw9k/1TL+In\n2xJs7y3h8vcu59prriVD/fgiMgYU/OOtZTf863XB0oC3/Ia+KfP53gt13P3rt5kzJZ/v3Xw+Z88q\nmehSikiIKfjHU/sB+JcVEO2AL/yCA9lV3HH/7/n99mauW1bJ3378nFNf6UpEZARKmfHSfRj+9ePQ\nvh8+/3Ne6azkK/+0iu5oH//wiXP5xAVVI694JSIyChT846G3Ax75JBzaBp/5MS90zuXWh19jzpR8\n7v2T8zViR0TGlYJ/rMV64LEbYe/r8Kl/4anORXztsbUsmVnMQzcvZ0pB9kSXUETSjIaNjKWmrcGF\n3B0vwXXf58edS/nqo29w3pxSHvnj9yj0RWRCqMU/FmLdsOrb8PJ3ILsArv8nHmy/kL9+8k0uWTCN\nf/zcBeRn659eRCaG0me0vfMCPPV1OLwDzv00XP133LO6lX94dhNX11Tw3c+cR07mMIubi4iMEwX/\naGk/AM/eCW89DlPmw+d/TnzuB/j7Z7bww5d3cN2ySv7hk0s1uZqITDgF/6mI9cC2Z+HNH8O254Jt\nl/0FXPw1DvYat//wNV7ZfogvvK+a/3ZtDRkZGq4pIhNPwX+yEn2wcxW8+RPYvBJ626CwAi78UvCY\nOp919S18+eG1NHdG+fYnl/KHF1RNdKlFRI5Q8J+Mw7vgoT+All2QXQRL/gDO/SRUfwAimbg7j766\nm79euZHpxTn89Mvv0/QLIjLpKPhTFeuBH38eulvgEw/Aoo9AVt6R3T2xPv7bz9/ix2v2cOnCcu6+\nYRml+RquKSKTj4I/Vc/8OTSsgxsehcUfOWbXzoOd3Pbo67y1t42vfvBM7rhyIRH154vIJJXSEBMz\nu8bMtppZnZl9c5j9ZWb2MzN708xeM7OzB+2PmNkbZvbUaBV8XL3xMLz+ELz/60NCf+X6fVz73Zep\nb+7m/s/X8vWrFyn0RWRSG7HFb2YR4B7gKmAPsNrMVrr7pgGH3Qmsc/ePm9ni5PFXDNh/B7AZKB61\nko+XhvXwiz+DeZfCB//Lkc3d0T7+x1MbefS1ei6YW8b/vfE8ZpXmneBEIiKTQyot/uVAnbtvd/co\n8BiwYtAxNcALAO6+Bag2swoAM6sCPgrcP2qlHi/dh+HfPgf5U4N+/YzgxqttB9pZcc/LPPpaPf/x\nsvk8dstFCn0ROW2k0sc/C6gf8H4P8J5Bx6wHrgdWmdlyYC5QBRwAvgP8OXB6TUGZSMATtwTLIt78\nSyiYBsBP1tTzX3/+FgXZmTx083IuXVg+wQUVETk5o3Ub6V1AqZmtA24H3gD6zOxaoNHd1450AjO7\nxczWmNmapqamUSrWKVj1reCmrGv+HqpqAXhhywH+8+Nvct7sMp654xKFvoicllJp8e8FZg94X5Xc\ndoS7twE3AViwmsgOYDvwaeBjZvYRIBcoNrOH3f2zg7/E3e8D7gOora31k6/KKNryNLz4P4O5di78\nEgBtPTHufOItFlYU8uDNF2q+HRE5baXS4l8NLDCzeWaWDdwArBx4gJmVJvcBfAl4yd3b3P0v3L3K\n3auTn3thuNCfVHa+DD/5Asw6H679TrA2LvD3T2+hsb2H//WJpQp9ETmtjdjid/e4md0GPAtEgAfc\nfaOZ3Zrcfy+wBHjIzBzYCHxxDMs8dhrWw6M3Qlk1/IfHITsfgN/VHeTR13bzx5fMY9ns0okto4jI\nKTL3ie1VGU5tba2vWbNmfL/00DvwwIcgMxdufhZKZgHQFY3zoe+8RMSMZ+74AHnZau2LyORjZmvd\nvTaVY3XnLkBbQ7BSlifgcz87EvoA33r2beqbu/m3Wy5S6ItIKCj4uw/Dw9dDVzP80ZMwbcGRXWt3\nHeaff7eDz100l/ecMXUCCykiMnrSO/ijnfCjT8OhuqBPf9b5R3b1xPr4xk/fpLIkj298ePEEFlJE\nZHSld/A/9XXYsxo++RCccekxu777wjbqGjt46OblFOak9z+TiIRL+q4DuPUZePMxuOQ/Qc3Hjtm1\nZX8b9/77dj5xQZVu0hKR0EnP4O8+DE9+DaafBR/4z0N23/38NvKzIvyXjy6ZgMKJiIyt9Az+X94J\nnU1w3fch89jFUt4+0M4zb+3nCxdXayEVEQml9Av+t5+F9T+C9/8pVC4bsvv7L9aRnx3hpovnTUDh\nRETGXnoFf3dL0MVTvgQu/fMhu3ce7GTl+n189qK5TClQa19Ewim9hqs895fQcQBueAQyc4bs/sFv\n3iEzksGXLlFrX0TCK31a/NueD5ZQvPiOY8br99vb0s1PX9/DjRfOZnpR7gQUUERkfKRH8Pe0wpNf\nhfLFcNmQJYMB+Md/fwcz+JNL549z4URExld6dPW88LfQ3gCf+tdhu3ga23p4bHU9n7igikotoSgi\nIRf+Fr87bPwZnPVxqLpg2EP+adV2+hLOly89c5wLJyIy/sIf/E1bgjH7Z1w+7O5DHb08/PvdrFha\nyZyp+eNcOBGR8Rf+4N+xKnied8mwux/47Q564n38x8vVty8i6SH8wb9zFZTMCVbVGqS1K8ZDv9vF\nR86eyZnTi8a/bCIiEyDcwZ9IBGvoVr9/2N0/em03Hb1xvnK5+vZFJH2EO/gbN0F383G7eZ7e0MB5\nc0qpqSwe54KJiEyclILfzK4xs61mVmdmQwbCm1mZmf3MzN40s9fM7Ozk9tlm9qKZbTKzjWZ2x2hX\n4IR2Jvv3q4cGf0NrNxv2tnJVTcW4FklEZKKNGPxmFgHuAT4M1AA3mlnNoMPuBNa5+7nA54G7k9vj\nwJ+5ew1wEfCVYT47dnasCvr2S2cP2fX85kYArlqi4BeR9JJKi385UOfu2909CjwGrBh0TA3wAoC7\nbwGqzazC3Rvc/fXk9nZgMzCL8ZDog10vD9vaB3h+0wGqp+Zz5vTCcSmOiMhkkUrwzwLqB7zfw9Dw\nXg9cD2Bmy4G5QNXAA8ysGjgPeHW4LzGzW8xsjZmtaWpqSqXsJ3bgrWCqhnkfGLKrozfOK+8c4sol\nFZjZqX+XiMhpZLQu7t4FlJrZOuB24A2gr3+nmRUCPwW+5u5tw53A3e9z91p3ry0vH4XlDvvH7w8z\nouelt5uI9iXUvy8iaSmVuXr2AgM7yauS245IhvlNABY0oXcA25PvswhC/xF3f2IUypyanatgynwo\nrhyy61ebDlCWn8UFc8vGrTgiIpNFKi3+1cACM5tnZtnADcDKgQeYWWlyH8CXgJfcvS35I/BDYLO7\n/+/RLPgJ9cVh1++GHcYZ70vwwpZGLl88ncxIuEeziogMZ8QWv7vHzew24FkgAjzg7hvN7Nbk/nuB\nJcBDZubARuCLyY9fDHwO2JDsBgK4092fHuV6HGv/euhtG/bC7uqdh2ntjnG1unlEJE2lNC1zMqif\nHrTt3gGvXwEWDvO5l4Hxv3q64/jj95/ffIDszAwuWTAK1xFERE5D4ezr2LkKpi2ComNb9e7OrzYd\n4OL5UynISY+lCEREBgtf8PfFYPfvh+3f39bYwe7mLq5UN4+IpLHwBf++dRDtGHYY5682HQDgSt2t\nKyJpLHzBv/Ol4HmY/v3nNh1g6exSKoq1mLqIpK/wBf+OVTC9BgqmHbO5sa2H9fUtXLVk+gQVTERk\ncghX8MejUP/qcUbzJCdlq5kx3qUSEZlUwhX8e9dCrGvYC7vPbz7A7Cl5LKzQpGwikt7CFfw7VwEG\ncy8+ZnNnb5yX6w5y1ZIZmpRNRNJe+IK/4mzIn3LM5lXbDhKNJ7iyRv37IiLhCf54L9S/dtxunpK8\nLJZXTxnmgyIi6SU8t69GsuFPXgqeB6lr7ODcqhJNyiYiQpiC3wzKFw27q7U7xuwp+eNcIBGRySkt\nmsCt3TFK8sLzGycicipCH/zuTmt3jNK8oV1AIiLpKPTB39Ebpy/hlORlTXRRREQmhdAHf0tXDICS\nfAW/iAikQfC3dieDXy1+EREgjYK/VMEvIgKkGPxmdo2ZbTWzOjP75jD7y8zsZ2b2ppm9ZmZnp/rZ\nsXakxa+uHhERIIXgN7MIcA/wYaAGuNHMagYddiewzt3PBT4P3H0Snx1T/X38GtUjIhJIpcW/HKhz\n9+3uHgUeA1YMOqYGeAHA3bcA1WZWkeJnx5T6+EVEjpVK8M8C6ge835PcNtB64HoAM1sOzAWqUvzs\nmGrpjpKdmUFuVugvZ4iIpGS00vAuoNTM1gG3A28AfSdzAjO7xczWmNmapqamUSoWtHXHKMnL0nTM\nIiJJqcxjsBeYPeB9VXLbEe7eBtwEYEHC7gC2A3kjfXbAOe4D7gOora311Io/spaumEb0iIgMkEqL\nfzWwwMzmmVk2cAOwcuABZlaa3AfwJeCl5I/BiJ8da63JFr+IiARGbPG7e9zMbgOeBSLAA+6+0cxu\nTe6/F1jYuuIkAAALEklEQVQCPGRmDmwEvniiz45NVYbX0hWjsjR3PL9SRGRSS2nKSnd/Gnh60LZ7\nB7x+BViY6mfHU2t3jMUziybq60VEJp3QD3XRzJwiIscKdfDH+hJ09MbVxy8iMkCog7+tf54eTdcg\nInJEqINfd+2KiAwV6uBv0QRtIiJDhDr41eIXERkq3MHfpeAXERks3MGvRVhERIYIdfD3z8VfrOAX\nETki1MHf2h2jMCeTrEioqykiclJCnYgt3VH174uIDBLq4G/TzJwiIkOEOvhbuhT8IiKDhTr4W7tj\nmq5BRGSQUAd/i7p6RESGCG3wu3uw+pZa/CIixwht8PfEEkTjCbX4RUQGCW3wH71rV4uwiIgMFNrg\nb+mOApqnR0RksJSC38yuMbOtZlZnZt8cZn+JmT1pZuvNbKOZ3TRg358mt71lZo+a2bisfN4/QZtG\n9YiIHGvE4DezCHAP8GGgBrjRzGoGHfYVYJO7LwUuA75tZtlmNgv4KlDr7mcDEeCGUSz/cbVoSmYR\nkWGl0uJfDtS5+3Z3jwKPASsGHeNAkZkZUAg0A/Hkvkwgz8wygXxg36iUfASai19EZHipBP8soH7A\n+z3JbQN9D1hCEOobgDvcPeHue4FvAbuBBqDV3Z875VKn4Mhc/OrqERE5xmhd3P0QsA6oBJYB3zOz\nYjMrI/jrYF5yX4GZfXa4E5jZLWa2xszWNDU1nXKBWrtjRDKMopzMUz6XiEiYpBL8e4HZA95XJbcN\ndBPwhAfqgB3AYuBKYIe7N7l7DHgCeN9wX+Lu97l7rbvXlpeXn2w9hmjpjlKcm0nQ+yQiIv1SCf7V\nwAIzm2dm2QQXZ1cOOmY3cAWAmVUAi4Dtye0XmVl+sv//CmDzaBX+RFq745Tmawy/iMhgI/aDuHvc\nzG4DniUYlfOAu280s1uT++8F/gZ40Mw2AAZ8w90PAgfN7HHgdYKLvW8A941NVY7V0hXVylsiIsNI\nqQPc3Z8Gnh607d4Br/cBVx/ns38F/NUplPFdaeuOqcUvIjKMEN+5q5k5RUSGE9rg11z8IiLDC2Xw\nJxLJKZnV4hcRGSKUwd/eG8ddd+2KiAwnlMF/5K5dBb+IyBDhDP7+ufg1qkdEZIhQBr/m4hcROb5Q\nBv/RFr+CX0RksFAGf4v6+EVEjiuUwa+5+EVEji+0wZ+TmUFuVmSiiyIiMumEM/i7dPOWiMjxhDL4\nW7qjurArInIcoQx+TdcgInJ8oQz+lq4YJXm6eUtEZDihDP42tfhFRI4rlMHfoimZRUSOK3TBH40n\n6Ir2qcUvInIcKQW/mV1jZlvNrM7MvjnM/hIze9LM1pvZRjO7acC+UjN73My2mNlmM3vvaFZgME3X\nICJyYiMGv5lFgHuADwM1wI1mVjPosK8Am9x9KXAZ8G0z67+6ejfwS3dfDCwFNo9S2Yelu3ZFRE4s\nlRb/cqDO3be7exR4DFgx6BgHiszMgEKgGYibWQnwAeCHAO4edfeWUSv9MFo1M6eIyAmlEvyzgPoB\n7/cktw30PWAJsA/YANzh7glgHtAE/LOZvWFm95tZwakX+/jU4hcRObHRurj7IWAdUAksA75nZsVA\nJnA+8AN3Pw/oBIZcIwAws1vMbI2ZrWlqanrXBemfmVOLsIiIDC+V4N8LzB7wviq5baCbgCc8UAfs\nABYT/HWwx91fTR73OMEPwRDufp+717p7bXl5+cnU4Rhq8YuInFgqwb8aWGBm85IXbG8AVg46Zjdw\nBYCZVQCLgO3uvh+oN7NFyeOuADaNSsmPo7/FX5ybOZZfIyJy2hoxHd09bma3Ac8CEeABd99oZrcm\n998L/A3woJltAAz4hrsfTJ7iduCR5I/GdoK/DsZMa3eMopxMMiOhu0VBRGRUpNQsdvengacHbbt3\nwOt9wNXH+ew6oPYUynhSWrtjlGgMv4jIcYWuWayZOUVETix0wd/Spbn4RUROJHTBrxa/iMiJhTT4\nNYZfROR4QhX87q4Wv4jICEIV/F3RPmJ9rj5+EZETCFXw665dEZGRhSr4j8zTo+AXETmuUAW/Wvwi\nIiMLWfAn5+JXH7+IyHGFLPjV4hcRGUmogl9z8YuIjCxUwd/aHSOSYRRkRya6KCIik1aogr+lO0Zp\nXhbB0r8iIjKcUAW/7toVERlZuIK/S3Pxi4iMJFzBrxa/iMiIQhX8Ld1R3bUrIjKCUAV/a5da/CIi\nI0kp+M3sGjPbamZ1ZvbNYfaXmNmTZrbezDaa2U2D9kfM7A0ze2q0Cj6Yu/PBxdNZOrt0rL5CRCQU\nRlxs3cwiwD3AVcAeYLWZrXT3TQMO+wqwyd3/wMzKga1m9oi7R5P77wA2A8WjW/xjysl3bjhvrE4v\nIhIaqbT4lwN17r49GeSPASsGHeNAkQUD6AuBZiAOYGZVwEeB+0et1CIi8q6lEvyzgPoB7/cktw30\nPWAJsA/YANzh7onkvu8Afw4kEBGRCTdaF3c/BKwDKoFlwPfMrNjMrgUa3X3tSCcws1vMbI2ZrWlq\nahqlYomIyGCpBP9eYPaA91XJbQPdBDzhgTpgB7AYuBj4mJntJOgi+qCZPTzcl7j7fe5e6+615eXl\nJ1kNERFJVSrBvxpYYGbzzCwbuAFYOeiY3cAVAGZWASwCtrv7X7h7lbtXJz/3grt/dtRKLyIiJ23E\nUT3uHjez24BngQjwgLtvNLNbk/vvBf4GeNDMNgAGfMPdD45huUVE5F0yd5/oMgxRW1vra9asmehi\niIicNsxsrbvXpnJsqO7cFRGRkU3KFr+ZNQG73uXHpwHp2M2keqcX1Tu9pFLvue6e0siYSRn8p8LM\n1qT6506YqN7pRfVOL6Ndb3X1iIikGQW/iEiaCWPw3zfRBZggqnd6Ub3Ty6jWO3R9/CIicmJhbPGL\niMgJhCb4R1osJkzM7AEzazSztwZsm2JmvzKzbcnnsoks42gzs9lm9qKZbUou9nNHcnvY651rZq8N\nWOTovye3h7re/QYv4pRG9d5pZhvMbJ2ZrUluG7W6hyL4BywW82GgBrjRzGomtlRj6kHgmkHbvgn8\n2t0XAL9Ovg+TOPBn7l4DXAR8Jfm/cdjr3Qt80N2XEsx8e42ZXUT4692vfxGnfulSb4DL3X3ZgGGc\no1b3UAQ/qS0WExru/hLBYjcDrQAeSr5+CLhuXAs1xty9wd1fT75uJwiDWYS/3u7uHcm3WcmHE/J6\nw3EXcQp9vU9g1OoeluBPZbGYsKtw94bk6/1AxUQWZiyZWTVwHvAqaVDvZHfHOqAR+JW7p0W9GX4R\np3SoNwQ/7s+b2VozuyW5bdTqPuLsnHL6cXc3s1AO1zKzQuCnwNfcvS1Y7TMQ1nq7ex+wzMxKgZ+Z\n2dmD9oeu3gMXcTKzy4Y7Joz1HuD97r7XzKYDvzKzLQN3nmrdw9LiT2WxmLA7YGYzAZLPjRNcnlFn\nZlkEof+Iuz+R3Bz6evdz9xbgRYLrO2Gv9/EWcQp7vQFw973J50bgZwTd2aNW97AEfyqLxYTdSuCP\nkq//CPj5BJZl1FnQtP8hsNnd//eAXWGvd3mypY+Z5QFXAVsIeb1PsIhTqOsNYGYFZlbU/xq4GniL\nUax7aG7gMrOPEPQJ9i8W83cTXKQxY2aPApcRzNh3APgr4P8BPwbmEMxs+il3H3wB+LRlZu8HVgEb\nONrneydBP3+Y630uwYW8CEFD7cfu/j/MbCohrvdAya6e/+Tu16ZDvc3sDIJWPgTd8T9y978bzbqH\nJvhFRCQ1YenqERGRFCn4RUTSjIJfRCTNKPhFRNKMgl9EJM0o+EVE0oyCX0QkzSj4RUTSzP8HF0QC\ns3u0vsYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcf741a19d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Net Hyper params:\n",
      "Layers: [784, 650, 650]\n",
      "Learning Rate: 1e-05\n",
      "Learning rate decay: 0.0001\n",
      "Epoch : 1 Loss : 2.074  Train Accuracy: 0.443 Validation Accuracy: 0.651\n",
      "Epoch : 2 Loss : 1.863  Train Accuracy: 0.673 Validation Accuracy: 0.729\n",
      "Epoch : 3 Loss : 1.734  Train Accuracy: 0.719 Validation Accuracy: 0.759\n",
      "Epoch : 4 Loss : 1.636  Train Accuracy: 0.741 Validation Accuracy: 0.773\n",
      "Epoch : 5 Loss : 1.555  Train Accuracy: 0.756 Validation Accuracy: 0.786\n",
      "Epoch : 6 Loss : 1.487  Train Accuracy: 0.767 Validation Accuracy: 0.795\n",
      "Epoch : 7 Loss : 1.429  Train Accuracy: 0.775 Validation Accuracy: 0.801\n",
      "Epoch : 8 Loss : 1.378  Train Accuracy: 0.782 Validation Accuracy: 0.806\n",
      "Epoch : 9 Loss : 1.334  Train Accuracy: 0.786 Validation Accuracy: 0.811\n",
      "Epoch : 10 Loss : 1.294  Train Accuracy: 0.791 Validation Accuracy: 0.815\n",
      "Epoch : 11 Loss : 1.259  Train Accuracy: 0.794 Validation Accuracy: 0.818\n",
      "Epoch : 12 Loss : 1.227  Train Accuracy: 0.797 Validation Accuracy: 0.820\n",
      "Epoch : 13 Loss : 1.198  Train Accuracy: 0.800 Validation Accuracy: 0.822\n",
      "Epoch : 14 Loss : 1.172  Train Accuracy: 0.803 Validation Accuracy: 0.825\n",
      "Epoch : 15 Loss : 1.148  Train Accuracy: 0.805 Validation Accuracy: 0.827\n",
      "Epoch : 16 Loss : 1.126  Train Accuracy: 0.807 Validation Accuracy: 0.830\n",
      "Epoch : 17 Loss : 1.106  Train Accuracy: 0.809 Validation Accuracy: 0.832\n",
      "Epoch : 18 Loss : 1.087  Train Accuracy: 0.811 Validation Accuracy: 0.833\n",
      "Epoch : 19 Loss : 1.070  Train Accuracy: 0.813 Validation Accuracy: 0.834\n",
      "Epoch : 20 Loss : 1.054  Train Accuracy: 0.814 Validation Accuracy: 0.836\n",
      "Epoch : 21 Loss : 1.039  Train Accuracy: 0.816 Validation Accuracy: 0.837\n",
      "Epoch : 22 Loss : 1.025  Train Accuracy: 0.817 Validation Accuracy: 0.838\n",
      "Epoch : 23 Loss : 1.012  Train Accuracy: 0.818 Validation Accuracy: 0.840\n",
      "Epoch : 24 Loss : 1.000  Train Accuracy: 0.819 Validation Accuracy: 0.841\n",
      "Epoch : 25 Loss : 0.988  Train Accuracy: 0.820 Validation Accuracy: 0.842\n",
      "Epoch : 26 Loss : 0.977  Train Accuracy: 0.821 Validation Accuracy: 0.843\n",
      "Epoch : 27 Loss : 0.967  Train Accuracy: 0.822 Validation Accuracy: 0.844\n",
      "Epoch : 28 Loss : 0.957  Train Accuracy: 0.823 Validation Accuracy: 0.845\n",
      "Epoch : 29 Loss : 0.948  Train Accuracy: 0.824 Validation Accuracy: 0.846\n",
      "Epoch : 30 Loss : 0.939  Train Accuracy: 0.825 Validation Accuracy: 0.847\n",
      "Epoch : 31 Loss : 0.931  Train Accuracy: 0.826 Validation Accuracy: 0.847\n",
      "Epoch : 32 Loss : 0.923  Train Accuracy: 0.826 Validation Accuracy: 0.848\n",
      "Epoch : 33 Loss : 0.915  Train Accuracy: 0.827 Validation Accuracy: 0.848\n",
      "Epoch : 34 Loss : 0.908  Train Accuracy: 0.828 Validation Accuracy: 0.849\n",
      "Epoch : 35 Loss : 0.901  Train Accuracy: 0.829 Validation Accuracy: 0.849\n",
      "Epoch : 36 Loss : 0.895  Train Accuracy: 0.829 Validation Accuracy: 0.850\n",
      "Epoch : 37 Loss : 0.888  Train Accuracy: 0.830 Validation Accuracy: 0.851\n",
      "Epoch : 38 Loss : 0.882  Train Accuracy: 0.831 Validation Accuracy: 0.851\n",
      "Epoch : 39 Loss : 0.876  Train Accuracy: 0.832 Validation Accuracy: 0.852\n",
      "Epoch : 40 Loss : 0.871  Train Accuracy: 0.832 Validation Accuracy: 0.853\n",
      "Epoch : 41 Loss : 0.866  Train Accuracy: 0.833 Validation Accuracy: 0.853\n",
      "Epoch : 42 Loss : 0.860  Train Accuracy: 0.833 Validation Accuracy: 0.853\n",
      "Epoch : 43 Loss : 0.856  Train Accuracy: 0.834 Validation Accuracy: 0.854\n",
      "Epoch : 44 Loss : 0.851  Train Accuracy: 0.835 Validation Accuracy: 0.854\n",
      "Epoch : 45 Loss : 0.846  Train Accuracy: 0.835 Validation Accuracy: 0.855\n",
      "Epoch : 46 Loss : 0.842  Train Accuracy: 0.836 Validation Accuracy: 0.855\n",
      "Epoch : 47 Loss : 0.837  Train Accuracy: 0.836 Validation Accuracy: 0.855\n",
      "Epoch : 48 Loss : 0.833  Train Accuracy: 0.836 Validation Accuracy: 0.855\n",
      "Epoch : 49 Loss : 0.829  Train Accuracy: 0.837 Validation Accuracy: 0.856\n",
      "Epoch : 50 Loss : 0.825  Train Accuracy: 0.837 Validation Accuracy: 0.857\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcf73f32d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Hyper parameter search\n",
    "\n",
    "hparams = {\n",
    "    \"layers\": [[28*28,500,250], [28*28,500,500],[28*28,650,650]],\n",
    "    \"lr\": [1e-3,1e-4,1e-5],\n",
    "    \"lr_decay\": [10e-5]\n",
    "}\n",
    "\n",
    "bestModel, bestValAcc = hyperparamsearch(50, hparams, mnist_train, mnist_val)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_MNIST(\n",
      "  (fc1): Linear(in_features=784, out_features=500)\n",
      "  (dropout1): Dropout(p=0.0)\n",
      "  (fc2): Linear(in_features=500, out_features=500)\n",
      "  (dropout2): Dropout(p=0.0)\n",
      "  (fc3): Linear(in_features=500, out_features=10)\n",
      ")\n",
      "0.9804\n"
     ]
    }
   ],
   "source": [
    "print bestModel\n",
    "print bestValAcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model was had the following hyperparameters:\n",
    "[784, 500, 500]\n",
    "0.001\n",
    "0.0001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 Loss : 0.253  Train Accuracy: 0.925 Validation Accuracy: 0.959\n",
      "Epoch : 2 Loss : 0.095  Train Accuracy: 0.971 Validation Accuracy: 0.971\n",
      "Epoch : 3 Loss : 0.055  Train Accuracy: 0.982 Validation Accuracy: 0.972\n",
      "Epoch : 4 Loss : 0.035  Train Accuracy: 0.989 Validation Accuracy: 0.976\n",
      "Epoch : 5 Loss : 0.022  Train Accuracy: 0.993 Validation Accuracy: 0.977\n",
      "Epoch : 6 Loss : 0.015  Train Accuracy: 0.995 Validation Accuracy: 0.979\n",
      "Epoch : 7 Loss : 0.010  Train Accuracy: 0.997 Validation Accuracy: 0.978\n",
      "Epoch : 8 Loss : 0.006  Train Accuracy: 0.998 Validation Accuracy: 0.978\n",
      "Epoch : 9 Loss : 0.005  Train Accuracy: 0.998 Validation Accuracy: 0.978\n",
      "Epoch : 10 Loss : 0.002  Train Accuracy: 0.999 Validation Accuracy: 0.978\n",
      "Epoch : 11 Loss : 0.002  Train Accuracy: 0.999 Validation Accuracy: 0.978\n",
      "Epoch : 12 Loss : 0.002  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 13 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 14 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 15 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 16 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 17 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 18 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980\n",
      "Epoch : 19 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.981\n",
      "Epoch : 20 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980\n",
      "Epoch : 21 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980\n",
      "Epoch : 22 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980\n",
      "Epoch : 23 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 24 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 25 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 26 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 27 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 28 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 29 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 30 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 31 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 32 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 33 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 34 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 35 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 36 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 37 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 38 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 39 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 40 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 41 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 42 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 43 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 44 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 45 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 46 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 47 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 48 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 49 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 50 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 51 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 52 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 53 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 54 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 55 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 56 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 57 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 58 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 59 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 60 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 61 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 62 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 63 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 64 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 65 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 66 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 67 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 68 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 69 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 70 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 71 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 72 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 73 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 74 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 75 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 76 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 77 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 78 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 79 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 80 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 81 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 82 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 83 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 84 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 85 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 86 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 87 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 88 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 89 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 90 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 91 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 92 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 93 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 94 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 95 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 96 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 97 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 98 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 99 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 100 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n"
     ]
    }
   ],
   "source": [
    "#Training the best model for 100 epochs\n",
    "layers = [784, 500, 500]\n",
    "lr = 0.001\n",
    "lr_decay=0.0001\n",
    "model = MLP_MNIST(layers,10)\n",
    "model = GlorotInitialize(model)\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=lr,lr_decay = lr_decay)\n",
    "train_results = train(model,100,mnist_train,optimizer,mnist_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_MNIST(\n",
      "  (fc1): Linear(in_features=784, out_features=500)\n",
      "  (dropout1): Dropout(p=0.0)\n",
      "  (fc2): Linear(in_features=500, out_features=500)\n",
      "  (dropout2): Dropout(p=0.0)\n",
      "  (fc3): Linear(in_features=500, out_features=10)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmDhjwW+MMXHGgt8YY+KMBb8xxsSZ/w8JNVpDP/eEQAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcf73f4a7d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(bestNetwork, epoch_loss, train_accuracy, val_accuracy, test_accuracy) = train_results\n",
    "\n",
    "print bestNetwork\n",
    "\n",
    "plt.plot(train_accuracy, label = 'Train Accuracy')\n",
    "plt.plot(val_accuracy, label= 'Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doubling the number of parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation accuracy: 0.9804\n",
      "Best model: MLP_MNIST(\n",
      "  (fc1): Linear(in_features=784, out_features=500)\n",
      "  (dropout1): Dropout(p=0.0)\n",
      "  (fc2): Linear(in_features=500, out_features=500)\n",
      "  (dropout2): Dropout(p=0.0)\n",
      "  (fc3): Linear(in_features=500, out_features=10)\n",
      ")\n",
      "torch.Size([500, 784])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 500])\n",
      "torch.Size([500])\n",
      "torch.Size([10, 500])\n",
      "torch.Size([10])\n",
      "Total number of parameters in best model: 648010\n",
      "1296020\n"
     ]
    }
   ],
   "source": [
    "print 'Best validation accuracy: ' + repr(bestValAcc)\n",
    "print 'Best model: ' + repr(bestModel)\n",
    "params = list(bestModel.parameters())\n",
    "sum_params=0\n",
    "for p in params:\n",
    "    print p.size()\n",
    "    if len(p.size()) > 1:\n",
    "        sum_params += p.size()[0]*p.size()[1]\n",
    "    else:\n",
    "        sum_params += p.size()[0]\n",
    "print ('Total number of parameters in best model: %d' %sum_params)\n",
    "print 2*sum_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently there are: (784+(784*500) + 800+(800*450)+ (450*10)+450+ 10) = 992,960 parameters\n",
    "\n",
    "\n",
    "(in_size+(in_size*size1) + size1 + (size1*size2) + size2 + (size2*outsize) + outsize\n",
    "\n",
    "We would like to create a new model with twice as many parameters, 1,985,920 parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1005, 784])\n",
      "torch.Size([1005])\n",
      "torch.Size([500, 1005])\n",
      "torch.Size([500])\n",
      "torch.Size([10, 500])\n",
      "torch.Size([10])\n",
      "Total number of parameters in double size model: 1296935\n"
     ]
    }
   ],
   "source": [
    "layerDims = [784,1005,500]\n",
    "lr=0.001\n",
    "lr_decay = 0.0001\n",
    "Double_NN = MLP_MNIST(layerDims,10)\n",
    "params = list(Double_NN.parameters())\n",
    "sum_params=0\n",
    "for p in params:\n",
    "    print p.size()\n",
    "    if len(p.size()) > 1:\n",
    "        sum_params += p.size()[0]*p.size()[1]\n",
    "    else:\n",
    "        sum_params += p.size()[0]\n",
    "        \n",
    "print ('Total number of parameters in double size model: %d' %sum_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 Loss : 0.219  Train Accuracy: 0.934 Validation Accuracy: 0.967\n",
      "Epoch : 2 Loss : 0.081  Train Accuracy: 0.975 Validation Accuracy: 0.975\n",
      "Epoch : 3 Loss : 0.048  Train Accuracy: 0.985 Validation Accuracy: 0.974\n",
      "Epoch : 4 Loss : 0.027  Train Accuracy: 0.991 Validation Accuracy: 0.976\n",
      "Epoch : 5 Loss : 0.017  Train Accuracy: 0.994 Validation Accuracy: 0.977\n",
      "Epoch : 6 Loss : 0.011  Train Accuracy: 0.996 Validation Accuracy: 0.979\n",
      "Epoch : 7 Loss : 0.007  Train Accuracy: 0.997 Validation Accuracy: 0.980\n",
      "Epoch : 8 Loss : 0.003  Train Accuracy: 0.999 Validation Accuracy: 0.979\n",
      "Epoch : 9 Loss : 0.003  Train Accuracy: 0.999 Validation Accuracy: 0.978\n",
      "Epoch : 10 Loss : 0.002  Train Accuracy: 0.999 Validation Accuracy: 0.980\n",
      "Epoch : 11 Loss : 0.002  Train Accuracy: 0.999 Validation Accuracy: 0.980\n",
      "Epoch : 12 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 13 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 14 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 15 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 16 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 17 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 18 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 19 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 20 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 21 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 22 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 23 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 24 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 25 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 26 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 27 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 28 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 29 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 30 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 31 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980\n",
      "Epoch : 32 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980\n",
      "Epoch : 33 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980\n",
      "Epoch : 34 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980\n",
      "Epoch : 35 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 36 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 37 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 38 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 39 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 40 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 41 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 42 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 43 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 44 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 45 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 46 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979\n",
      "Epoch : 47 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 48 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 49 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 50 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 51 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 52 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 53 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 54 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 55 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 56 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 57 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 58 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 59 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 60 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 61 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 62 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 63 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 64 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 65 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 66 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 67 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 68 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 69 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 70 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 71 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 72 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 73 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 74 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 75 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 76 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 77 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 78 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 79 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 80 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 81 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 82 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 83 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978\n",
      "Epoch : 84 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977\n",
      "Epoch : 85 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977\n",
      "Epoch : 86 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977\n",
      "Epoch : 87 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977\n",
      "Epoch : 88 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977\n",
      "Epoch : 89 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977\n",
      "Epoch : 90 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977\n",
      "Epoch : 91 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977\n",
      "Epoch : 92 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977\n",
      "Epoch : 93 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977\n",
      "Epoch : 94 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977\n",
      "Epoch : 95 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977\n",
      "Epoch : 96 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977\n",
      "Epoch : 97 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977\n",
      "Epoch : 98 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977\n",
      "Epoch : 99 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977\n",
      "Epoch : 100 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977\n"
     ]
    }
   ],
   "source": [
    "Double_NN = GlorotInitialize(Double_NN)\n",
    "optimizer = torch.optim.Adagrad(Double_NN.parameters(), lr=lr,lr_decay=lr_decay)\n",
    "(Double_NN, epoch_loss, double_train_accuracy, double_val_accuracy, double_test_acc) = train(Double_NN,100,mnist_train,optimizer,mnist_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98\n"
     ]
    }
   ],
   "source": [
    "print max(double_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why does a network with double the size perform worse on the validation set?\n",
    "## GC: Here we need to explain why the model with double the capacity performed worse than the best model with half the capacity\n",
    "Here we have not increased regularization within the network.  By doubling the capacity of the model, we have also increased the inherent variance found within the model.  While the bias may have been reduced, the model now has an increased error from variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Set Size, Generalization Gap, and Standard Error\n",
    "\n",
    "First we'll randomly sample subsets of the training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 Loss : 0.385  Train Accuracy: 0.894 Validation Accuracy: 0.947 Test Accuracy: 0.943\n",
      "Epoch : 2 Loss : 0.151  Train Accuracy: 0.954 Validation Accuracy: 0.962 Test Accuracy: 0.959\n",
      "Epoch : 3 Loss : 0.103  Train Accuracy: 0.969 Validation Accuracy: 0.967 Test Accuracy: 0.968\n",
      "Epoch : 4 Loss : 0.075  Train Accuracy: 0.976 Validation Accuracy: 0.969 Test Accuracy: 0.965\n",
      "Epoch : 5 Loss : 0.057  Train Accuracy: 0.982 Validation Accuracy: 0.974 Test Accuracy: 0.974\n",
      "Epoch : 6 Loss : 0.040  Train Accuracy: 0.987 Validation Accuracy: 0.974 Test Accuracy: 0.974\n",
      "Epoch : 7 Loss : 0.031  Train Accuracy: 0.991 Validation Accuracy: 0.974 Test Accuracy: 0.974\n",
      "Epoch : 8 Loss : 0.024  Train Accuracy: 0.992 Validation Accuracy: 0.979 Test Accuracy: 0.976\n",
      "Epoch : 9 Loss : 0.017  Train Accuracy: 0.994 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 10 Loss : 0.013  Train Accuracy: 0.996 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 11 Loss : 0.009  Train Accuracy: 0.997 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 12 Loss : 0.007  Train Accuracy: 0.998 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 13 Loss : 0.005  Train Accuracy: 0.998 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 14 Loss : 0.004  Train Accuracy: 0.999 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 15 Loss : 0.003  Train Accuracy: 0.999 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 16 Loss : 0.003  Train Accuracy: 0.999 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 17 Loss : 0.002  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 18 Loss : 0.002  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 19 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.976\n",
      "Epoch : 20 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 21 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 22 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 23 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.978\n",
      "Epoch : 24 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 25 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 26 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 27 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 28 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 29 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 30 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.977\n",
      "Epoch : 31 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 32 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.977\n",
      "Epoch : 33 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.977\n",
      "Epoch : 34 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.978\n",
      "Epoch : 35 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.978\n",
      "Epoch : 36 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.978\n",
      "Epoch : 37 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.978\n",
      "Epoch : 38 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.978\n",
      "Epoch : 39 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 40 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 41 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 42 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 43 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 44 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 45 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 46 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 47 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 48 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 49 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 50 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 51 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 52 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 53 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 54 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 55 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 56 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 57 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 58 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 59 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 60 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 61 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 62 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 63 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 64 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 65 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 66 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 67 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 68 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 69 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 70 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 71 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 72 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 73 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 74 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 75 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 76 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 77 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 78 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 79 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 80 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 81 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 82 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 83 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 84 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 85 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 86 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 87 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 88 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 89 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 90 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 91 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 92 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 93 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 94 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 95 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 96 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 97 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 98 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 99 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 100 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 1 Loss : 0.374  Train Accuracy: 0.895 Validation Accuracy: 0.951 Test Accuracy: 0.946\n",
      "Epoch : 2 Loss : 0.149  Train Accuracy: 0.955 Validation Accuracy: 0.964 Test Accuracy: 0.961\n",
      "Epoch : 3 Loss : 0.097  Train Accuracy: 0.970 Validation Accuracy: 0.969 Test Accuracy: 0.969\n",
      "Epoch : 4 Loss : 0.071  Train Accuracy: 0.977 Validation Accuracy: 0.970 Test Accuracy: 0.971\n",
      "Epoch : 5 Loss : 0.051  Train Accuracy: 0.984 Validation Accuracy: 0.974 Test Accuracy: 0.974\n",
      "Epoch : 6 Loss : 0.037  Train Accuracy: 0.989 Validation Accuracy: 0.974 Test Accuracy: 0.976\n",
      "Epoch : 7 Loss : 0.026  Train Accuracy: 0.992 Validation Accuracy: 0.975 Test Accuracy: 0.976\n",
      "Epoch : 8 Loss : 0.019  Train Accuracy: 0.994 Validation Accuracy: 0.975 Test Accuracy: 0.977\n",
      "Epoch : 9 Loss : 0.013  Train Accuracy: 0.996 Validation Accuracy: 0.975 Test Accuracy: 0.977\n",
      "Epoch : 10 Loss : 0.009  Train Accuracy: 0.997 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 11 Loss : 0.007  Train Accuracy: 0.998 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 12 Loss : 0.004  Train Accuracy: 0.999 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 13 Loss : 0.004  Train Accuracy: 0.999 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 14 Loss : 0.003  Train Accuracy: 0.999 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 15 Loss : 0.002  Train Accuracy: 0.999 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 16 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.980\n",
      "Epoch : 17 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.980\n",
      "Epoch : 18 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 19 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 20 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 21 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.980\n",
      "Epoch : 22 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 23 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.980\n",
      "Epoch : 24 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.980\n",
      "Epoch : 25 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.980\n",
      "Epoch : 26 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.980\n",
      "Epoch : 27 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 28 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 29 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 30 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 31 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 32 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 33 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 34 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 35 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 36 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 37 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 38 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.980\n",
      "Epoch : 39 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 40 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 41 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 42 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 43 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 44 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 45 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 46 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 47 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 48 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 49 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 50 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 51 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 52 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 53 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 54 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 55 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 56 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 57 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.980\n",
      "Epoch : 58 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.980\n",
      "Epoch : 59 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.980\n",
      "Epoch : 60 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 61 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 62 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 63 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 64 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 65 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 66 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 67 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 68 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 69 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 70 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 71 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 72 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 73 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 74 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 75 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 76 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 77 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 78 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 79 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 80 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 81 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 82 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 83 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 84 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 85 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 86 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 87 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 88 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 89 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 90 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 91 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 92 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 93 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 94 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 95 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 96 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 97 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 98 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 99 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 100 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 1 Loss : 0.398  Train Accuracy: 0.890 Validation Accuracy: 0.943 Test Accuracy: 0.936\n",
      "Epoch : 2 Loss : 0.163  Train Accuracy: 0.950 Validation Accuracy: 0.962 Test Accuracy: 0.961\n",
      "Epoch : 3 Loss : 0.107  Train Accuracy: 0.968 Validation Accuracy: 0.968 Test Accuracy: 0.966\n",
      "Epoch : 4 Loss : 0.076  Train Accuracy: 0.976 Validation Accuracy: 0.973 Test Accuracy: 0.969\n",
      "Epoch : 5 Loss : 0.053  Train Accuracy: 0.984 Validation Accuracy: 0.974 Test Accuracy: 0.973\n",
      "Epoch : 6 Loss : 0.042  Train Accuracy: 0.987 Validation Accuracy: 0.974 Test Accuracy: 0.973\n",
      "Epoch : 7 Loss : 0.031  Train Accuracy: 0.990 Validation Accuracy: 0.974 Test Accuracy: 0.975\n",
      "Epoch : 8 Loss : 0.024  Train Accuracy: 0.992 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 9 Loss : 0.017  Train Accuracy: 0.994 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 10 Loss : 0.013  Train Accuracy: 0.996 Validation Accuracy: 0.975 Test Accuracy: 0.975\n",
      "Epoch : 11 Loss : 0.008  Train Accuracy: 0.998 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 12 Loss : 0.006  Train Accuracy: 0.998 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 13 Loss : 0.004  Train Accuracy: 0.999 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 14 Loss : 0.003  Train Accuracy: 0.999 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 15 Loss : 0.003  Train Accuracy: 0.999 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 16 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 17 Loss : 0.002  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 18 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 19 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 20 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 21 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 22 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 23 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.979\n",
      "Epoch : 24 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 25 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 26 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 27 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 28 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 29 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 30 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 31 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 32 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 33 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 34 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 35 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 36 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 37 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 38 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 39 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 40 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 41 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 42 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 43 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 44 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 45 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 46 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 47 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 48 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 49 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 50 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 51 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 52 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 53 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 54 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 55 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 56 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 57 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 58 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 59 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 60 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 61 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 62 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 63 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 64 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 65 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 66 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 67 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 68 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 69 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 70 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 71 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 72 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 73 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 74 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 75 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 76 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 77 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 78 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 79 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 80 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 81 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 82 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 83 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 84 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 85 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 86 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 87 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 88 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 89 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 90 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 91 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 92 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 93 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 94 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 95 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 96 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 97 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 98 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 99 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 100 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 1 Loss : 0.386  Train Accuracy: 0.894 Validation Accuracy: 0.947 Test Accuracy: 0.941\n",
      "Epoch : 2 Loss : 0.153  Train Accuracy: 0.953 Validation Accuracy: 0.962 Test Accuracy: 0.958\n",
      "Epoch : 3 Loss : 0.099  Train Accuracy: 0.969 Validation Accuracy: 0.968 Test Accuracy: 0.967\n",
      "Epoch : 4 Loss : 0.070  Train Accuracy: 0.978 Validation Accuracy: 0.971 Test Accuracy: 0.972\n",
      "Epoch : 5 Loss : 0.051  Train Accuracy: 0.984 Validation Accuracy: 0.972 Test Accuracy: 0.974\n",
      "Epoch : 6 Loss : 0.038  Train Accuracy: 0.988 Validation Accuracy: 0.974 Test Accuracy: 0.974\n",
      "Epoch : 7 Loss : 0.028  Train Accuracy: 0.991 Validation Accuracy: 0.975 Test Accuracy: 0.974\n",
      "Epoch : 8 Loss : 0.020  Train Accuracy: 0.994 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 9 Loss : 0.016  Train Accuracy: 0.995 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 10 Loss : 0.011  Train Accuracy: 0.997 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 11 Loss : 0.009  Train Accuracy: 0.997 Validation Accuracy: 0.975 Test Accuracy: 0.976\n",
      "Epoch : 12 Loss : 0.006  Train Accuracy: 0.998 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 13 Loss : 0.005  Train Accuracy: 0.999 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 14 Loss : 0.003  Train Accuracy: 0.999 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 15 Loss : 0.003  Train Accuracy: 0.999 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 16 Loss : 0.002  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 17 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 18 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 19 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 20 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 21 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 22 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 23 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 24 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 25 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 26 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 27 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 28 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 29 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 30 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 31 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 32 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 33 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 34 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 35 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 36 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 37 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 38 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 39 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 40 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 41 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 42 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 43 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 44 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 45 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 46 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 47 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 48 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 49 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 50 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 51 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 52 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 53 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 54 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 55 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 56 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 57 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 58 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 59 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 60 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 61 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 62 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 63 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 64 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 65 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 66 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 67 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 68 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 69 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 70 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 71 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 72 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 73 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 74 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 75 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 76 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 77 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 78 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 79 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 80 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 81 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 82 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 83 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 84 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 85 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 86 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 87 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 88 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 89 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 90 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 91 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 92 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 93 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 94 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 95 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 96 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 97 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 98 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 99 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 100 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 1 Loss : 0.381  Train Accuracy: 0.893 Validation Accuracy: 0.950 Test Accuracy: 0.944\n",
      "Epoch : 2 Loss : 0.148  Train Accuracy: 0.956 Validation Accuracy: 0.962 Test Accuracy: 0.960\n",
      "Epoch : 3 Loss : 0.099  Train Accuracy: 0.969 Validation Accuracy: 0.966 Test Accuracy: 0.965\n",
      "Epoch : 4 Loss : 0.072  Train Accuracy: 0.978 Validation Accuracy: 0.970 Test Accuracy: 0.968\n",
      "Epoch : 5 Loss : 0.053  Train Accuracy: 0.983 Validation Accuracy: 0.974 Test Accuracy: 0.972\n",
      "Epoch : 6 Loss : 0.040  Train Accuracy: 0.987 Validation Accuracy: 0.972 Test Accuracy: 0.974\n",
      "Epoch : 7 Loss : 0.030  Train Accuracy: 0.990 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 8 Loss : 0.021  Train Accuracy: 0.993 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 9 Loss : 0.016  Train Accuracy: 0.995 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 10 Loss : 0.012  Train Accuracy: 0.996 Validation Accuracy: 0.977 Test Accuracy: 0.975\n",
      "Epoch : 11 Loss : 0.010  Train Accuracy: 0.997 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 12 Loss : 0.008  Train Accuracy: 0.997 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 13 Loss : 0.005  Train Accuracy: 0.998 Validation Accuracy: 0.980 Test Accuracy: 0.978\n",
      "Epoch : 14 Loss : 0.003  Train Accuracy: 0.999 Validation Accuracy: 0.980 Test Accuracy: 0.979\n",
      "Epoch : 15 Loss : 0.003  Train Accuracy: 0.999 Validation Accuracy: 0.980 Test Accuracy: 0.979\n",
      "Epoch : 16 Loss : 0.003  Train Accuracy: 0.999 Validation Accuracy: 0.980 Test Accuracy: 0.978\n",
      "Epoch : 17 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 18 Loss : 0.002  Train Accuracy: 0.999 Validation Accuracy: 0.980 Test Accuracy: 0.980\n",
      "Epoch : 19 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 20 Loss : 0.001  Train Accuracy: 0.999 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 21 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.978\n",
      "Epoch : 22 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 23 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 24 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.980\n",
      "Epoch : 25 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.980\n",
      "Epoch : 26 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.980\n",
      "Epoch : 27 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 28 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.980\n",
      "Epoch : 29 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.980\n",
      "Epoch : 30 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.980\n",
      "Epoch : 31 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.980\n",
      "Epoch : 32 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.980\n",
      "Epoch : 33 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.979\n",
      "Epoch : 34 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 35 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 36 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.979\n",
      "Epoch : 37 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.979\n",
      "Epoch : 38 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.979\n",
      "Epoch : 39 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.979\n",
      "Epoch : 40 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 41 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 42 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 43 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 44 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 45 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.978\n",
      "Epoch : 46 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 47 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 48 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 49 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 50 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 51 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 52 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 53 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 54 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 55 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 56 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 57 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 58 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 59 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 60 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 61 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 62 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 63 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 64 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 65 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 66 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 67 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 68 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 69 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 70 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 71 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 72 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 73 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 74 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 75 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 76 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 77 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 78 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 79 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 80 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 81 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 82 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 83 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 84 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 85 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 86 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 87 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 88 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 89 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 90 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 91 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 92 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 93 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 94 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 95 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 96 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 97 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 98 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 99 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 100 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 1 Loss : 0.374  Train Accuracy: 0.896 Validation Accuracy: 0.951 Test Accuracy: 0.948\n",
      "Epoch : 2 Loss : 0.150  Train Accuracy: 0.954 Validation Accuracy: 0.958 Test Accuracy: 0.958\n",
      "Epoch : 3 Loss : 0.096  Train Accuracy: 0.970 Validation Accuracy: 0.968 Test Accuracy: 0.965\n",
      "Epoch : 4 Loss : 0.067  Train Accuracy: 0.979 Validation Accuracy: 0.967 Test Accuracy: 0.971\n",
      "Epoch : 5 Loss : 0.048  Train Accuracy: 0.985 Validation Accuracy: 0.974 Test Accuracy: 0.973\n",
      "Epoch : 6 Loss : 0.036  Train Accuracy: 0.988 Validation Accuracy: 0.971 Test Accuracy: 0.972\n",
      "Epoch : 7 Loss : 0.026  Train Accuracy: 0.992 Validation Accuracy: 0.975 Test Accuracy: 0.976\n",
      "Epoch : 8 Loss : 0.017  Train Accuracy: 0.994 Validation Accuracy: 0.975 Test Accuracy: 0.975\n",
      "Epoch : 9 Loss : 0.012  Train Accuracy: 0.996 Validation Accuracy: 0.975 Test Accuracy: 0.976\n",
      "Epoch : 10 Loss : 0.008  Train Accuracy: 0.997 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 11 Loss : 0.006  Train Accuracy: 0.998 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 12 Loss : 0.004  Train Accuracy: 0.999 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 13 Loss : 0.003  Train Accuracy: 0.999 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 14 Loss : 0.002  Train Accuracy: 0.999 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 15 Loss : 0.002  Train Accuracy: 0.999 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 16 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 17 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 18 Loss : 0.002  Train Accuracy: 0.999 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 19 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 20 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 21 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 22 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 23 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 24 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 25 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 26 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 27 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 28 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 29 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 30 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 31 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 32 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 33 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 34 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 35 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 36 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 37 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 38 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 39 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 40 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 41 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 42 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 43 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 44 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 45 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 46 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 47 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 48 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 49 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 50 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.979\n",
      "Epoch : 51 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 52 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.979\n",
      "Epoch : 53 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 54 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 55 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 56 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 57 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 58 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 59 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 60 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 61 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 62 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 63 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 64 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 65 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 66 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 67 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 68 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 69 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 70 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 71 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 72 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 73 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 74 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 75 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 76 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 77 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 78 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 79 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 80 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 81 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 82 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.975 Test Accuracy: 0.977\n",
      "Epoch : 83 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.975 Test Accuracy: 0.977\n",
      "Epoch : 84 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.975 Test Accuracy: 0.978\n",
      "Epoch : 85 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.975 Test Accuracy: 0.978\n",
      "Epoch : 86 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.975 Test Accuracy: 0.978\n",
      "Epoch : 87 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 88 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 89 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 90 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 91 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 92 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 93 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 94 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 95 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.975 Test Accuracy: 0.977\n",
      "Epoch : 96 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.975 Test Accuracy: 0.977\n",
      "Epoch : 97 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.975 Test Accuracy: 0.977\n",
      "Epoch : 98 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.975 Test Accuracy: 0.977\n",
      "Epoch : 99 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.975 Test Accuracy: 0.977\n",
      "Epoch : 100 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.975 Test Accuracy: 0.977\n",
      "Epoch : 1 Loss : 0.387  Train Accuracy: 0.892 Validation Accuracy: 0.947 Test Accuracy: 0.941\n",
      "Epoch : 2 Loss : 0.149  Train Accuracy: 0.954 Validation Accuracy: 0.963 Test Accuracy: 0.963\n",
      "Epoch : 3 Loss : 0.100  Train Accuracy: 0.969 Validation Accuracy: 0.963 Test Accuracy: 0.961\n",
      "Epoch : 4 Loss : 0.073  Train Accuracy: 0.977 Validation Accuracy: 0.971 Test Accuracy: 0.971\n",
      "Epoch : 5 Loss : 0.053  Train Accuracy: 0.983 Validation Accuracy: 0.972 Test Accuracy: 0.972\n",
      "Epoch : 6 Loss : 0.039  Train Accuracy: 0.987 Validation Accuracy: 0.974 Test Accuracy: 0.975\n",
      "Epoch : 7 Loss : 0.029  Train Accuracy: 0.991 Validation Accuracy: 0.973 Test Accuracy: 0.977\n",
      "Epoch : 8 Loss : 0.024  Train Accuracy: 0.992 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 9 Loss : 0.018  Train Accuracy: 0.994 Validation Accuracy: 0.974 Test Accuracy: 0.974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 10 Loss : 0.014  Train Accuracy: 0.995 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 11 Loss : 0.011  Train Accuracy: 0.996 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 12 Loss : 0.007  Train Accuracy: 0.998 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 13 Loss : 0.006  Train Accuracy: 0.998 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 14 Loss : 0.004  Train Accuracy: 0.999 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 15 Loss : 0.003  Train Accuracy: 0.999 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 16 Loss : 0.002  Train Accuracy: 0.999 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 17 Loss : 0.002  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 18 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 19 Loss : 0.002  Train Accuracy: 0.999 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 20 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 21 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 22 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 23 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 24 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 25 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 26 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 27 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 28 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.980\n",
      "Epoch : 29 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.980\n",
      "Epoch : 30 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.980\n",
      "Epoch : 31 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.981\n",
      "Epoch : 32 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.981\n",
      "Epoch : 33 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.980\n",
      "Epoch : 34 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.980\n",
      "Epoch : 35 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.980\n",
      "Epoch : 36 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.980\n",
      "Epoch : 37 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.980\n",
      "Epoch : 38 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.980\n",
      "Epoch : 39 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.980\n",
      "Epoch : 40 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.980\n",
      "Epoch : 41 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 42 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 43 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 44 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 45 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 46 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 47 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 48 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 49 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 50 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 51 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 52 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 53 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 54 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 55 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 56 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 57 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 58 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 59 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 60 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 61 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 62 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 63 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 64 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 65 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 66 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 67 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 68 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 69 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 70 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 71 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 72 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 73 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 74 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 75 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 76 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 77 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 78 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 79 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 80 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 81 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 82 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 83 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 84 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 85 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 86 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 87 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 88 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 89 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 90 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 91 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 92 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 93 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 94 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 95 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 96 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 97 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 98 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 99 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 100 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 1 Loss : 0.385  Train Accuracy: 0.893 Validation Accuracy: 0.950 Test Accuracy: 0.943\n",
      "Epoch : 2 Loss : 0.160  Train Accuracy: 0.952 Validation Accuracy: 0.958 Test Accuracy: 0.955\n",
      "Epoch : 3 Loss : 0.104  Train Accuracy: 0.968 Validation Accuracy: 0.968 Test Accuracy: 0.967\n",
      "Epoch : 4 Loss : 0.075  Train Accuracy: 0.976 Validation Accuracy: 0.970 Test Accuracy: 0.969\n",
      "Epoch : 5 Loss : 0.055  Train Accuracy: 0.983 Validation Accuracy: 0.971 Test Accuracy: 0.972\n",
      "Epoch : 6 Loss : 0.039  Train Accuracy: 0.988 Validation Accuracy: 0.973 Test Accuracy: 0.975\n",
      "Epoch : 7 Loss : 0.027  Train Accuracy: 0.991 Validation Accuracy: 0.975 Test Accuracy: 0.975\n",
      "Epoch : 8 Loss : 0.020  Train Accuracy: 0.993 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 9 Loss : 0.014  Train Accuracy: 0.996 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 10 Loss : 0.010  Train Accuracy: 0.997 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 11 Loss : 0.007  Train Accuracy: 0.997 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 12 Loss : 0.005  Train Accuracy: 0.999 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 13 Loss : 0.004  Train Accuracy: 0.999 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 14 Loss : 0.003  Train Accuracy: 0.999 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 15 Loss : 0.002  Train Accuracy: 0.999 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 16 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 17 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.980\n",
      "Epoch : 18 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 19 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 20 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 21 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 22 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 23 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 24 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 25 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 26 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 27 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 28 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 29 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 30 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 31 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 32 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 33 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 34 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 35 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 36 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 37 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 38 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 39 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 40 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 41 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 42 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 43 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 44 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 45 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 46 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 47 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 48 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 49 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 50 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 51 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 52 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 53 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 54 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 55 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 56 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 57 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 58 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 59 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 60 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 61 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 62 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 63 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 64 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 65 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 66 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 67 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 68 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 69 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 70 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 71 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.979\n",
      "Epoch : 72 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 73 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 74 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 75 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 76 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 77 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 78 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 79 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 80 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 81 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 82 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 83 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 84 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 85 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 86 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 87 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 88 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 89 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 90 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 91 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 92 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 93 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 94 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 95 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 96 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 97 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 98 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 99 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 100 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 1 Loss : 0.374  Train Accuracy: 0.896 Validation Accuracy: 0.945 Test Accuracy: 0.942\n",
      "Epoch : 2 Loss : 0.146  Train Accuracy: 0.956 Validation Accuracy: 0.965 Test Accuracy: 0.961\n",
      "Epoch : 3 Loss : 0.094  Train Accuracy: 0.971 Validation Accuracy: 0.969 Test Accuracy: 0.966\n",
      "Epoch : 4 Loss : 0.067  Train Accuracy: 0.979 Validation Accuracy: 0.971 Test Accuracy: 0.970\n",
      "Epoch : 5 Loss : 0.047  Train Accuracy: 0.985 Validation Accuracy: 0.973 Test Accuracy: 0.972\n",
      "Epoch : 6 Loss : 0.034  Train Accuracy: 0.989 Validation Accuracy: 0.973 Test Accuracy: 0.973\n",
      "Epoch : 7 Loss : 0.027  Train Accuracy: 0.991 Validation Accuracy: 0.975 Test Accuracy: 0.973\n",
      "Epoch : 8 Loss : 0.019  Train Accuracy: 0.994 Validation Accuracy: 0.975 Test Accuracy: 0.977\n",
      "Epoch : 9 Loss : 0.015  Train Accuracy: 0.995 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 10 Loss : 0.011  Train Accuracy: 0.996 Validation Accuracy: 0.974 Test Accuracy: 0.974\n",
      "Epoch : 11 Loss : 0.009  Train Accuracy: 0.997 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 12 Loss : 0.005  Train Accuracy: 0.998 Validation Accuracy: 0.978 Test Accuracy: 0.975\n",
      "Epoch : 13 Loss : 0.005  Train Accuracy: 0.998 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 14 Loss : 0.003  Train Accuracy: 0.999 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 15 Loss : 0.004  Train Accuracy: 0.999 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 16 Loss : 0.003  Train Accuracy: 0.999 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 17 Loss : 0.002  Train Accuracy: 0.999 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 18 Loss : 0.002  Train Accuracy: 0.999 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 19 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 20 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 21 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 22 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 23 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 24 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 25 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 26 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 27 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 28 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 29 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 30 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 31 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 32 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 33 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 34 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 35 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 36 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 37 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 38 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 39 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 40 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 41 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 42 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 43 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 44 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 45 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 46 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 47 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 48 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 49 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 50 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 51 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 52 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 53 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 54 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 55 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 56 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 57 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 58 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 59 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 60 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 61 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 62 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 63 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 64 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 65 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 66 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 67 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 68 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 69 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 70 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 71 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 72 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 73 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 74 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 75 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 76 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 77 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 78 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 79 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 80 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 81 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 82 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 83 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 84 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 85 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 86 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 87 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 88 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 89 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 90 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 91 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 92 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 93 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 94 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 95 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 96 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 97 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 98 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 99 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 100 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 1 Loss : 0.384  Train Accuracy: 0.893 Validation Accuracy: 0.946 Test Accuracy: 0.942\n",
      "Epoch : 2 Loss : 0.153  Train Accuracy: 0.954 Validation Accuracy: 0.963 Test Accuracy: 0.958\n",
      "Epoch : 3 Loss : 0.103  Train Accuracy: 0.968 Validation Accuracy: 0.969 Test Accuracy: 0.967\n",
      "Epoch : 4 Loss : 0.074  Train Accuracy: 0.977 Validation Accuracy: 0.969 Test Accuracy: 0.968\n",
      "Epoch : 5 Loss : 0.054  Train Accuracy: 0.983 Validation Accuracy: 0.972 Test Accuracy: 0.973\n",
      "Epoch : 6 Loss : 0.038  Train Accuracy: 0.988 Validation Accuracy: 0.974 Test Accuracy: 0.973\n",
      "Epoch : 7 Loss : 0.028  Train Accuracy: 0.991 Validation Accuracy: 0.975 Test Accuracy: 0.975\n",
      "Epoch : 8 Loss : 0.020  Train Accuracy: 0.994 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 9 Loss : 0.014  Train Accuracy: 0.996 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 10 Loss : 0.010  Train Accuracy: 0.997 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 11 Loss : 0.006  Train Accuracy: 0.998 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 12 Loss : 0.005  Train Accuracy: 0.999 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 13 Loss : 0.003  Train Accuracy: 0.999 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 14 Loss : 0.002  Train Accuracy: 0.999 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 15 Loss : 0.002  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 16 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 17 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.980\n",
      "Epoch : 18 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 19 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 20 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 21 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 22 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 23 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 24 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 25 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 26 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 27 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 28 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 29 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 30 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 31 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 32 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 33 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 34 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 35 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 36 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 37 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 38 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 39 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 40 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 41 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 42 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 43 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 44 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 45 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 46 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 47 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 48 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 49 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 50 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 51 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 52 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 53 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 54 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 55 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 56 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 57 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 58 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 59 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 60 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 61 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 62 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 63 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 64 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 65 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 66 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 67 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 68 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 69 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 70 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 71 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 72 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 73 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 74 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 75 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 76 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 77 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 78 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 79 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 80 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 81 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 82 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 83 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 84 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 85 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 86 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 87 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 88 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 89 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 90 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 91 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 92 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 93 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.976\n",
      "Epoch : 94 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.976\n",
      "Epoch : 95 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.976\n",
      "Epoch : 96 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.976\n",
      "Epoch : 97 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.976\n",
      "Epoch : 98 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.976\n",
      "Epoch : 99 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.976\n",
      "Epoch : 100 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.976\n",
      "Epoch : 1 Loss : 0.391  Train Accuracy: 0.894 Validation Accuracy: 0.945 Test Accuracy: 0.941\n",
      "Epoch : 2 Loss : 0.156  Train Accuracy: 0.952 Validation Accuracy: 0.961 Test Accuracy: 0.958\n",
      "Epoch : 3 Loss : 0.101  Train Accuracy: 0.968 Validation Accuracy: 0.968 Test Accuracy: 0.969\n",
      "Epoch : 4 Loss : 0.073  Train Accuracy: 0.976 Validation Accuracy: 0.969 Test Accuracy: 0.970\n",
      "Epoch : 5 Loss : 0.052  Train Accuracy: 0.983 Validation Accuracy: 0.973 Test Accuracy: 0.972\n",
      "Epoch : 6 Loss : 0.040  Train Accuracy: 0.987 Validation Accuracy: 0.975 Test Accuracy: 0.974\n",
      "Epoch : 7 Loss : 0.027  Train Accuracy: 0.992 Validation Accuracy: 0.975 Test Accuracy: 0.975\n",
      "Epoch : 8 Loss : 0.020  Train Accuracy: 0.994 Validation Accuracy: 0.975 Test Accuracy: 0.976\n",
      "Epoch : 9 Loss : 0.015  Train Accuracy: 0.995 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 10 Loss : 0.011  Train Accuracy: 0.997 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 11 Loss : 0.008  Train Accuracy: 0.998 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 12 Loss : 0.006  Train Accuracy: 0.998 Validation Accuracy: 0.975 Test Accuracy: 0.975\n",
      "Epoch : 13 Loss : 0.005  Train Accuracy: 0.998 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 14 Loss : 0.003  Train Accuracy: 0.999 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 15 Loss : 0.003  Train Accuracy: 0.999 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 16 Loss : 0.002  Train Accuracy: 0.999 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 17 Loss : 0.002  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.976\n",
      "Epoch : 18 Loss : 0.002  Train Accuracy: 0.999 Validation Accuracy: 0.979 Test Accuracy: 0.976\n",
      "Epoch : 19 Loss : 0.002  Train Accuracy: 0.999 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 20 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.977\n",
      "Epoch : 21 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 22 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.976\n",
      "Epoch : 23 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 24 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 25 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.978\n",
      "Epoch : 26 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 27 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 28 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.977\n",
      "Epoch : 29 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.978\n",
      "Epoch : 30 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.978\n",
      "Epoch : 31 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.977\n",
      "Epoch : 32 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.977\n",
      "Epoch : 33 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.977\n",
      "Epoch : 34 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 35 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 36 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.978\n",
      "Epoch : 37 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.978\n",
      "Epoch : 38 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.978\n",
      "Epoch : 39 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.978\n",
      "Epoch : 40 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.978\n",
      "Epoch : 41 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.978\n",
      "Epoch : 42 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.978\n",
      "Epoch : 43 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.977\n",
      "Epoch : 44 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 45 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.977\n",
      "Epoch : 46 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.977\n",
      "Epoch : 47 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 48 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 49 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 50 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 51 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 52 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 53 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 54 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 55 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 56 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 57 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 58 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 59 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 60 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 61 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 62 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 63 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 64 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 65 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 66 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 67 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 68 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 69 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 70 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 71 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 72 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 73 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 74 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 75 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 76 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 77 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 78 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 79 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 80 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 81 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 82 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 83 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 84 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 85 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 86 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 87 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 88 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 89 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 90 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 91 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 92 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 93 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 94 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 95 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 96 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 97 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 98 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 99 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 100 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 1 Loss : 0.374  Train Accuracy: 0.895 Validation Accuracy: 0.948 Test Accuracy: 0.940\n",
      "Epoch : 2 Loss : 0.150  Train Accuracy: 0.954 Validation Accuracy: 0.962 Test Accuracy: 0.961\n",
      "Epoch : 3 Loss : 0.103  Train Accuracy: 0.968 Validation Accuracy: 0.967 Test Accuracy: 0.963\n",
      "Epoch : 4 Loss : 0.075  Train Accuracy: 0.976 Validation Accuracy: 0.973 Test Accuracy: 0.972\n",
      "Epoch : 5 Loss : 0.052  Train Accuracy: 0.984 Validation Accuracy: 0.975 Test Accuracy: 0.974\n",
      "Epoch : 6 Loss : 0.037  Train Accuracy: 0.988 Validation Accuracy: 0.975 Test Accuracy: 0.976\n",
      "Epoch : 7 Loss : 0.029  Train Accuracy: 0.991 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 8 Loss : 0.022  Train Accuracy: 0.993 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 9 Loss : 0.015  Train Accuracy: 0.995 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 10 Loss : 0.011  Train Accuracy: 0.997 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 11 Loss : 0.008  Train Accuracy: 0.998 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 12 Loss : 0.005  Train Accuracy: 0.999 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 13 Loss : 0.004  Train Accuracy: 0.999 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 14 Loss : 0.003  Train Accuracy: 0.999 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 15 Loss : 0.002  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 16 Loss : 0.002  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 17 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 18 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 19 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 20 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 21 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 22 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 23 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 24 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 25 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 26 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 27 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 28 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 29 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 30 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 31 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 32 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 33 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 34 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 35 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 36 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 37 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 38 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 39 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 40 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 41 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 42 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 43 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 44 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 45 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 46 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 47 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 48 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 49 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 50 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 51 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 52 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 53 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 54 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 55 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 56 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 57 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 58 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 59 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 60 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 61 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 62 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 63 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 64 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 65 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 66 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 67 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 68 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 69 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 70 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 71 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 72 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 73 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 74 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 75 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 76 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 77 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 78 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.976\n",
      "Epoch : 79 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 80 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 81 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 82 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 83 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 84 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 85 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 86 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 87 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 88 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 89 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 90 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 91 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 92 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 93 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 94 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 95 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 96 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 97 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 98 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 99 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 100 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 1 Loss : 0.385  Train Accuracy: 0.893 Validation Accuracy: 0.947 Test Accuracy: 0.942\n",
      "Epoch : 2 Loss : 0.150  Train Accuracy: 0.954 Validation Accuracy: 0.963 Test Accuracy: 0.959\n",
      "Epoch : 3 Loss : 0.102  Train Accuracy: 0.968 Validation Accuracy: 0.969 Test Accuracy: 0.966\n",
      "Epoch : 4 Loss : 0.073  Train Accuracy: 0.977 Validation Accuracy: 0.971 Test Accuracy: 0.970\n",
      "Epoch : 5 Loss : 0.054  Train Accuracy: 0.983 Validation Accuracy: 0.973 Test Accuracy: 0.972\n",
      "Epoch : 6 Loss : 0.038  Train Accuracy: 0.988 Validation Accuracy: 0.974 Test Accuracy: 0.973\n",
      "Epoch : 7 Loss : 0.027  Train Accuracy: 0.991 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 8 Loss : 0.021  Train Accuracy: 0.994 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 9 Loss : 0.016  Train Accuracy: 0.995 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 10 Loss : 0.012  Train Accuracy: 0.996 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 11 Loss : 0.009  Train Accuracy: 0.997 Validation Accuracy: 0.977 Test Accuracy: 0.975\n",
      "Epoch : 12 Loss : 0.006  Train Accuracy: 0.998 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 13 Loss : 0.005  Train Accuracy: 0.998 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 14 Loss : 0.004  Train Accuracy: 0.999 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 15 Loss : 0.002  Train Accuracy: 0.999 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 16 Loss : 0.002  Train Accuracy: 0.999 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 17 Loss : 0.002  Train Accuracy: 0.999 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 18 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 19 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 20 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 21 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 22 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 23 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 24 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 25 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 26 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 27 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 28 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 29 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 30 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 31 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 32 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 33 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 34 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 35 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 36 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 37 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 38 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 39 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 40 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 41 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 42 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 43 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 44 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 45 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 46 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 47 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 48 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 49 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 50 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 51 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 52 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 53 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 54 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 55 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 56 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 57 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 58 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 59 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 60 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 61 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 62 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 63 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 64 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 65 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 66 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.977\n",
      "Epoch : 67 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.977\n",
      "Epoch : 68 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.977\n",
      "Epoch : 69 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.977\n",
      "Epoch : 70 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.977\n",
      "Epoch : 71 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.977\n",
      "Epoch : 72 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.977\n",
      "Epoch : 73 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.977\n",
      "Epoch : 74 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.977\n",
      "Epoch : 75 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.977\n",
      "Epoch : 76 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.977\n",
      "Epoch : 77 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.977\n",
      "Epoch : 78 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.977\n",
      "Epoch : 79 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.977\n",
      "Epoch : 80 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 81 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.977\n",
      "Epoch : 82 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 83 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 84 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 85 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.977\n",
      "Epoch : 86 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.977\n",
      "Epoch : 87 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.977\n",
      "Epoch : 88 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.977\n",
      "Epoch : 89 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 90 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 91 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 92 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 93 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 94 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 95 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 96 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 97 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 98 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 99 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 100 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 1 Loss : 0.381  Train Accuracy: 0.895 Validation Accuracy: 0.949 Test Accuracy: 0.944\n",
      "Epoch : 2 Loss : 0.148  Train Accuracy: 0.955 Validation Accuracy: 0.961 Test Accuracy: 0.958\n",
      "Epoch : 3 Loss : 0.097  Train Accuracy: 0.969 Validation Accuracy: 0.970 Test Accuracy: 0.967\n",
      "Epoch : 4 Loss : 0.070  Train Accuracy: 0.978 Validation Accuracy: 0.971 Test Accuracy: 0.968\n",
      "Epoch : 5 Loss : 0.048  Train Accuracy: 0.985 Validation Accuracy: 0.974 Test Accuracy: 0.974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 6 Loss : 0.033  Train Accuracy: 0.989 Validation Accuracy: 0.975 Test Accuracy: 0.974\n",
      "Epoch : 7 Loss : 0.024  Train Accuracy: 0.992 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 8 Loss : 0.018  Train Accuracy: 0.994 Validation Accuracy: 0.975 Test Accuracy: 0.976\n",
      "Epoch : 9 Loss : 0.013  Train Accuracy: 0.996 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 10 Loss : 0.010  Train Accuracy: 0.997 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 11 Loss : 0.008  Train Accuracy: 0.997 Validation Accuracy: 0.976 Test Accuracy: 0.974\n",
      "Epoch : 12 Loss : 0.005  Train Accuracy: 0.998 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 13 Loss : 0.005  Train Accuracy: 0.998 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 14 Loss : 0.003  Train Accuracy: 0.999 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 15 Loss : 0.003  Train Accuracy: 0.999 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 16 Loss : 0.002  Train Accuracy: 0.999 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 17 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 18 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 19 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 20 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 21 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 22 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 23 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 24 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 25 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 26 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 27 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 28 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 29 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 30 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 31 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 32 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 33 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 34 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 35 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 36 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 37 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 38 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 39 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 40 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 41 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 42 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 43 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 44 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 45 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 46 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 47 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 48 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 49 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 50 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 51 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 52 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 53 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 54 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 55 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 56 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 57 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 58 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 59 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 60 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 61 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 62 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 63 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 64 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 65 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 66 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 67 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 68 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 69 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 70 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 71 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 72 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 73 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 74 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 75 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 76 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 77 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 78 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 79 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 80 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 81 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 82 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 83 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 84 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 85 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 86 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 87 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 88 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 89 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 90 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 91 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 92 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 93 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 94 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 95 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 96 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 97 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 98 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 99 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 100 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 1 Loss : 0.374  Train Accuracy: 0.895 Validation Accuracy: 0.944 Test Accuracy: 0.943\n",
      "Epoch : 2 Loss : 0.154  Train Accuracy: 0.953 Validation Accuracy: 0.965 Test Accuracy: 0.960\n",
      "Epoch : 3 Loss : 0.096  Train Accuracy: 0.970 Validation Accuracy: 0.969 Test Accuracy: 0.968\n",
      "Epoch : 4 Loss : 0.067  Train Accuracy: 0.979 Validation Accuracy: 0.971 Test Accuracy: 0.973\n",
      "Epoch : 5 Loss : 0.049  Train Accuracy: 0.984 Validation Accuracy: 0.973 Test Accuracy: 0.973\n",
      "Epoch : 6 Loss : 0.035  Train Accuracy: 0.989 Validation Accuracy: 0.976 Test Accuracy: 0.974\n",
      "Epoch : 7 Loss : 0.026  Train Accuracy: 0.992 Validation Accuracy: 0.975 Test Accuracy: 0.975\n",
      "Epoch : 8 Loss : 0.019  Train Accuracy: 0.994 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 9 Loss : 0.016  Train Accuracy: 0.995 Validation Accuracy: 0.977 Test Accuracy: 0.975\n",
      "Epoch : 10 Loss : 0.012  Train Accuracy: 0.996 Validation Accuracy: 0.975 Test Accuracy: 0.975\n",
      "Epoch : 11 Loss : 0.010  Train Accuracy: 0.997 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 12 Loss : 0.006  Train Accuracy: 0.998 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 13 Loss : 0.005  Train Accuracy: 0.998 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 14 Loss : 0.004  Train Accuracy: 0.999 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 15 Loss : 0.003  Train Accuracy: 0.999 Validation Accuracy: 0.978 Test Accuracy: 0.980\n",
      "Epoch : 16 Loss : 0.002  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 17 Loss : 0.002  Train Accuracy: 0.999 Validation Accuracy: 0.978 Test Accuracy: 0.981\n",
      "Epoch : 18 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.981\n",
      "Epoch : 19 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.981\n",
      "Epoch : 20 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.980\n",
      "Epoch : 21 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 22 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 23 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 24 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.980\n",
      "Epoch : 25 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.980\n",
      "Epoch : 26 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.980\n",
      "Epoch : 27 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.981\n",
      "Epoch : 28 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.981\n",
      "Epoch : 29 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.981\n",
      "Epoch : 30 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 31 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.980\n",
      "Epoch : 32 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.980\n",
      "Epoch : 33 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.980\n",
      "Epoch : 34 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 35 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 36 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 37 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.980\n",
      "Epoch : 38 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.980\n",
      "Epoch : 39 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 40 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 41 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 42 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 43 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 44 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 45 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 46 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 47 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 48 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 49 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 50 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 51 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 52 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 53 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 54 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 55 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 56 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 57 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 58 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.980\n",
      "Epoch : 59 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 60 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 61 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 62 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 63 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.980\n",
      "Epoch : 64 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 65 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 66 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 67 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 68 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.979\n",
      "Epoch : 69 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.979\n",
      "Epoch : 70 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.979\n",
      "Epoch : 71 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.979\n",
      "Epoch : 72 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.979\n",
      "Epoch : 73 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.979\n",
      "Epoch : 74 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.979\n",
      "Epoch : 75 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.979\n",
      "Epoch : 76 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.979\n",
      "Epoch : 77 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.979\n",
      "Epoch : 78 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 79 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 80 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 81 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.979\n",
      "Epoch : 82 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.979\n",
      "Epoch : 83 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.979\n",
      "Epoch : 84 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.979\n",
      "Epoch : 85 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.979\n",
      "Epoch : 86 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.979\n",
      "Epoch : 87 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 88 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 89 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 90 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 91 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 92 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 93 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 94 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 95 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 96 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 97 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 98 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 99 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 100 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 1 Loss : 0.375  Train Accuracy: 0.897 Validation Accuracy: 0.947 Test Accuracy: 0.944\n",
      "Epoch : 2 Loss : 0.145  Train Accuracy: 0.956 Validation Accuracy: 0.964 Test Accuracy: 0.962\n",
      "Epoch : 3 Loss : 0.092  Train Accuracy: 0.972 Validation Accuracy: 0.968 Test Accuracy: 0.968\n",
      "Epoch : 4 Loss : 0.068  Train Accuracy: 0.979 Validation Accuracy: 0.972 Test Accuracy: 0.968\n",
      "Epoch : 5 Loss : 0.050  Train Accuracy: 0.984 Validation Accuracy: 0.976 Test Accuracy: 0.974\n",
      "Epoch : 6 Loss : 0.036  Train Accuracy: 0.988 Validation Accuracy: 0.976 Test Accuracy: 0.974\n",
      "Epoch : 7 Loss : 0.027  Train Accuracy: 0.991 Validation Accuracy: 0.977 Test Accuracy: 0.974\n",
      "Epoch : 8 Loss : 0.021  Train Accuracy: 0.994 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 9 Loss : 0.014  Train Accuracy: 0.996 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 10 Loss : 0.011  Train Accuracy: 0.996 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 11 Loss : 0.008  Train Accuracy: 0.997 Validation Accuracy: 0.977 Test Accuracy: 0.975\n",
      "Epoch : 12 Loss : 0.006  Train Accuracy: 0.998 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 13 Loss : 0.004  Train Accuracy: 0.999 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 14 Loss : 0.003  Train Accuracy: 0.999 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 15 Loss : 0.003  Train Accuracy: 0.999 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 16 Loss : 0.002  Train Accuracy: 0.999 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 17 Loss : 0.002  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.980\n",
      "Epoch : 18 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 19 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.980\n",
      "Epoch : 20 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.980\n",
      "Epoch : 21 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.980\n",
      "Epoch : 22 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.978\n",
      "Epoch : 23 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.979\n",
      "Epoch : 24 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.979\n",
      "Epoch : 25 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.981 Test Accuracy: 0.980\n",
      "Epoch : 26 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.981 Test Accuracy: 0.980\n",
      "Epoch : 27 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.981 Test Accuracy: 0.980\n",
      "Epoch : 28 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.981 Test Accuracy: 0.981\n",
      "Epoch : 29 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.980\n",
      "Epoch : 30 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.980\n",
      "Epoch : 31 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.980\n",
      "Epoch : 32 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.979\n",
      "Epoch : 33 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.980\n",
      "Epoch : 34 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.980\n",
      "Epoch : 35 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.980\n",
      "Epoch : 36 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.980\n",
      "Epoch : 37 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.980\n",
      "Epoch : 38 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.979\n",
      "Epoch : 39 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.979\n",
      "Epoch : 40 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.979\n",
      "Epoch : 41 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.979\n",
      "Epoch : 42 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.979\n",
      "Epoch : 43 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.979\n",
      "Epoch : 44 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.979\n",
      "Epoch : 45 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.979\n",
      "Epoch : 46 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.980\n",
      "Epoch : 47 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.979\n",
      "Epoch : 48 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.979\n",
      "Epoch : 49 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.979\n",
      "Epoch : 50 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.979\n",
      "Epoch : 51 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.979\n",
      "Epoch : 52 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.979\n",
      "Epoch : 53 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.979\n",
      "Epoch : 54 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.979\n",
      "Epoch : 55 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.979\n",
      "Epoch : 56 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.979\n",
      "Epoch : 57 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.979\n",
      "Epoch : 58 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.979\n",
      "Epoch : 59 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.980\n",
      "Epoch : 60 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.979\n",
      "Epoch : 61 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.979\n",
      "Epoch : 62 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 63 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 64 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 65 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 66 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 67 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 68 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.979\n",
      "Epoch : 69 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.978\n",
      "Epoch : 70 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 71 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 72 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 73 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 74 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 75 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 76 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 77 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 78 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 79 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 80 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 81 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 82 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 83 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 84 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 85 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 86 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 87 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 88 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 89 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 90 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 91 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 92 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 93 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 94 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 95 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 96 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 97 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 98 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 99 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 100 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 1 Loss : 0.371  Train Accuracy: 0.896 Validation Accuracy: 0.946 Test Accuracy: 0.941\n",
      "Epoch : 2 Loss : 0.142  Train Accuracy: 0.957 Validation Accuracy: 0.961 Test Accuracy: 0.959\n",
      "Epoch : 3 Loss : 0.089  Train Accuracy: 0.972 Validation Accuracy: 0.971 Test Accuracy: 0.969\n",
      "Epoch : 4 Loss : 0.061  Train Accuracy: 0.981 Validation Accuracy: 0.973 Test Accuracy: 0.971\n",
      "Epoch : 5 Loss : 0.043  Train Accuracy: 0.987 Validation Accuracy: 0.974 Test Accuracy: 0.975\n",
      "Epoch : 6 Loss : 0.033  Train Accuracy: 0.990 Validation Accuracy: 0.972 Test Accuracy: 0.974\n",
      "Epoch : 7 Loss : 0.023  Train Accuracy: 0.993 Validation Accuracy: 0.975 Test Accuracy: 0.977\n",
      "Epoch : 8 Loss : 0.019  Train Accuracy: 0.994 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 9 Loss : 0.014  Train Accuracy: 0.996 Validation Accuracy: 0.974 Test Accuracy: 0.976\n",
      "Epoch : 10 Loss : 0.010  Train Accuracy: 0.996 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 11 Loss : 0.009  Train Accuracy: 0.997 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 12 Loss : 0.007  Train Accuracy: 0.998 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 13 Loss : 0.005  Train Accuracy: 0.998 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 14 Loss : 0.003  Train Accuracy: 0.999 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 15 Loss : 0.002  Train Accuracy: 0.999 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 16 Loss : 0.003  Train Accuracy: 0.999 Validation Accuracy: 0.977 Test Accuracy: 0.979\n",
      "Epoch : 17 Loss : 0.003  Train Accuracy: 0.999 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 18 Loss : 0.003  Train Accuracy: 0.999 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 19 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 20 Loss : 0.001  Train Accuracy: 0.999 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 21 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 22 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.978\n",
      "Epoch : 23 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 24 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 25 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 26 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 27 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 28 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 29 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.980\n",
      "Epoch : 30 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 31 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.979\n",
      "Epoch : 32 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 33 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 34 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 35 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 36 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 37 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 38 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 39 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 40 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 41 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 42 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 43 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 44 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 45 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 46 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 47 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 48 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 49 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 50 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 51 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 52 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 53 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 54 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 55 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 56 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 57 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 58 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 59 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 60 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 61 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 62 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 63 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 64 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 65 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 66 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 67 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 68 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 69 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 70 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 71 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 72 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 73 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 74 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 75 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 76 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 77 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 78 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 79 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 80 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 81 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 82 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 83 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 84 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 85 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 86 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 87 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 88 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 89 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 90 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 91 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 92 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 93 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 94 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 95 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 96 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 97 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 98 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 99 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 100 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 1 Loss : 0.391  Train Accuracy: 0.889 Validation Accuracy: 0.943 Test Accuracy: 0.937\n",
      "Epoch : 2 Loss : 0.158  Train Accuracy: 0.952 Validation Accuracy: 0.959 Test Accuracy: 0.957\n",
      "Epoch : 3 Loss : 0.100  Train Accuracy: 0.969 Validation Accuracy: 0.968 Test Accuracy: 0.965\n",
      "Epoch : 4 Loss : 0.072  Train Accuracy: 0.978 Validation Accuracy: 0.971 Test Accuracy: 0.969\n",
      "Epoch : 5 Loss : 0.052  Train Accuracy: 0.984 Validation Accuracy: 0.973 Test Accuracy: 0.971\n",
      "Epoch : 6 Loss : 0.039  Train Accuracy: 0.988 Validation Accuracy: 0.975 Test Accuracy: 0.975\n",
      "Epoch : 7 Loss : 0.029  Train Accuracy: 0.991 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 8 Loss : 0.021  Train Accuracy: 0.993 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 9 Loss : 0.015  Train Accuracy: 0.995 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 10 Loss : 0.012  Train Accuracy: 0.996 Validation Accuracy: 0.976 Test Accuracy: 0.975\n",
      "Epoch : 11 Loss : 0.010  Train Accuracy: 0.997 Validation Accuracy: 0.976 Test Accuracy: 0.978\n",
      "Epoch : 12 Loss : 0.008  Train Accuracy: 0.998 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 13 Loss : 0.007  Train Accuracy: 0.998 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 14 Loss : 0.005  Train Accuracy: 0.998 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 15 Loss : 0.003  Train Accuracy: 0.999 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 16 Loss : 0.003  Train Accuracy: 0.999 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 17 Loss : 0.002  Train Accuracy: 0.999 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 18 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 19 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 20 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 21 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 22 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 23 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 24 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 25 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.979\n",
      "Epoch : 26 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.980 Test Accuracy: 0.978\n",
      "Epoch : 27 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 28 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 29 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 30 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 31 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 32 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 33 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 34 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 35 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 36 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 37 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 38 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 39 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 40 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 41 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 42 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 43 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 44 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 45 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 46 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 47 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 48 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 49 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 50 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 51 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 52 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 53 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 54 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 55 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 56 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 57 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 58 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 59 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 60 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 61 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 62 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 63 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 64 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 65 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 66 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 67 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 68 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 69 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 70 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 71 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 72 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 73 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 74 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 75 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 76 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 77 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 78 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 79 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 80 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 81 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 82 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 83 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 84 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 85 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 86 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 87 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 88 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 89 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 90 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 91 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 92 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 93 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 94 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 95 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 96 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 97 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 98 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 99 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 100 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 1 Loss : 0.382  Train Accuracy: 0.895 Validation Accuracy: 0.950 Test Accuracy: 0.945\n",
      "Epoch : 2 Loss : 0.150  Train Accuracy: 0.954 Validation Accuracy: 0.962 Test Accuracy: 0.958\n",
      "Epoch : 3 Loss : 0.099  Train Accuracy: 0.969 Validation Accuracy: 0.970 Test Accuracy: 0.968\n",
      "Epoch : 4 Loss : 0.069  Train Accuracy: 0.978 Validation Accuracy: 0.973 Test Accuracy: 0.973\n",
      "Epoch : 5 Loss : 0.048  Train Accuracy: 0.985 Validation Accuracy: 0.976 Test Accuracy: 0.973\n",
      "Epoch : 6 Loss : 0.033  Train Accuracy: 0.990 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 7 Loss : 0.024  Train Accuracy: 0.993 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 8 Loss : 0.016  Train Accuracy: 0.995 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 9 Loss : 0.011  Train Accuracy: 0.997 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 10 Loss : 0.008  Train Accuracy: 0.998 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 11 Loss : 0.006  Train Accuracy: 0.998 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 12 Loss : 0.004  Train Accuracy: 0.999 Validation Accuracy: 0.977 Test Accuracy: 0.978\n",
      "Epoch : 13 Loss : 0.003  Train Accuracy: 0.999 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 14 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 15 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 16 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 17 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 18 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 19 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 20 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.979\n",
      "Epoch : 21 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 22 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 23 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 24 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 25 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 26 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 27 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 28 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 29 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 30 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 31 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 32 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 33 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 34 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 35 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 36 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 37 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 38 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 39 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 40 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 41 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 42 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 43 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 44 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 45 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 46 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 47 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 48 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 49 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 50 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 51 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 52 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 53 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 54 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 55 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 56 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 57 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 58 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 59 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 60 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 61 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 62 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 63 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 64 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 65 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 66 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 67 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 68 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 69 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 70 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 71 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 72 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 73 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 74 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 75 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 76 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 77 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 78 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 79 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 80 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 81 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 82 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 83 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 84 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 85 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 86 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 87 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 88 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 89 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 90 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 91 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 92 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 93 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 94 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.976\n",
      "Epoch : 95 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 96 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 97 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 98 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 99 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 100 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 1 Loss : 0.381  Train Accuracy: 0.892 Validation Accuracy: 0.948 Test Accuracy: 0.942\n",
      "Epoch : 2 Loss : 0.154  Train Accuracy: 0.953 Validation Accuracy: 0.959 Test Accuracy: 0.957\n",
      "Epoch : 3 Loss : 0.103  Train Accuracy: 0.968 Validation Accuracy: 0.967 Test Accuracy: 0.963\n",
      "Epoch : 4 Loss : 0.075  Train Accuracy: 0.976 Validation Accuracy: 0.968 Test Accuracy: 0.969\n",
      "Epoch : 5 Loss : 0.053  Train Accuracy: 0.983 Validation Accuracy: 0.973 Test Accuracy: 0.974\n",
      "Epoch : 6 Loss : 0.040  Train Accuracy: 0.988 Validation Accuracy: 0.973 Test Accuracy: 0.974\n",
      "Epoch : 7 Loss : 0.029  Train Accuracy: 0.990 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 8 Loss : 0.022  Train Accuracy: 0.993 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 9 Loss : 0.015  Train Accuracy: 0.995 Validation Accuracy: 0.976 Test Accuracy: 0.977\n",
      "Epoch : 10 Loss : 0.010  Train Accuracy: 0.997 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 11 Loss : 0.007  Train Accuracy: 0.998 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 12 Loss : 0.005  Train Accuracy: 0.999 Validation Accuracy: 0.976 Test Accuracy: 0.976\n",
      "Epoch : 13 Loss : 0.004  Train Accuracy: 0.999 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 14 Loss : 0.002  Train Accuracy: 0.999 Validation Accuracy: 0.978 Test Accuracy: 0.976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 15 Loss : 0.002  Train Accuracy: 0.999 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 16 Loss : 0.002  Train Accuracy: 0.999 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 17 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.976\n",
      "Epoch : 18 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.976\n",
      "Epoch : 19 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 20 Loss : 0.001  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 21 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 22 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.976\n",
      "Epoch : 23 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 24 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 25 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.978\n",
      "Epoch : 26 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.978\n",
      "Epoch : 27 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 28 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 29 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 30 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 31 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 32 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 33 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 34 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 35 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 36 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 37 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 38 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 39 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 40 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 41 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.979 Test Accuracy: 0.977\n",
      "Epoch : 42 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 43 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 44 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 45 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 46 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 47 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 48 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 49 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 50 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 51 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 52 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 53 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 54 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 55 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 56 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 57 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 58 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 59 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 60 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 61 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 62 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 63 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 64 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 65 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 66 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 67 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 68 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 69 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 70 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 71 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 72 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.978 Test Accuracy: 0.977\n",
      "Epoch : 73 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 74 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 75 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 76 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 77 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 78 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 79 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 80 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 81 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 82 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 83 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 84 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 85 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 86 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 87 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 88 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 89 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 90 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 91 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 92 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 93 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 94 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 95 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 96 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 97 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 98 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 99 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n",
      "Epoch : 100 Loss : 0.000  Train Accuracy: 1.000 Validation Accuracy: 0.977 Test Accuracy: 0.977\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.01,0.02,0.05,0.1,1.0]\n",
    "TRAINSIZE = mnist_train_data.shape[0]\n",
    "indices = list(range(TRAINSIZE))\n",
    "\n",
    "#Hyperparameters for model\n",
    "layerDims = [784,500,500]\n",
    "lr=0.001\n",
    "lr_decay = .0001\n",
    "batch_size=128\n",
    "repeat_test_results = [{}]*5\n",
    "for i in range(1,5):\n",
    "    for alpha in alphas:\n",
    "        subindices = np.random.choice(indices,size=[int(alpha*TRAINSIZE)])\n",
    "        train_data_subset = mnist_train_data[indices,:]\n",
    "        train_labels_subset = mnist_train_labels[indices]\n",
    "        train_data_tensor = torch.utils.data.TensorDataset(torch.Tensor(train_data_subset),torch.IntTensor(train_labels_subset))\n",
    "\n",
    "        subtrainloader = torch.utils.data.DataLoader(train_data_tensor, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "        model = MLP_MNIST(layerDims,10,dropout)\n",
    "        optimizer = torch.optim.Adagrad(model.parameters(),lr=lr, lr_decay=lr_decay)\n",
    "        repeat_test_results[i][alpha] = train(model,100,subtrainloader,optimizer,mnist_val,mnist_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.01\n",
      "0.02\n",
      "0.02\n",
      "0.02\n",
      "0.02\n",
      "0.02\n",
      "Alpha: 0.02\n",
      "0.02226\n",
      "0.02226\n",
      "0.02226\n",
      "0.02226\n",
      "0.02226\n",
      "Alpha: 0.05\n",
      "0.02166\n",
      "0.02166\n",
      "0.02166\n",
      "0.02166\n",
      "0.02166\n",
      "Alpha: 0.1\n",
      "0.0228\n",
      "0.0228\n",
      "0.0228\n",
      "0.0228\n",
      "0.0228\n",
      "Alpha: 1.0\n",
      "0.0228\n",
      "0.0228\n",
      "0.0228\n",
      "0.0228\n",
      "0.0228\n"
     ]
    }
   ],
   "source": [
    "#print repeat_test_results[0][0.05][5]\n",
    "\n",
    "def GeneralizationGap(results):\n",
    "    gen_gap_error = np.zeros(shape=(5,5))\n",
    "    for i in range(0,5):\n",
    "        for j,alpha in enumerate([0.01,0.02,0.05,0.1,1.0]):\n",
    "            best_epoch = np.argmax(results[i][alpha][3])\n",
    "            gen_gap_error[i,j] = results[i][alpha][2][int(best_epoch)] - results[i][alpha][4][int(best_epoch)]\n",
    "            \n",
    "    return gen_gap_error\n",
    "\n",
    "\n",
    "alpha_results = GeneralizationGap(repeat_test_results)\n",
    "for i in range(0,5):\n",
    "    print \"Alpha: \" + repr(alphas[i])\n",
    "    for j in range(0,5):\n",
    "        print alpha_results[j,i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Alpha  | Result 1 | Result 2 |  Result 3 |  Result 4 |  Result 5 |\n",
    "|--------|----------|----------|-----------|-----------|-----------|\n",
    "|  0.01  |   0.02   |   0.02   |    0.02   |    0.02   |   0.02    |\n",
    "|  0.02  |   0.02226|   0.02226|   0.02226 |   0.02226 |   0.02226 |\n",
    "|  0.05  | 0.021666 | 0.021666 |  0.021666 |  0.021666 |  0.021666 |\n",
    "|  0.1   | 0.0228   | 0.0228   |  0.0228   |  0.0228   |  0.0228   |\n",
    "|  1.0   | 0.0228   | 0.0228   | 0.0228    |  0.0228   |  0.0228   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "data = np.loadtxt('./data/20news-bydate/matlab/train.data')\n",
    "labels = np.loadtxt('./data/20news-bydate/matlab/train.label')\n",
    "labels -= 1\n",
    "\n",
    "test_data = np.loadtxt('./data/20news-bydate/matlab/test.data')\n",
    "test_labels = np.loadtxt('./data/20news-bydate/matlab/test.label')\n",
    "test_labels -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_20(nn.Module):\n",
    "    \n",
    "    def __init__(self,insize,outsize):\n",
    "        super(MLP_20,self).__init__()\n",
    "        self.fc1 = nn.Linear(insize,100)\n",
    "        self.fc2 = nn.Linear(100,outsize)\n",
    "        self.insize = insize\n",
    "        \n",
    "    def forward(self,x):\n",
    "        a1 = self.fc1(x)\n",
    "        h1 = nn.functional.relu(a1)\n",
    "        \n",
    "        logits = self.fc2(h1)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def prediction(self,logits):\n",
    "        \n",
    "        values, indices = torch.max(logits.data,1)\n",
    "        \n",
    "        return values, indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restructure_data(data):\n",
    "    samples, vocab_size, max_word_count = np.max(data,axis=0)\n",
    "    restructured_data = np.zeros((int(samples),int(vocab_size)))\n",
    "    for i in range(len(data)):\n",
    "        if (data[i,1]-1)<=vocab_size:\n",
    "            restructured_data[int(data[i,0]-1),int(data[i,1]-1)]=data[i,2]\n",
    "    return restructured_data\n",
    "\n",
    "def tfidf(data):\n",
    "    idf =  np.log(np.divide((data.shape[0]*np.ones_like(data)),1+np.sum(np.equal(data>0,1),axis=0)[None,:].astype(np.float)))\n",
    "    print idf[0:10,:]\n",
    "    #tf = 0.5+0.5*np.divide(data,np.max(data,axis=1)[:,None].astype(np.float))\n",
    "    return np.multiply(data,idf)\n",
    "\n",
    "def make_loaders(data,labels,val_split,test_data=None,test_labels=None,batch_size=128):\n",
    "    val_loader = 0\n",
    "    indices = np.arange(data.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(val_split*indices.shape[0])\n",
    "    train_data, val_data = data[indices[:split],:], data[indices[split:],:]\n",
    "    train_labels, val_labels = labels[indices[:split]], labels[indices[split:]]\n",
    "    del indices\n",
    "    print train_data.shape, train_labels.shape\n",
    "    print val_data.shape, val_labels.shape\n",
    "    \n",
    "    train_tensor = torch.utils.data.TensorDataset(torch.Tensor(train_data),torch.IntTensor(train_labels))\n",
    "    del train_data\n",
    "    del train_labels\n",
    "    train_loader = torch.utils.data.DataLoader(train_tensor,batch_size=batch_size, shuffle = True)\n",
    "    del train_tensor\n",
    "    if val_split<1:\n",
    "        val_tensor = torch.utils.data.TensorDataset(torch.Tensor(val_data), torch.IntTensor(val_labels))\n",
    "        del val_data\n",
    "        del val_labels\n",
    "        val_loader = torch.utils.data.DataLoader(val_tensor,batch_size=batch_size, shuffle = True)\n",
    "        del val_tensor\n",
    "        \n",
    "    if (type(test_data) != type(None)):\n",
    "        test_tensor = torch.utils.data.TensorDataset(torch.Tensor(test_data),torch.IntTensor(test_labels))\n",
    "        test_loader = torch.utils.data.DataLoader(test_tensor,batch_size=batch_size, shuffle = True)\n",
    "        return train_loader, val_loader, test_loader\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def standardize_data(data):\n",
    "    s = StandardScaler()\n",
    "    s.fit(data)\n",
    "    s.var_ += 1e-5\n",
    "    standardized_data = s.transform(data)\n",
    "    return standardized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11269, 53975)\n",
      "(7505, 61188)\n",
      "(7505, 53975)\n"
     ]
    }
   ],
   "source": [
    "data = restructure_data(data)\n",
    "Vocab_size = data.shape[1]\n",
    "test_data = restructure_data(test_data)[:,:Vocab_size]\n",
    "print test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11269, 53975)\n",
      "(7505, 61188)\n",
      "(9015, 53975) (9015,)\n",
      "(2254, 53975) (2254,)\n",
      "Learning rate: 1e-06\n",
      "Epoch : 1 Loss : 3.018  Train Accuracy: 0.046 Validation Accuracy: 0.067 Test Accuracy: 0.061\n",
      "Epoch : 2 Loss : 2.988  Train Accuracy: 0.061 Validation Accuracy: 0.070 Test Accuracy: 0.068\n",
      "Epoch : 3 Loss : 2.957  Train Accuracy: 0.076 Validation Accuracy: 0.098 Test Accuracy: 0.089\n",
      "Epoch : 4 Loss : 2.919  Train Accuracy: 0.121 Validation Accuracy: 0.143 Test Accuracy: 0.120\n",
      "Epoch : 5 Loss : 2.872  Train Accuracy: 0.147 Validation Accuracy: 0.155 Test Accuracy: 0.136\n",
      "Epoch : 6 Loss : 2.828  Train Accuracy: 0.161 Validation Accuracy: 0.180 Test Accuracy: 0.169\n",
      "Epoch : 7 Loss : 2.758  Train Accuracy: 0.225 Validation Accuracy: 0.219 Test Accuracy: 0.200\n",
      "Epoch : 8 Loss : 2.699  Train Accuracy: 0.257 Validation Accuracy: 0.255 Test Accuracy: 0.223\n",
      "Epoch : 9 Loss : 2.621  Train Accuracy: 0.306 Validation Accuracy: 0.311 Test Accuracy: 0.263\n",
      "Epoch : 10 Loss : 2.581  Train Accuracy: 0.314 Validation Accuracy: 0.299 Test Accuracy: 0.263\n",
      "Epoch : 11 Loss : 2.491  Train Accuracy: 0.338 Validation Accuracy: 0.321 Test Accuracy: 0.297\n",
      "Epoch : 12 Loss : 2.433  Train Accuracy: 0.347 Validation Accuracy: 0.335 Test Accuracy: 0.300\n",
      "Epoch : 13 Loss : 2.353  Train Accuracy: 0.383 Validation Accuracy: 0.372 Test Accuracy: 0.323\n",
      "Epoch : 14 Loss : 2.284  Train Accuracy: 0.416 Validation Accuracy: 0.402 Test Accuracy: 0.359\n",
      "Epoch : 15 Loss : 2.291  Train Accuracy: 0.411 Validation Accuracy: 0.426 Test Accuracy: 0.372\n",
      "Epoch : 16 Loss : 2.206  Train Accuracy: 0.455 Validation Accuracy: 0.454 Test Accuracy: 0.388\n",
      "Epoch : 17 Loss : 2.085  Train Accuracy: 0.458 Validation Accuracy: 0.415 Test Accuracy: 0.365\n",
      "Epoch : 18 Loss : 2.139  Train Accuracy: 0.432 Validation Accuracy: 0.347 Test Accuracy: 0.321\n",
      "Epoch : 19 Loss : 2.201  Train Accuracy: 0.398 Validation Accuracy: 0.437 Test Accuracy: 0.403\n",
      "Epoch : 20 Loss : 1.966  Train Accuracy: 0.504 Validation Accuracy: 0.463 Test Accuracy: 0.398\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6a32a1e310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 1e-07\n",
      "Epoch : 1 Loss : 3.030  Train Accuracy: 0.040 Validation Accuracy: 0.042 Test Accuracy: 0.040\n",
      "Epoch : 2 Loss : 3.007  Train Accuracy: 0.040 Validation Accuracy: 0.049 Test Accuracy: 0.042\n",
      "Epoch : 3 Loss : 2.993  Train Accuracy: 0.045 Validation Accuracy: 0.063 Test Accuracy: 0.049\n",
      "Epoch : 4 Loss : 2.983  Train Accuracy: 0.053 Validation Accuracy: 0.070 Test Accuracy: 0.053\n",
      "Epoch : 5 Loss : 2.975  Train Accuracy: 0.061 Validation Accuracy: 0.071 Test Accuracy: 0.055\n",
      "Epoch : 6 Loss : 2.965  Train Accuracy: 0.064 Validation Accuracy: 0.072 Test Accuracy: 0.060\n",
      "Epoch : 7 Loss : 2.953  Train Accuracy: 0.071 Validation Accuracy: 0.083 Test Accuracy: 0.065\n",
      "Epoch : 8 Loss : 2.941  Train Accuracy: 0.080 Validation Accuracy: 0.086 Test Accuracy: 0.074\n",
      "Epoch : 9 Loss : 2.932  Train Accuracy: 0.092 Validation Accuracy: 0.102 Test Accuracy: 0.083\n",
      "Epoch : 10 Loss : 2.922  Train Accuracy: 0.106 Validation Accuracy: 0.115 Test Accuracy: 0.095\n",
      "Epoch : 11 Loss : 2.909  Train Accuracy: 0.121 Validation Accuracy: 0.133 Test Accuracy: 0.111\n",
      "Epoch : 12 Loss : 2.890  Train Accuracy: 0.135 Validation Accuracy: 0.144 Test Accuracy: 0.119\n",
      "Epoch : 13 Loss : 2.874  Train Accuracy: 0.144 Validation Accuracy: 0.150 Test Accuracy: 0.119\n",
      "Epoch : 14 Loss : 2.857  Train Accuracy: 0.155 Validation Accuracy: 0.162 Test Accuracy: 0.133\n",
      "Epoch : 15 Loss : 2.839  Train Accuracy: 0.170 Validation Accuracy: 0.180 Test Accuracy: 0.149\n",
      "Epoch : 16 Loss : 2.822  Train Accuracy: 0.189 Validation Accuracy: 0.189 Test Accuracy: 0.159\n",
      "Epoch : 17 Loss : 2.803  Train Accuracy: 0.210 Validation Accuracy: 0.205 Test Accuracy: 0.176\n",
      "Epoch : 18 Loss : 2.782  Train Accuracy: 0.222 Validation Accuracy: 0.213 Test Accuracy: 0.188\n",
      "Epoch : 19 Loss : 2.761  Train Accuracy: 0.229 Validation Accuracy: 0.222 Test Accuracy: 0.195\n",
      "Epoch : 20 Loss : 2.740  Train Accuracy: 0.239 Validation Accuracy: 0.233 Test Accuracy: 0.207\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6a32a1e350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 1e-08\n",
      "Epoch : 1 Loss : 3.043  Train Accuracy: 0.061 Validation Accuracy: 0.063 Test Accuracy: 0.064\n",
      "Epoch : 2 Loss : 3.039  Train Accuracy: 0.063 Validation Accuracy: 0.064 Test Accuracy: 0.066\n",
      "Epoch : 3 Loss : 3.033  Train Accuracy: 0.065 Validation Accuracy: 0.063 Test Accuracy: 0.067\n",
      "Epoch : 4 Loss : 3.024  Train Accuracy: 0.064 Validation Accuracy: 0.064 Test Accuracy: 0.067\n",
      "Epoch : 5 Loss : 3.016  Train Accuracy: 0.059 Validation Accuracy: 0.059 Test Accuracy: 0.064\n",
      "Epoch : 6 Loss : 3.013  Train Accuracy: 0.054 Validation Accuracy: 0.053 Test Accuracy: 0.058\n",
      "Epoch : 7 Loss : 3.010  Train Accuracy: 0.055 Validation Accuracy: 0.051 Test Accuracy: 0.054\n",
      "Epoch : 8 Loss : 3.009  Train Accuracy: 0.056 Validation Accuracy: 0.050 Test Accuracy: 0.053\n",
      "Epoch : 9 Loss : 3.007  Train Accuracy: 0.056 Validation Accuracy: 0.049 Test Accuracy: 0.052\n",
      "Epoch : 10 Loss : 3.002  Train Accuracy: 0.057 Validation Accuracy: 0.049 Test Accuracy: 0.050\n",
      "Epoch : 11 Loss : 2.998  Train Accuracy: 0.057 Validation Accuracy: 0.052 Test Accuracy: 0.051\n",
      "Epoch : 12 Loss : 2.994  Train Accuracy: 0.058 Validation Accuracy: 0.052 Test Accuracy: 0.053\n",
      "Epoch : 13 Loss : 2.989  Train Accuracy: 0.060 Validation Accuracy: 0.055 Test Accuracy: 0.056\n",
      "Epoch : 14 Loss : 2.986  Train Accuracy: 0.063 Validation Accuracy: 0.060 Test Accuracy: 0.058\n",
      "Epoch : 15 Loss : 2.981  Train Accuracy: 0.065 Validation Accuracy: 0.063 Test Accuracy: 0.062\n",
      "Epoch : 16 Loss : 2.977  Train Accuracy: 0.068 Validation Accuracy: 0.069 Test Accuracy: 0.064\n",
      "Epoch : 17 Loss : 2.972  Train Accuracy: 0.067 Validation Accuracy: 0.070 Test Accuracy: 0.063\n",
      "Epoch : 18 Loss : 2.968  Train Accuracy: 0.068 Validation Accuracy: 0.074 Test Accuracy: 0.066\n",
      "Epoch : 19 Loss : 2.964  Train Accuracy: 0.071 Validation Accuracy: 0.076 Test Accuracy: 0.071\n",
      "Epoch : 20 Loss : 2.963  Train Accuracy: 0.075 Validation Accuracy: 0.081 Test Accuracy: 0.074\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6a32a1e2d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 1e-09\n",
      "Epoch : 1 Loss : 3.007  Train Accuracy: 0.047 Validation Accuracy: 0.045 Test Accuracy: 0.053\n",
      "Epoch : 2 Loss : 3.006  Train Accuracy: 0.047 Validation Accuracy: 0.045 Test Accuracy: 0.052\n",
      "Epoch : 3 Loss : 3.005  Train Accuracy: 0.046 Validation Accuracy: 0.044 Test Accuracy: 0.053\n",
      "Epoch : 4 Loss : 3.005  Train Accuracy: 0.047 Validation Accuracy: 0.044 Test Accuracy: 0.052\n",
      "Epoch : 5 Loss : 3.004  Train Accuracy: 0.047 Validation Accuracy: 0.044 Test Accuracy: 0.052\n",
      "Epoch : 6 Loss : 3.004  Train Accuracy: 0.047 Validation Accuracy: 0.045 Test Accuracy: 0.052\n",
      "Epoch : 7 Loss : 3.002  Train Accuracy: 0.047 Validation Accuracy: 0.046 Test Accuracy: 0.053\n",
      "Epoch : 8 Loss : 3.002  Train Accuracy: 0.048 Validation Accuracy: 0.047 Test Accuracy: 0.054\n",
      "Epoch : 9 Loss : 3.001  Train Accuracy: 0.048 Validation Accuracy: 0.047 Test Accuracy: 0.054\n",
      "Epoch : 10 Loss : 3.000  Train Accuracy: 0.051 Validation Accuracy: 0.049 Test Accuracy: 0.054\n",
      "Epoch : 11 Loss : 2.998  Train Accuracy: 0.052 Validation Accuracy: 0.051 Test Accuracy: 0.055\n",
      "Epoch : 12 Loss : 2.997  Train Accuracy: 0.052 Validation Accuracy: 0.051 Test Accuracy: 0.055\n",
      "Epoch : 13 Loss : 2.996  Train Accuracy: 0.054 Validation Accuracy: 0.051 Test Accuracy: 0.056\n",
      "Epoch : 14 Loss : 2.994  Train Accuracy: 0.055 Validation Accuracy: 0.051 Test Accuracy: 0.058\n",
      "Epoch : 15 Loss : 2.993  Train Accuracy: 0.055 Validation Accuracy: 0.055 Test Accuracy: 0.058\n",
      "Epoch : 16 Loss : 2.991  Train Accuracy: 0.056 Validation Accuracy: 0.056 Test Accuracy: 0.058\n",
      "Epoch : 17 Loss : 2.990  Train Accuracy: 0.057 Validation Accuracy: 0.057 Test Accuracy: 0.059\n",
      "Epoch : 18 Loss : 2.988  Train Accuracy: 0.058 Validation Accuracy: 0.059 Test Accuracy: 0.060\n",
      "Epoch : 19 Loss : 2.987  Train Accuracy: 0.059 Validation Accuracy: 0.059 Test Accuracy: 0.060\n",
      "Epoch : 20 Loss : 2.986  Train Accuracy: 0.059 Validation Accuracy: 0.060 Test Accuracy: 0.060\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD8CAYAAABpcuN4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5XJQmJlZLgeFmZnlclCYmVmu/wEZjzhHlCFUpAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6a37ee1510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = np.loadtxt('./data/20news-bydate/matlab/train.data')\n",
    "labels = np.loadtxt('./data/20news-bydate/matlab/train.label')\n",
    "labels -= 1\n",
    "\n",
    "test_data = np.loadtxt('./data/20news-bydate/matlab/test.data')\n",
    "test_labels = np.loadtxt('./data/20news-bydate/matlab/test.label')\n",
    "test_labels -= 1\n",
    "\n",
    "data = restructure_data(data)\n",
    "Vocab_size = data.shape[1]\n",
    "test_data = restructure_data(test_data)[:,:Vocab_size]\n",
    "\n",
    "\n",
    "train_loader_newsgroups, val_loader_newsgroups, test_loader_newsgroups = make_loaders(data,labels,0.8,test_data,test_labels)\n",
    "del data\n",
    "del test_data\n",
    "\n",
    "\n",
    "lrs = [1e-4,1e-5,1e-6,1e-7]\n",
    "models = [0]*len(lrs)\n",
    "train_results = [0]*len(lrs)\n",
    "for i,lr in enumerate(lrs):\n",
    "    print \"Learning rate: \" + repr(lr)\n",
    "    models[i] = MLP_20(Vocab_size,20)\n",
    "    models[i] = GlorotInitialize(models[i])\n",
    "    optimizer = torch.optim.SGD(models[i].parameters(),lr=lr,momentum=0.9)\n",
    "    loss_crit = nn.CrossEntropyLoss()\n",
    "    train_results[i] = train(models[i],20,train_loader_newsgroups,optimizer,val_loader_newsgroups,test_loader_newsgroups)\n",
    "    (bestNetwork, epoch_loss, train_accuracy, val_accuracy, test_accuracy) = train_results[i]\n",
    "    \n",
    "    plt.plot(train_accuracy, label='Train accuracy')\n",
    "    plt.plot(val_accuracy, label='Validation accuracy')\n",
    "    plt.plot(test_accuracy, label='Test accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11269, 53975)\n",
      "(11269, 53975)\n",
      "(7505, 61188)\n",
      "(7505, 53975)\n",
      "(9015, 53975) (9015,)\n",
      "(2254, 53975) (2254,)\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF Preprocessing\n",
    "data = np.loadtxt('./data/20news-bydate/matlab/train.data')\n",
    "labels = np.loadtxt('./data/20news-bydate/matlab/train.label')\n",
    "labels -= 1\n",
    "\n",
    "test_data = np.loadtxt('./data/20news-bydate/matlab/test.data')\n",
    "test_labels = np.loadtxt('./data/20news-bydate/matlab/test.label')\n",
    "test_labels -= 1\n",
    "\n",
    "data = restructure_data(data)\n",
    "data = tfidf(data)\n",
    "Vocab_size = data.shape[1]\n",
    "test_data = restructure_data(test_data)[:,:Vocab_size]\n",
    "test_data = tfidf(test_data)\n",
    "\n",
    "train_loader_tfidf, val_loader_tfidf, test_loader_tfidf = make_loaders(data,labels,0.8,test_data,test_labels)\n",
    "del data\n",
    "del test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.001\n",
      "Epoch : 1 Loss : 3.641  Train Accuracy: 0.463 Validation Accuracy: 0.545 Test Accuracy: 0.498\n",
      "Epoch : 2 Loss : 23.806  Train Accuracy: 0.613 Validation Accuracy: 0.531 Test Accuracy: 0.444\n",
      "Epoch : 3 Loss : 100.750  Train Accuracy: 0.616 Validation Accuracy: 0.513 Test Accuracy: 0.437\n",
      "Epoch : 4 Loss : 299.960  Train Accuracy: 0.581 Validation Accuracy: 0.533 Test Accuracy: 0.460\n",
      "Epoch : 5 Loss : 4742.845  Train Accuracy: 0.613 Validation Accuracy: 0.464 Test Accuracy: 0.395\n",
      "Epoch : 6 Loss : 15556.319  Train Accuracy: 0.527 Validation Accuracy: 0.410 Test Accuracy: 0.344\n",
      "Epoch : 7 Loss : 6818.882  Train Accuracy: 0.466 Validation Accuracy: 0.384 Test Accuracy: 0.334\n",
      "Epoch : 8 Loss : 1285384.161  Train Accuracy: 0.424 Validation Accuracy: 0.340 Test Accuracy: 0.299\n",
      "Epoch : 9 Loss : 759963.529  Train Accuracy: 0.382 Validation Accuracy: 0.331 Test Accuracy: 0.292\n",
      "Epoch : 10 Loss : 374729.348  Train Accuracy: 0.391 Validation Accuracy: 0.354 Test Accuracy: 0.328\n",
      "Epoch : 11 Loss : 870472.906  Train Accuracy: 0.384 Validation Accuracy: 0.298 Test Accuracy: 0.281\n",
      "Epoch : 12 Loss : 36264659.200  Train Accuracy: 0.328 Validation Accuracy: 0.268 Test Accuracy: 0.233\n",
      "Epoch : 13 Loss : 2453922014.634  Train Accuracy: 0.291 Validation Accuracy: 0.231 Test Accuracy: 0.221\n",
      "Epoch : 14 Loss : 1478291997831.003  Train Accuracy: 0.303 Validation Accuracy: 0.303 Test Accuracy: 0.280\n",
      "Epoch : 15 Loss : 422960558.363  Train Accuracy: 0.330 Validation Accuracy: 0.274 Test Accuracy: 0.249\n",
      "Epoch : 16 Loss : 1287855526.203  Train Accuracy: 0.319 Validation Accuracy: 0.271 Test Accuracy: 0.249\n",
      "Epoch : 17 Loss : 1125177349.362  Train Accuracy: 0.304 Validation Accuracy: 0.267 Test Accuracy: 0.228\n",
      "Epoch : 18 Loss : 5367224.473  Train Accuracy: 0.278 Validation Accuracy: 0.237 Test Accuracy: 0.205\n",
      "Epoch : 19 Loss : 3089503.258  Train Accuracy: 0.243 Validation Accuracy: 0.218 Test Accuracy: 0.191\n",
      "Epoch : 20 Loss : 1645779244.902  Train Accuracy: 0.242 Validation Accuracy: 0.214 Test Accuracy: 0.183\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTtWS9GwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc7402cbf10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0001\n",
      "Epoch : 1 Loss : 2.398  Train Accuracy: 0.393 Validation Accuracy: 0.719 Test Accuracy: 0.640\n",
      "Epoch : 2 Loss : 0.789  Train Accuracy: 0.842 Validation Accuracy: 0.783 Test Accuracy: 0.705\n",
      "Epoch : 3 Loss : 1.435  Train Accuracy: 0.834 Validation Accuracy: 0.635 Test Accuracy: 0.541\n",
      "Epoch : 4 Loss : 4.770  Train Accuracy: 0.773 Validation Accuracy: 0.737 Test Accuracy: 0.659\n",
      "Epoch : 5 Loss : 2.866  Train Accuracy: 0.807 Validation Accuracy: 0.732 Test Accuracy: 0.624\n",
      "Epoch : 6 Loss : 4.912  Train Accuracy: 0.748 Validation Accuracy: 0.633 Test Accuracy: 0.535\n",
      "Epoch : 7 Loss : 13.443  Train Accuracy: 0.806 Validation Accuracy: 0.709 Test Accuracy: 0.602\n",
      "Epoch : 8 Loss : 2.543  Train Accuracy: 0.818 Validation Accuracy: 0.741 Test Accuracy: 0.635\n",
      "Epoch : 9 Loss : 3.095  Train Accuracy: 0.846 Validation Accuracy: 0.729 Test Accuracy: 0.614\n",
      "Epoch : 10 Loss : 1.806  Train Accuracy: 0.897 Validation Accuracy: 0.733 Test Accuracy: 0.617\n",
      "Epoch : 11 Loss : 12.957  Train Accuracy: 0.907 Validation Accuracy: 0.768 Test Accuracy: 0.644\n",
      "Epoch : 12 Loss : 7.848  Train Accuracy: 0.927 Validation Accuracy: 0.764 Test Accuracy: 0.631\n",
      "Epoch : 13 Loss : 33.351  Train Accuracy: 0.924 Validation Accuracy: 0.742 Test Accuracy: 0.620\n",
      "Epoch : 14 Loss : 45.310  Train Accuracy: 0.913 Validation Accuracy: 0.765 Test Accuracy: 0.652\n",
      "Epoch : 15 Loss : 28.875  Train Accuracy: 0.915 Validation Accuracy: 0.773 Test Accuracy: 0.663\n",
      "Epoch : 16 Loss : 17.119  Train Accuracy: 0.934 Validation Accuracy: 0.764 Test Accuracy: 0.646\n",
      "Epoch : 17 Loss : 6.810  Train Accuracy: 0.946 Validation Accuracy: 0.791 Test Accuracy: 0.664\n",
      "Epoch : 18 Loss : 8.776  Train Accuracy: 0.943 Validation Accuracy: 0.776 Test Accuracy: 0.653\n",
      "Epoch : 19 Loss : 7.124  Train Accuracy: 0.949 Validation Accuracy: 0.791 Test Accuracy: 0.666\n",
      "Epoch : 20 Loss : 10.843  Train Accuracy: 0.960 Validation Accuracy: 0.795 Test Accuracy: 0.671\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5C6EEG2QJHchhGiD/h/dErJtVpxdcwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc7403549d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 1e-05\n",
      "Epoch : 1 Loss : 2.955  Train Accuracy: 0.121 Validation Accuracy: 0.295 Test Accuracy: 0.259\n",
      "Epoch : 2 Loss : 2.205  Train Accuracy: 0.567 Validation Accuracy: 0.659 Test Accuracy: 0.590\n",
      "Epoch : 3 Loss : 1.359  Train Accuracy: 0.812 Validation Accuracy: 0.789 Test Accuracy: 0.714\n",
      "Epoch : 4 Loss : 0.775  Train Accuracy: 0.874 Validation Accuracy: 0.825 Test Accuracy: 0.747\n",
      "Epoch : 5 Loss : 0.483  Train Accuracy: 0.905 Validation Accuracy: 0.839 Test Accuracy: 0.754\n",
      "Epoch : 6 Loss : 0.363  Train Accuracy: 0.917 Validation Accuracy: 0.841 Test Accuracy: 0.756\n",
      "Epoch : 7 Loss : 0.294  Train Accuracy: 0.925 Validation Accuracy: 0.835 Test Accuracy: 0.757\n",
      "Epoch : 8 Loss : 0.238  Train Accuracy: 0.942 Validation Accuracy: 0.854 Test Accuracy: 0.778\n",
      "Epoch : 9 Loss : 0.222  Train Accuracy: 0.946 Validation Accuracy: 0.851 Test Accuracy: 0.769\n",
      "Epoch : 10 Loss : 0.213  Train Accuracy: 0.952 Validation Accuracy: 0.805 Test Accuracy: 0.727\n",
      "Epoch : 11 Loss : 0.261  Train Accuracy: 0.951 Validation Accuracy: 0.840 Test Accuracy: 0.751\n",
      "Epoch : 12 Loss : 0.641  Train Accuracy: 0.940 Validation Accuracy: 0.825 Test Accuracy: 0.742\n",
      "Epoch : 13 Loss : 2.078  Train Accuracy: 0.919 Validation Accuracy: 0.812 Test Accuracy: 0.739\n",
      "Epoch : 14 Loss : 1.525  Train Accuracy: 0.938 Validation Accuracy: 0.793 Test Accuracy: 0.712\n",
      "Epoch : 15 Loss : 4.286  Train Accuracy: 0.918 Validation Accuracy: 0.810 Test Accuracy: 0.712\n",
      "Epoch : 16 Loss : 1.000  Train Accuracy: 0.951 Validation Accuracy: 0.810 Test Accuracy: 0.721\n",
      "Epoch : 17 Loss : 3.585  Train Accuracy: 0.900 Validation Accuracy: 0.808 Test Accuracy: 0.720\n",
      "Epoch : 18 Loss : 1.423  Train Accuracy: 0.949 Validation Accuracy: 0.830 Test Accuracy: 0.735\n",
      "Epoch : 19 Loss : 0.697  Train Accuracy: 0.966 Validation Accuracy: 0.846 Test Accuracy: 0.750\n",
      "Epoch : 20 Loss : 1.750  Train Accuracy: 0.924 Validation Accuracy: 0.796 Test Accuracy: 0.693\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncfoQL+6O4zh9iBd3x3GcPuQ5kfFHVhgayRUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc740180ad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 1e-06\n",
      "Epoch : 1 Loss : 3.117  Train Accuracy: 0.053 Validation Accuracy: 0.075 Test Accuracy: 0.062\n",
      "Epoch : 2 Loss : 2.938  Train Accuracy: 0.104 Validation Accuracy: 0.153 Test Accuracy: 0.126\n",
      "Epoch : 3 Loss : 2.720  Train Accuracy: 0.246 Validation Accuracy: 0.291 Test Accuracy: 0.246\n",
      "Epoch : 4 Loss : 2.489  Train Accuracy: 0.433 Validation Accuracy: 0.442 Test Accuracy: 0.380\n",
      "Epoch : 5 Loss : 2.234  Train Accuracy: 0.597 Validation Accuracy: 0.570 Test Accuracy: 0.497\n",
      "Epoch : 6 Loss : 1.953  Train Accuracy: 0.705 Validation Accuracy: 0.654 Test Accuracy: 0.586\n",
      "Epoch : 7 Loss : 1.659  Train Accuracy: 0.779 Validation Accuracy: 0.715 Test Accuracy: 0.645\n",
      "Epoch : 8 Loss : 1.381  Train Accuracy: 0.825 Validation Accuracy: 0.755 Test Accuracy: 0.686\n",
      "Epoch : 9 Loss : 1.141  Train Accuracy: 0.855 Validation Accuracy: 0.793 Test Accuracy: 0.712\n",
      "Epoch : 10 Loss : 0.945  Train Accuracy: 0.872 Validation Accuracy: 0.812 Test Accuracy: 0.727\n",
      "Epoch : 11 Loss : 0.791  Train Accuracy: 0.888 Validation Accuracy: 0.819 Test Accuracy: 0.740\n",
      "Epoch : 12 Loss : 0.671  Train Accuracy: 0.897 Validation Accuracy: 0.824 Test Accuracy: 0.744\n",
      "Epoch : 13 Loss : 0.575  Train Accuracy: 0.904 Validation Accuracy: 0.831 Test Accuracy: 0.752\n",
      "Epoch : 14 Loss : 0.492  Train Accuracy: 0.913 Validation Accuracy: 0.842 Test Accuracy: 0.762\n",
      "Epoch : 15 Loss : 0.428  Train Accuracy: 0.920 Validation Accuracy: 0.845 Test Accuracy: 0.764\n",
      "Epoch : 16 Loss : 0.379  Train Accuracy: 0.923 Validation Accuracy: 0.843 Test Accuracy: 0.764\n",
      "Epoch : 17 Loss : 0.345  Train Accuracy: 0.924 Validation Accuracy: 0.840 Test Accuracy: 0.761\n",
      "Epoch : 18 Loss : 0.317  Train Accuracy: 0.927 Validation Accuracy: 0.839 Test Accuracy: 0.760\n",
      "Epoch : 19 Loss : 0.293  Train Accuracy: 0.930 Validation Accuracy: 0.836 Test Accuracy: 0.760\n",
      "Epoch : 20 Loss : 0.270  Train Accuracy: 0.934 Validation Accuracy: 0.839 Test Accuracy: 0.763\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGu4iIi2k4S4i0kIa7iIiLXQHvWqNkvqt9GwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc740180f90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrs = [1e-5,1e-6,1e-7,1e-8]\n",
    "models_tfidf = [0]*len(lrs)\n",
    "train_results_tfidf = [0]*len(lrs)\n",
    "for i,lr in enumerate(lrs):\n",
    "    print \"Learning rate: \" + repr(lr)\n",
    "    models_tfidf[i] = MLP_20(Vocab_size,20)\n",
    "    models_tfidf[i] = GlorotInitialize(models_tfidf[i])\n",
    "    optimizer = torch.optim.SGD(models_tfidf[i].parameters(),lr=lr,momentum=0.9)\n",
    "    loss_crit = nn.CrossEntropyLoss()\n",
    "    train_results_tfidf[i] = train(models_tfidf[i],20,train_loader_tfidf,optimizer,val_loader_tfidf,test_loader_tfidf)\n",
    "    (bestNetwork, epoch_loss, train_accuracy, val_accuracy, test_accuracy) = train_results_tfidf[i]\n",
    "    \n",
    "    plt.plot(train_accuracy, label='Train accuracy')\n",
    "    plt.plot(val_accuracy, label='Validation accuracy')\n",
    "    plt.plot(test_accuracy, label='Test accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Standardization preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f59e8413fad2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/20news-bydate/matlab/train.data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/20news-bydate/matlab/train.label'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/20news-bydate/matlab/test.data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kyle/Downloads/yes/lib/python2.7/site-packages/numpy/lib/npyio.pyc\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m             \u001b[0;31m# Convert each value according to its column and store\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;31m# Then pack it according to the dtype's nesting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpacking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kyle/Downloads/yes/lib/python2.7/site-packages/numpy/lib/npyio.pyc\u001b[0m in \u001b[0;36mfloatconv\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34mb'0x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromhex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0mtyp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = np.loadtxt('./data/20news-bydate/matlab/train.data')\n",
    "labels = np.loadtxt('./data/20news-bydate/matlab/train.label')\n",
    "labels -= 1\n",
    "\n",
    "test_data = np.loadtxt('./data/20news-bydate/matlab/test.data')\n",
    "test_labels = np.loadtxt('./data/20news-bydate/matlab/test.label')\n",
    "test_labels -= 1\n",
    "\n",
    "data = restructure_data(data)\n",
    "data = standardize_data(data)\n",
    "Vocab_size = data.shape[1]\n",
    "test_data = restructure_data(test_data)[:,:Vocab_size]\n",
    "test_data = standardize_data(test_data)\n",
    "\n",
    "train_loader_std, val_loader_std, test_loader_std = make_loaders(data,labels,0.8,test_data,test_labels)\n",
    "del data\n",
    "del test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0001\n",
      "Epoch : 1 Loss : 2.402  Train Accuracy: 0.356 Validation Accuracy: 0.726 Test Accuracy: 0.620\n",
      "Epoch : 2 Loss : 0.257  Train Accuracy: 0.935 Validation Accuracy: 0.800 Test Accuracy: 0.676\n",
      "Epoch : 3 Loss : 0.179  Train Accuracy: 0.965 Validation Accuracy: 0.817 Test Accuracy: 0.689\n",
      "Epoch : 4 Loss : 0.210  Train Accuracy: 0.969 Validation Accuracy: 0.841 Test Accuracy: 0.711\n",
      "Epoch : 5 Loss : 0.526  Train Accuracy: 0.963 Validation Accuracy: 0.839 Test Accuracy: 0.716\n",
      "Epoch : 6 Loss : 0.572  Train Accuracy: 0.967 Validation Accuracy: 0.832 Test Accuracy: 0.702\n",
      "Epoch : 7 Loss : 0.986  Train Accuracy: 0.968 Validation Accuracy: 0.818 Test Accuracy: 0.690\n",
      "Epoch : 8 Loss : 1.752  Train Accuracy: 0.966 Validation Accuracy: 0.823 Test Accuracy: 0.699\n",
      "Epoch : 9 Loss : 2.973  Train Accuracy: 0.964 Validation Accuracy: 0.816 Test Accuracy: 0.684\n",
      "Epoch : 10 Loss : 0.705  Train Accuracy: 0.985 Validation Accuracy: 0.833 Test Accuracy: 0.697\n",
      "Epoch : 11 Loss : 2.505  Train Accuracy: 0.983 Validation Accuracy: 0.831 Test Accuracy: 0.690\n",
      "Epoch : 12 Loss : 2.882  Train Accuracy: 0.987 Validation Accuracy: 0.832 Test Accuracy: 0.683\n",
      "Epoch : 13 Loss : 2.047  Train Accuracy: 0.988 Validation Accuracy: 0.812 Test Accuracy: 0.652\n",
      "Epoch : 14 Loss : 4.938  Train Accuracy: 0.987 Validation Accuracy: 0.816 Test Accuracy: 0.649\n",
      "Epoch : 15 Loss : 1.598  Train Accuracy: 0.993 Validation Accuracy: 0.813 Test Accuracy: 0.659\n",
      "Epoch : 16 Loss : 5.491  Train Accuracy: 0.989 Validation Accuracy: 0.804 Test Accuracy: 0.655\n",
      "Epoch : 17 Loss : 6.859  Train Accuracy: 0.990 Validation Accuracy: 0.796 Test Accuracy: 0.645\n",
      "Epoch : 18 Loss : 6.006  Train Accuracy: 0.992 Validation Accuracy: 0.789 Test Accuracy: 0.642\n",
      "Epoch : 19 Loss : 7.183  Train Accuracy: 0.990 Validation Accuracy: 0.785 Test Accuracy: 0.637\n",
      "Epoch : 20 Loss : 13.172  Train Accuracy: 0.989 Validation Accuracy: 0.782 Test Accuracy: 0.630\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCCHCkIS7EEKEIQl3IYQIQ/8fzlttPkvMM9UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f53f0df67d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 1e-05\n",
      "Epoch : 1 Loss : 3.428  Train Accuracy: 0.077 Validation Accuracy: 0.178 Test Accuracy: 0.149\n",
      "Epoch : 2 Loss : 1.452  Train Accuracy: 0.678 Validation Accuracy: 0.628 Test Accuracy: 0.524\n",
      "Epoch : 3 Loss : 0.491  Train Accuracy: 0.940 Validation Accuracy: 0.766 Test Accuracy: 0.656\n",
      "Epoch : 4 Loss : 0.226  Train Accuracy: 0.957 Validation Accuracy: 0.779 Test Accuracy: 0.661\n",
      "Epoch : 5 Loss : 0.161  Train Accuracy: 0.959 Validation Accuracy: 0.780 Test Accuracy: 0.658\n",
      "Epoch : 6 Loss : 0.121  Train Accuracy: 0.970 Validation Accuracy: 0.796 Test Accuracy: 0.669\n",
      "Epoch : 7 Loss : 0.088  Train Accuracy: 0.976 Validation Accuracy: 0.811 Test Accuracy: 0.676\n",
      "Epoch : 8 Loss : 0.064  Train Accuracy: 0.984 Validation Accuracy: 0.831 Test Accuracy: 0.697\n",
      "Epoch : 9 Loss : 0.059  Train Accuracy: 0.986 Validation Accuracy: 0.841 Test Accuracy: 0.708\n",
      "Epoch : 10 Loss : 0.056  Train Accuracy: 0.986 Validation Accuracy: 0.850 Test Accuracy: 0.716\n",
      "Epoch : 11 Loss : 0.047  Train Accuracy: 0.988 Validation Accuracy: 0.859 Test Accuracy: 0.728\n",
      "Epoch : 12 Loss : 0.041  Train Accuracy: 0.990 Validation Accuracy: 0.864 Test Accuracy: 0.734\n",
      "Epoch : 13 Loss : 0.034  Train Accuracy: 0.992 Validation Accuracy: 0.872 Test Accuracy: 0.741\n",
      "Epoch : 14 Loss : 0.031  Train Accuracy: 0.994 Validation Accuracy: 0.870 Test Accuracy: 0.745\n",
      "Epoch : 15 Loss : 0.032  Train Accuracy: 0.993 Validation Accuracy: 0.868 Test Accuracy: 0.739\n",
      "Epoch : 16 Loss : 0.040  Train Accuracy: 0.992 Validation Accuracy: 0.868 Test Accuracy: 0.740\n",
      "Epoch : 17 Loss : 0.036  Train Accuracy: 0.993 Validation Accuracy: 0.869 Test Accuracy: 0.742\n",
      "Epoch : 18 Loss : 0.030  Train Accuracy: 0.994 Validation Accuracy: 0.866 Test Accuracy: 0.740\n",
      "Epoch : 19 Loss : 0.037  Train Accuracy: 0.994 Validation Accuracy: 0.861 Test Accuracy: 0.736\n",
      "Epoch : 20 Loss : 0.040  Train Accuracy: 0.994 Validation Accuracy: 0.858 Test Accuracy: 0.738\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nM3G3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f537bc03290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 1e-06\n",
      "Epoch : 1 Loss : 3.648  Train Accuracy: 0.053 Validation Accuracy: 0.056 Test Accuracy: 0.058\n",
      "Epoch : 2 Loss : 3.207  Train Accuracy: 0.094 Validation Accuracy: 0.093 Test Accuracy: 0.085\n",
      "Epoch : 3 Loss : 2.542  Train Accuracy: 0.248 Validation Accuracy: 0.187 Test Accuracy: 0.155\n",
      "Epoch : 4 Loss : 1.876  Train Accuracy: 0.513 Validation Accuracy: 0.345 Test Accuracy: 0.281\n",
      "Epoch : 5 Loss : 1.340  Train Accuracy: 0.744 Validation Accuracy: 0.500 Test Accuracy: 0.407\n",
      "Epoch : 6 Loss : 0.953  Train Accuracy: 0.864 Validation Accuracy: 0.610 Test Accuracy: 0.503\n",
      "Epoch : 7 Loss : 0.692  Train Accuracy: 0.920 Validation Accuracy: 0.672 Test Accuracy: 0.564\n",
      "Epoch : 8 Loss : 0.517  Train Accuracy: 0.945 Validation Accuracy: 0.714 Test Accuracy: 0.605\n",
      "Epoch : 9 Loss : 0.395  Train Accuracy: 0.954 Validation Accuracy: 0.734 Test Accuracy: 0.629\n",
      "Epoch : 10 Loss : 0.313  Train Accuracy: 0.956 Validation Accuracy: 0.751 Test Accuracy: 0.640\n",
      "Epoch : 11 Loss : 0.257  Train Accuracy: 0.957 Validation Accuracy: 0.760 Test Accuracy: 0.647\n",
      "Epoch : 12 Loss : 0.216  Train Accuracy: 0.959 Validation Accuracy: 0.764 Test Accuracy: 0.652\n",
      "Epoch : 13 Loss : 0.185  Train Accuracy: 0.962 Validation Accuracy: 0.770 Test Accuracy: 0.657\n",
      "Epoch : 14 Loss : 0.158  Train Accuracy: 0.964 Validation Accuracy: 0.777 Test Accuracy: 0.664\n",
      "Epoch : 15 Loss : 0.136  Train Accuracy: 0.970 Validation Accuracy: 0.787 Test Accuracy: 0.672\n",
      "Epoch : 16 Loss : 0.115  Train Accuracy: 0.973 Validation Accuracy: 0.799 Test Accuracy: 0.680\n",
      "Epoch : 17 Loss : 0.098  Train Accuracy: 0.977 Validation Accuracy: 0.806 Test Accuracy: 0.688\n",
      "Epoch : 18 Loss : 0.084  Train Accuracy: 0.981 Validation Accuracy: 0.819 Test Accuracy: 0.698\n",
      "Epoch : 19 Loss : 0.074  Train Accuracy: 0.984 Validation Accuracy: 0.830 Test Accuracy: 0.707\n",
      "Epoch : 20 Loss : 0.065  Train Accuracy: 0.986 Validation Accuracy: 0.837 Test Accuracy: 0.713\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4S5JBRnuklSQ4S5JBRnuklTQEwRKLyL6mQYiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f537bc03310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 1e-07\n",
      "Epoch : 1 Loss : 3.759  Train Accuracy: 0.052 Validation Accuracy: 0.058 Test Accuracy: 0.052\n",
      "Epoch : 2 Loss : 3.705  Train Accuracy: 0.056 Validation Accuracy: 0.058 Test Accuracy: 0.054\n",
      "Epoch : 3 Loss : 3.593  Train Accuracy: 0.062 Validation Accuracy: 0.061 Test Accuracy: 0.058\n",
      "Epoch : 4 Loss : 3.437  Train Accuracy: 0.073 Validation Accuracy: 0.066 Test Accuracy: 0.062\n",
      "Epoch : 5 Loss : 3.242  Train Accuracy: 0.090 Validation Accuracy: 0.070 Test Accuracy: 0.070\n",
      "Epoch : 6 Loss : 3.028  Train Accuracy: 0.122 Validation Accuracy: 0.083 Test Accuracy: 0.081\n",
      "Epoch : 7 Loss : 2.804  Train Accuracy: 0.167 Validation Accuracy: 0.104 Test Accuracy: 0.100\n",
      "Epoch : 8 Loss : 2.571  Train Accuracy: 0.231 Validation Accuracy: 0.134 Test Accuracy: 0.120\n",
      "Epoch : 9 Loss : 2.340  Train Accuracy: 0.313 Validation Accuracy: 0.157 Test Accuracy: 0.148\n",
      "Epoch : 10 Loss : 2.115  Train Accuracy: 0.406 Validation Accuracy: 0.195 Test Accuracy: 0.179\n",
      "Epoch : 11 Loss : 1.901  Train Accuracy: 0.503 Validation Accuracy: 0.245 Test Accuracy: 0.221\n",
      "Epoch : 12 Loss : 1.701  Train Accuracy: 0.596 Validation Accuracy: 0.297 Test Accuracy: 0.260\n",
      "Epoch : 13 Loss : 1.517  Train Accuracy: 0.675 Validation Accuracy: 0.350 Test Accuracy: 0.303\n",
      "Epoch : 14 Loss : 1.354  Train Accuracy: 0.742 Validation Accuracy: 0.415 Test Accuracy: 0.342\n",
      "Epoch : 15 Loss : 1.205  Train Accuracy: 0.796 Validation Accuracy: 0.472 Test Accuracy: 0.380\n",
      "Epoch : 16 Loss : 1.075  Train Accuracy: 0.838 Validation Accuracy: 0.523 Test Accuracy: 0.419\n",
      "Epoch : 17 Loss : 0.961  Train Accuracy: 0.868 Validation Accuracy: 0.560 Test Accuracy: 0.452\n",
      "Epoch : 18 Loss : 0.858  Train Accuracy: 0.892 Validation Accuracy: 0.592 Test Accuracy: 0.483\n",
      "Epoch : 19 Loss : 0.771  Train Accuracy: 0.907 Validation Accuracy: 0.618 Test Accuracy: 0.511\n",
      "Epoch : 20 Loss : 0.693  Train Accuracy: 0.919 Validation Accuracy: 0.642 Test Accuracy: 0.532\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f537f488c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrs = [1e-4,1e-5,1e-6,1e-7]\n",
    "models_std = [0]*len(lrs)\n",
    "train_results_std = [0]*len(lrs)\n",
    "for i,lr in enumerate(lrs):\n",
    "    print \"Learning rate: \" + repr(lr)\n",
    "    models_std[i] = MLP_20(Vocab_size,20)\n",
    "    models_std[i] = GlorotInitialize(models_std[i])\n",
    "    optimizer = torch.optim.SGD(models_std[i].parameters(),lr=lr,momentum=0.9)\n",
    "    loss_crit = nn.CrossEntropyLoss()\n",
    "    train_results_std[i] = train(models_std[i],20,train_loader_std,optimizer,val_loader_std,test_loader_std)\n",
    "    (bestNetwork, epoch_loss, train_accuracy, val_accuracy, test_accuracy) = train_results_std[i]\n",
    "    \n",
    "    plt.plot(train_accuracy, label='Train accuracy')\n",
    "    plt.plot(val_accuracy, label='Validation accuracy')\n",
    "    plt.plot(test_accuracy, label='Test accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance in Training\n",
    "\n",
    "Best results from before:\n",
    "\n",
    "\n",
    "Using the best performing network for this section. Standardization with learning rate 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_5000(model,train_loader,optimizer,loss_func):\n",
    "    losses = [0]*5000\n",
    "    batch_number = 0\n",
    "    model = GlorotInitialize(model)\n",
    "    for i in range(1000):\n",
    "        for batch_index, (inputs, targets) in enumerate(train_loader):\n",
    "            \n",
    "            x, targets = Variable(inputs.view([-1,model.insize])), Variable(targets)\n",
    "\n",
    "            logits = model.forward(x)\n",
    "            loss = loss_func(logits,targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses[batch_number] = (loss.data[0])\n",
    "            print \"Iteration: \" + repr(batch_number + 1) + \" Loss: \" + repr(loss.data[0])\n",
    "            batch_number += 1\n",
    "            if batch_number == 5000:\n",
    "                plt.plot(losses, label='Train losses')\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "                return model,losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11269, 53975) (11269,)\n",
      "(0, 53975) (0,)\n",
      "Iteration: 1 Loss: Variable containing:\n",
      " 4.3365\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2 Loss: Variable containing:\n",
      " 4.2725\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3 Loss: Variable containing:\n",
      " 15.2051\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4 Loss: Variable containing:\n",
      " 49.0164\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 5 Loss: Variable containing:\n",
      " 106.3580\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 6 Loss: Variable containing:\n",
      " 126.7990\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 7 Loss: Variable containing:\n",
      " 2332.5491\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 8 Loss: Variable containing:\n",
      " 7783.4409\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 9 Loss: Variable containing:\n",
      "1.00000e+05 *\n",
      "  1.0858\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 10 Loss: Variable containing:\n",
      " 56449.0469\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 11 Loss: Variable containing:\n",
      "1.00000e+06 *\n",
      "  4.6409\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 12 Loss: Variable containing:\n",
      " 3.3458e+07\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 13 Loss: Variable containing:\n",
      " 2.9288e+07\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 14 Loss: Variable containing:\n",
      " 9.5465e+09\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 15 Loss: Variable containing:\n",
      " 1.2499e+11\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 16 Loss: Variable containing:\n",
      " 4.0402e+11\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 17 Loss: Variable containing:\n",
      " 5.4140e+07\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 18 Loss: Variable containing:\n",
      " 1.2278e+13\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 19 Loss: Variable containing:\n",
      " 3.9569e+13\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 20 Loss: Variable containing:\n",
      " 4.1524e+14\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 21 Loss: Variable containing:\n",
      " 3.6182e+08\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 22 Loss: Variable containing:\n",
      " 8.4414e+15\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 23 Loss: Variable containing:\n",
      " 1.1870e+13\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 24 Loss: Variable containing:\n",
      " 2.2873e+17\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 25 Loss: Variable containing:\n",
      " 3.6694e+18\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 26 Loss: Variable containing:\n",
      " 2.6117e+19\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 27 Loss: Variable containing:\n",
      " 5.8162e+12\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 28 Loss: Variable containing:\n",
      " 8.0480e+20\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 29 Loss: Variable containing:\n",
      " 5.9259e+14\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 30 Loss: Variable containing:\n",
      " 7.8780e+22\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 31 Loss: Variable containing:\n",
      " 1.3738e+23\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 32 Loss: Variable containing:\n",
      " 9.2661e+23\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 33 Loss: Variable containing:\n",
      " 0\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 34 Loss: Variable containing:\n",
      " 4.5001e+25\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 35 Loss: Variable containing:\n",
      " 2.5115e+26\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 36 Loss: Variable containing:\n",
      " 2.0276e+27\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 37 Loss: Variable containing:\n",
      " 4.9553e+27\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 38 Loss: Variable containing:\n",
      " 6.8511e+18\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 39 Loss: Variable containing:\n",
      " 1.4509e+17\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 40 Loss: Variable containing:\n",
      " 1.5345e+19\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 41 Loss: Variable containing:\n",
      " 137.8909\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 42 Loss: Variable containing:\n",
      " 1.1521e+17\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 43 Loss: Variable containing:\n",
      " 5.8477e+13\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 44 Loss: Variable containing:\n",
      " 153.1650\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 45 Loss: Variable containing:\n",
      " 74.0076\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 46 Loss: Variable containing:\n",
      " 2.0512e+31\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 47 Loss: Variable containing:\n",
      " 6.3102e+21\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 48 Loss: Variable containing:\n",
      " 3.9882e+32\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 49 Loss: Variable containing:\n",
      " 1.3754e+24\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 50 Loss: Variable containing:\n",
      " 1.7272e+23\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 51 Loss: Variable containing:\n",
      " 4.7946e+35\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 52 Loss: Variable containing:\n",
      " 212.6346\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 53 Loss: Variable containing:\n",
      " 2.2747e+38\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 54 Loss: Variable containing:\n",
      " 130.2073\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 55 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 56 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 57 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 58 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 59 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 60 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 61 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 62 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 63 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 64 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 65 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 66 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 67 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 68 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 69 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 70 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 71 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 72 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 73 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 74 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 75 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 76 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 77 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 78 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 79 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 80 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 81 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 82 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 83 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 84 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 85 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 86 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 87 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 88 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 89 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 90 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 91 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 92 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 93 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 94 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 95 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 96 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 97 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 98 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 99 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 100 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 101 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 102 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 103 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 104 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 105 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 106 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 107 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 108 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 109 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 110 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 111 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 112 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 113 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 114 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 115 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 116 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 117 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 118 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 119 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 120 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 121 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 122 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 123 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 124 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 125 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 126 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 127 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 128 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 129 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 130 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 131 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 132 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 133 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 134 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 135 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 136 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 137 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 138 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 139 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 140 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 141 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 142 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 143 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 144 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 145 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 146 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 147 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 148 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 149 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 150 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 151 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 152 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 153 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 154 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 155 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 156 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 157 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 158 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 159 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 160 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 161 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 162 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 163 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 164 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 165 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 166 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 167 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 168 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 169 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 170 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 171 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 172 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 173 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 174 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 175 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 176 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 177 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 178 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 179 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 180 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 181 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 182 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 183 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 184 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 185 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 186 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 187 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 188 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 189 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 190 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 191 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 192 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 193 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 194 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 195 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 196 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 197 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 198 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 199 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 200 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 201 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 202 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 203 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 204 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 205 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 206 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 207 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 208 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 209 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 210 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 211 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 212 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 213 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 214 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 215 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 216 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 217 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 218 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 219 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 220 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 221 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 222 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 223 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 224 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 225 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 226 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 227 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 228 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 229 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 230 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 231 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 232 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 233 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 234 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 235 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 236 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 237 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 238 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 239 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 240 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 241 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 242 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 243 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 244 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 245 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 246 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 247 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 248 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 249 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 250 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 251 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 252 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 253 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 254 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 255 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 256 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 257 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 258 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 259 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 260 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 261 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 262 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 263 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 264 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 265 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 266 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 267 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 268 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 269 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 270 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 271 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 272 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 273 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 274 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 275 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 276 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 277 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 278 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 279 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 280 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 281 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 282 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 283 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 284 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 285 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 286 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 287 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 288 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 289 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 290 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 291 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 292 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 293 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 294 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 295 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 296 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 297 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 298 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 299 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 300 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 301 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 302 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 303 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 304 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 305 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 306 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 307 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 308 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 309 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 310 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 311 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 312 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 313 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 314 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 315 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 316 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 317 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 318 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 319 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 320 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 321 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 322 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 323 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 324 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 325 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 326 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 327 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 328 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 329 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 330 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 331 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 332 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 333 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 334 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 335 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 336 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 337 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 338 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 339 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 340 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 341 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 342 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 343 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 344 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 345 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 346 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 347 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 348 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 349 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 350 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 351 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 352 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 353 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 354 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 355 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 356 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 357 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 358 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 359 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 360 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 361 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 362 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 363 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 364 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 365 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 366 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 367 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 368 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 369 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 370 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 371 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 372 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 373 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 374 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 375 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 376 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 377 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 378 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 379 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 380 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 381 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 382 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 383 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 384 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 385 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 386 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 387 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 388 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 389 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 390 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 391 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 392 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 393 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 394 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 395 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 396 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 397 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 398 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 399 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 400 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 401 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 402 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 403 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 404 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 405 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 406 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 407 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 408 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 409 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 410 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 411 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 412 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 413 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 414 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 415 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 416 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 417 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 418 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 419 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 420 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 421 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 422 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 423 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 424 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 425 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 426 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 427 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 428 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 429 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 430 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 431 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 432 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 433 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 434 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 435 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 436 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 437 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 438 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 439 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 440 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 441 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 442 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 443 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 444 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 445 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 446 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 447 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 448 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 449 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 450 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 451 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 452 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 453 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 454 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 455 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 456 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 457 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 458 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 459 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 460 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 461 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 462 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 463 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 464 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 465 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 466 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 467 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 468 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 469 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 470 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 471 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 472 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 473 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 474 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 475 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 476 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 477 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 478 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 479 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 480 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 481 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 482 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 483 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 484 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 485 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 486 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 487 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 488 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 489 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 490 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 491 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 492 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 493 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 494 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 495 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 496 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 497 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 498 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 499 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 500 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 501 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 502 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 503 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 504 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 505 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 506 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 507 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 508 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 509 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 510 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 511 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 512 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 513 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 514 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 515 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 516 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 517 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 518 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 519 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 520 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 521 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 522 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 523 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 524 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 525 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 526 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 527 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 528 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 529 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 530 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 531 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 532 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 533 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 534 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 535 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 536 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 537 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 538 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 539 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 540 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 541 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 542 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 543 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 544 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 545 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 546 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 547 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 548 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 549 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 550 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 551 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 552 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 553 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 554 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 555 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 556 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 557 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 558 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 559 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 560 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 561 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 562 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 563 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 564 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 565 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 566 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 567 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 568 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 569 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 570 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 571 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 572 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 573 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 574 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 575 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 576 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 577 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 578 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 579 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 580 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 581 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 582 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 583 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 584 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 585 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 586 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 587 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 588 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 589 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 590 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 591 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 592 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 593 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 594 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 595 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 596 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 597 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 598 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 599 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 600 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 601 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 602 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 603 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 604 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 605 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 606 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 607 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 608 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 609 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 610 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 611 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 612 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 613 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 614 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 615 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 616 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 617 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 618 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 619 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 620 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 621 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 622 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 623 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 624 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 625 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 626 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 627 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 628 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 629 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 630 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 631 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 632 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 633 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 634 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 635 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 636 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 637 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 638 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 639 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 640 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 641 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 642 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 643 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 644 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 645 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 646 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 647 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 648 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 649 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 650 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 651 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 652 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 653 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 654 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 655 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 656 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 657 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 658 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 659 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 660 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 661 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 662 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 663 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 664 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 665 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 666 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 667 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 668 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 669 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 670 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 671 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 672 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 673 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 674 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 675 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 676 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 677 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 678 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 679 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 680 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 681 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 682 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 683 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 684 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 685 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 686 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 687 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 688 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 689 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 690 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 691 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 692 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 693 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 694 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 695 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 696 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 697 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 698 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 699 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 700 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 701 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 702 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 703 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 704 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 705 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 706 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 707 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 708 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 709 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 710 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 711 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 712 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 713 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 714 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 715 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 716 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 717 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 718 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 719 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 720 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 721 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 722 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 723 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 724 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 725 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 726 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 727 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 728 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 729 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 730 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 731 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 732 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 733 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 734 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 735 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 736 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 737 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 738 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 739 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 740 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 741 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 742 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 743 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 744 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 745 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 746 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 747 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 748 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 749 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 750 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 751 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 752 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 753 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 754 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 755 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 756 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 757 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 758 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 759 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 760 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 761 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 762 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 763 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 764 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 765 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 766 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 767 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 768 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 769 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 770 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 771 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 772 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 773 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 774 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 775 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 776 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 777 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 778 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 779 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 780 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 781 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 782 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 783 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 784 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 785 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 786 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 787 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 788 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 789 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 790 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 791 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 792 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 793 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 794 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 795 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 796 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 797 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 798 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 799 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 800 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 801 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 802 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 803 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 804 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 805 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 806 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 807 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 808 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 809 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 810 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 811 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 812 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 813 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 814 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 815 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 816 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 817 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 818 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 819 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 820 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 821 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 822 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 823 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 824 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 825 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 826 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 827 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 828 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 829 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 830 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 831 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 832 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 833 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 834 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 835 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 836 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 837 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 838 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 839 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 840 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 841 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 842 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 843 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 844 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 845 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 846 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 847 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 848 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 849 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 850 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 851 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 852 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 853 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 854 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 855 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 856 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 857 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 858 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 859 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 860 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 861 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 862 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 863 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 864 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 865 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 866 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 867 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 868 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 869 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 870 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 871 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 872 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 873 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 874 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 875 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 876 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 877 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 878 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 879 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 880 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 881 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 882 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 883 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 884 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 885 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 886 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 887 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 888 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 889 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 890 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 891 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 892 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 893 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 894 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 895 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 896 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 897 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 898 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 899 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 900 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 901 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 902 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 903 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 904 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 905 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 906 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 907 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 908 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 909 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 910 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 911 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 912 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 913 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 914 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 915 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 916 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 917 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 918 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 919 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 920 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 921 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 922 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 923 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 924 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 925 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 926 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 927 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 928 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 929 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 930 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 931 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 932 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 933 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 934 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 935 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 936 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 937 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 938 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 939 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 940 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 941 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 942 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 943 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 944 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 945 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 946 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 947 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 948 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 949 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 950 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 951 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 952 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 953 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 954 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 955 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 956 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 957 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 958 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 959 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 960 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 961 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 962 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 963 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 964 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 965 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 966 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 967 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 968 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 969 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 970 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 971 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 972 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 973 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 974 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 975 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 976 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 977 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 978 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 979 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 980 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 981 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 982 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 983 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 984 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 985 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 986 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 987 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 988 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 989 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 990 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 991 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 992 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 993 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 994 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 995 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 996 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 997 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 998 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 999 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1000 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1001 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1002 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1003 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1004 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1005 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1006 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 1007 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1008 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1009 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1010 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1011 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1012 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1013 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1014 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1015 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1016 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1017 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1018 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1019 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1020 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1021 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1022 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1023 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1024 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1025 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1026 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1027 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1028 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1029 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1030 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1031 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1032 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1033 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1034 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1035 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1036 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1037 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1038 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1039 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1040 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1041 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1042 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1043 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1044 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1045 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1046 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1047 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1048 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1049 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1050 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1051 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1052 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1053 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1054 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1055 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1056 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1057 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1058 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1059 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1060 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1061 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1062 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1063 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1064 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1065 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1066 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1067 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1068 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1069 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1070 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1071 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1072 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1073 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1074 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1075 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1076 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1077 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1078 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1079 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1080 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1081 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1082 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1083 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1084 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1085 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1086 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1087 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1088 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1089 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1090 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1091 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1092 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1093 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1094 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1095 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1096 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1097 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1098 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1099 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1100 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1101 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1102 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1103 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1104 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1105 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1106 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1107 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1108 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1109 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1110 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1111 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1112 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1113 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1114 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1115 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1116 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1117 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1118 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1119 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1120 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1121 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1122 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1123 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1124 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1125 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1126 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1127 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1128 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1129 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1130 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1131 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1132 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1133 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1134 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1135 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1136 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1137 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1138 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1139 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1140 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1141 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1142 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1143 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1144 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1145 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1146 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1147 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1148 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1149 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1150 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1151 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1152 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1153 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1154 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1155 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1156 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1157 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1158 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1159 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1160 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1161 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1162 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1163 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1164 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1165 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1166 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1167 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1168 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1169 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1170 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1171 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1172 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1173 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1174 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1175 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1176 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1177 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1178 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1179 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1180 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1181 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1182 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1183 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1184 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1185 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1186 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1187 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1188 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1189 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1190 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1191 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1192 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1193 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1194 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1195 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1196 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1197 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1198 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1199 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1200 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1201 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1202 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1203 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1204 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1205 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1206 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1207 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1208 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1209 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1210 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1211 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1212 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1213 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1214 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1215 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1216 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1217 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1218 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1219 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1220 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1221 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1222 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1223 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1224 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1225 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1226 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1227 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1228 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1229 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1230 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1231 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1232 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1233 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1234 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1235 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1236 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1237 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1238 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1239 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1240 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1241 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1242 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1243 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1244 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1245 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1246 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1247 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1248 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1249 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1250 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1251 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1252 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1253 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1254 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1255 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1256 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1257 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1258 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1259 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1260 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1261 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1262 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1263 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1264 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1265 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1266 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1267 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1268 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1269 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1270 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1271 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1272 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1273 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1274 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1275 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1276 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1277 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1278 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1279 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1280 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1281 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1282 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1283 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1284 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1285 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1286 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1287 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1288 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1289 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1290 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1291 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1292 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1293 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1294 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1295 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1296 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1297 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1298 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1299 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1300 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1301 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1302 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1303 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1304 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1305 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1306 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1307 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1308 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1309 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1310 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1311 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1312 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1313 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1314 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1315 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1316 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1317 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1318 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1319 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1320 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1321 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1322 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1323 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1324 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1325 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1326 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1327 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1328 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1329 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1330 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1331 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1332 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1333 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1334 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1335 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1336 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1337 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1338 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1339 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1340 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1341 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1342 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1343 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1344 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1345 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1346 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1347 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1348 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1349 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1350 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1351 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1352 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1353 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1354 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1355 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1356 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1357 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1358 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1359 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1360 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1361 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1362 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1363 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1364 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1365 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1366 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1367 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1368 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1369 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1370 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1371 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1372 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1373 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1374 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1375 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1376 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1377 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1378 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1379 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1380 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1381 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1382 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1383 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1384 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1385 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1386 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1387 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1388 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1389 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1390 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1391 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1392 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1393 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1394 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1395 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1396 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1397 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1398 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1399 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1400 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1401 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1402 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1403 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1404 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1405 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1406 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1407 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1408 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1409 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1410 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1411 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1412 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1413 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1414 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1415 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1416 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1417 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1418 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1419 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1420 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1421 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1422 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1423 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1424 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1425 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1426 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1427 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1428 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1429 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1430 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1431 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1432 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1433 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1434 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1435 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1436 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1437 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1438 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1439 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1440 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1441 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1442 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1443 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1444 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1445 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1446 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1447 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1448 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1449 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1450 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1451 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1452 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1453 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1454 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1455 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1456 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1457 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1458 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1459 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1460 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1461 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1462 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1463 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1464 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1465 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1466 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1467 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1468 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1469 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1470 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1471 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1472 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1473 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1474 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1475 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1476 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1477 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1478 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1479 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1480 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1481 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1482 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1483 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1484 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1485 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1486 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1487 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1488 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1489 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1490 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1491 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1492 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1493 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1494 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1495 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1496 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1497 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1498 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1499 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1500 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1501 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1502 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1503 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1504 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1505 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1506 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1507 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1508 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1509 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1510 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1511 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1512 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1513 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1514 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1515 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1516 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1517 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1518 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1519 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1520 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1521 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1522 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1523 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1524 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1525 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1526 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1527 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1528 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1529 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1530 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1531 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1532 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1533 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1534 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1535 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1536 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1537 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1538 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1539 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1540 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1541 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1542 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1543 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1544 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1545 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1546 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1547 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1548 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1549 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1550 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1551 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1552 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1553 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1554 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1555 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1556 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1557 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1558 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1559 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1560 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1561 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1562 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1563 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1564 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1565 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1566 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1567 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1568 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1569 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1570 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1571 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1572 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1573 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1574 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1575 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1576 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1577 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1578 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1579 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1580 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1581 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1582 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1583 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1584 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1585 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1586 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1587 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1588 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1589 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1590 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1591 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1592 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1593 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1594 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1595 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1596 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1597 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1598 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1599 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1600 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1601 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1602 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1603 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1604 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1605 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1606 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1607 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1608 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1609 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1610 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1611 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1612 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1613 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1614 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1615 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1616 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1617 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1618 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1619 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1620 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1621 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1622 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1623 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1624 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1625 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1626 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1627 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1628 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1629 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1630 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1631 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1632 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1633 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1634 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1635 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1636 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1637 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1638 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1639 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1640 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1641 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1642 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1643 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1644 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1645 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1646 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1647 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1648 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1649 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1650 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1651 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1652 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1653 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1654 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1655 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1656 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1657 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1658 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1659 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1660 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1661 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1662 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1663 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1664 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1665 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1666 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1667 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1668 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1669 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1670 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1671 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1672 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1673 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1674 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1675 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1676 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1677 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1678 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1679 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1680 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1681 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1682 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1683 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1684 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1685 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1686 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1687 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1688 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1689 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1690 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1691 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1692 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1693 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1694 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1695 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1696 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1697 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1698 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1699 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1700 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1701 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1702 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1703 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1704 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1705 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1706 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1707 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1708 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1709 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1710 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1711 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1712 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1713 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1714 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1715 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1716 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1717 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1718 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1719 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1720 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1721 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1722 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1723 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1724 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1725 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1726 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1727 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1728 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1729 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1730 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1731 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1732 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1733 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1734 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1735 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1736 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1737 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1738 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1739 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1740 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1741 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1742 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1743 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1744 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1745 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1746 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1747 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1748 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1749 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1750 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1751 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1752 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1753 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1754 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1755 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1756 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1757 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1758 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1759 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1760 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1761 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1762 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1763 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1764 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1765 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1766 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1767 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1768 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1769 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1770 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1771 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1772 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1773 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1774 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1775 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1776 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1777 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1778 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1779 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1780 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1781 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1782 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1783 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1784 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1785 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1786 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1787 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1788 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1789 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1790 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1791 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1792 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1793 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1794 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1795 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1796 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1797 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1798 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1799 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1800 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1801 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1802 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1803 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1804 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1805 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1806 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1807 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1808 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1809 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1810 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1811 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1812 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1813 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1814 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1815 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1816 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1817 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1818 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1819 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1820 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1821 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1822 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1823 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1824 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1825 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1826 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1827 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1828 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1829 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1830 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1831 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1832 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1833 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1834 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1835 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1836 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1837 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1838 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1839 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1840 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1841 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1842 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1843 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1844 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1845 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1846 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1847 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1848 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1849 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1850 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1851 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1852 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1853 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1854 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1855 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1856 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1857 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1858 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1859 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1860 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1861 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1862 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1863 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1864 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1865 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1866 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1867 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1868 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1869 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1870 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1871 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1872 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1873 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1874 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1875 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1876 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1877 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1878 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1879 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1880 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1881 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1882 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1883 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1884 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1885 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1886 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1887 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1888 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1889 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1890 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1891 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1892 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1893 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1894 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1895 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1896 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1897 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1898 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1899 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1900 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1901 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1902 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1903 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1904 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1905 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1906 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1907 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1908 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1909 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1910 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1911 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1912 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1913 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1914 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1915 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1916 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1917 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1918 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1919 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1920 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1921 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1922 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1923 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1924 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1925 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1926 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1927 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1928 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1929 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1930 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1931 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1932 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1933 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1934 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1935 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1936 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1937 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1938 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1939 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1940 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1941 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1942 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1943 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1944 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1945 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1946 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1947 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1948 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1949 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1950 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1951 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1952 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1953 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1954 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1955 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1956 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1957 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1958 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1959 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1960 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1961 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1962 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1963 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1964 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1965 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1966 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1967 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1968 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1969 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1970 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1971 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1972 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1973 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1974 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1975 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1976 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1977 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1978 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1979 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1980 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1981 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1982 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1983 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1984 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1985 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1986 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1987 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1988 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1989 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1990 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1991 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1992 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1993 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1994 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1995 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1996 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1997 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1998 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 1999 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2000 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2001 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2002 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2003 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2004 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2005 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2006 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2007 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2008 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2009 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2010 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2011 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2012 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2013 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2014 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2015 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2016 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2017 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2018 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2019 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2020 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2021 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2022 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2023 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2024 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2025 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2026 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2027 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2028 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2029 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2030 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2031 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2032 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2033 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2034 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2035 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2036 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2037 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2038 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2039 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2040 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2041 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2042 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2043 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2044 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2045 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2046 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2047 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2048 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2049 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2050 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2051 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2052 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2053 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2054 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2055 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2056 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2057 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2058 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2059 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2060 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2061 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2062 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2063 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2064 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2065 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2066 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2067 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2068 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2069 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2070 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2071 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2072 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2073 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2074 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2075 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2076 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2077 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2078 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2079 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2080 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2081 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2082 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2083 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2084 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2085 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2086 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2087 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2088 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2089 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2090 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2091 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2092 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2093 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2094 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2095 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2096 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2097 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2098 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2099 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2100 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2101 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2102 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2103 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2104 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2105 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2106 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2107 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2108 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2109 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2110 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2111 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2112 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2113 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2114 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2115 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2116 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2117 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2118 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2119 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2120 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2121 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2122 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2123 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2124 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2125 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2126 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2127 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2128 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2129 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2130 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2131 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2132 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2133 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2134 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2135 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2136 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2137 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2138 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2139 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2140 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2141 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2142 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2143 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2144 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2145 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2146 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2147 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2148 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2149 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2150 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2151 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2152 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2153 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2154 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2155 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2156 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2157 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2158 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2159 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2160 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2161 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2162 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2163 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2164 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2165 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2166 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2167 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2168 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2169 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2170 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2171 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2172 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2173 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2174 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2175 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2176 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2177 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2178 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2179 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2180 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2181 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2182 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2183 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2184 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2185 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2186 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2187 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2188 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2189 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2190 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2191 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2192 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2193 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2194 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2195 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2196 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2197 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2198 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2199 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2200 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2201 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2202 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2203 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2204 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2205 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2206 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2207 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2208 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2209 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2210 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2211 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2212 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2213 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2214 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2215 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2216 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2217 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2218 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2219 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2220 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2221 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2222 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2223 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2224 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2225 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2226 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2227 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2228 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2229 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2230 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2231 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2232 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2233 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2234 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2235 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2236 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2237 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2238 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2239 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2240 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2241 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2242 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2243 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2244 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2245 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2246 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2247 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2248 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2249 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2250 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2251 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2252 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2253 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2254 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2255 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2256 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2257 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2258 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2259 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2260 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2261 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2262 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2263 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2264 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2265 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2266 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2267 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2268 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2269 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2270 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2271 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2272 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2273 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2274 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2275 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2276 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2277 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2278 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2279 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2280 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2281 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2282 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2283 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2284 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2285 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2286 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2287 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2288 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2289 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2290 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2291 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2292 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2293 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2294 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2295 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2296 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2297 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2298 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2299 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2300 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2301 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2302 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2303 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2304 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2305 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2306 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2307 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2308 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2309 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2310 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2311 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2312 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2313 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2314 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2315 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2316 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2317 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2318 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2319 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2320 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2321 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2322 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2323 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2324 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2325 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2326 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2327 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2328 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2329 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2330 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2331 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2332 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2333 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2334 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2335 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2336 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2337 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2338 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2339 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2340 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2341 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2342 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2343 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2344 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2345 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2346 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2347 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2348 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2349 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2350 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2351 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2352 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2353 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2354 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2355 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2356 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2357 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2358 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2359 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2360 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2361 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2362 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2363 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2364 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2365 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2366 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2367 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2368 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2369 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2370 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2371 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2372 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2373 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2374 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2375 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2376 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2377 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2378 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2379 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2380 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2381 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2382 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2383 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2384 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2385 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2386 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2387 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2388 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2389 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2390 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2391 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2392 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2393 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2394 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2395 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2396 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2397 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2398 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2399 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2400 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2401 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2402 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2403 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2404 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2405 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2406 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2407 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2408 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2409 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2410 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2411 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2412 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2413 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2414 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2415 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2416 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2417 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2418 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2419 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2420 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2421 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2422 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2423 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2424 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2425 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2426 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2427 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2428 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2429 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2430 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2431 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2432 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2433 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2434 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2435 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2436 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2437 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2438 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2439 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2440 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2441 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2442 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2443 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2444 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2445 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2446 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2447 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2448 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2449 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2450 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2451 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2452 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2453 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2454 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2455 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2456 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2457 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2458 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2459 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2460 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2461 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2462 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2463 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2464 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2465 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2466 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2467 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2468 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2469 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2470 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2471 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2472 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2473 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2474 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2475 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2476 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2477 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2478 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2479 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2480 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2481 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2482 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2483 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2484 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2485 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2486 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2487 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2488 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2489 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2490 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2491 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2492 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2493 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2494 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2495 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2496 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2497 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2498 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2499 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2500 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2501 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2502 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2503 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2504 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2505 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2506 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2507 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2508 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2509 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2510 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2511 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2512 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2513 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2514 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2515 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2516 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2517 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2518 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2519 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2520 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2521 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2522 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2523 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2524 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2525 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2526 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2527 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2528 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2529 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2530 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2531 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2532 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2533 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2534 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2535 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2536 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2537 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2538 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2539 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2540 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2541 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2542 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2543 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2544 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2545 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2546 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2547 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2548 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2549 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2550 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2551 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2552 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2553 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2554 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2555 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2556 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2557 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2558 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2559 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2560 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2561 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2562 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2563 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2564 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2565 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2566 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2567 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2568 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2569 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2570 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2571 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2572 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2573 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2574 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2575 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2576 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2577 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2578 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2579 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2580 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2581 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2582 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2583 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2584 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2585 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2586 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2587 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2588 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2589 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2590 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2591 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2592 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2593 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2594 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2595 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2596 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2597 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2598 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2599 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2600 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2601 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2602 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2603 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2604 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2605 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2606 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2607 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2608 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2609 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2610 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2611 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2612 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2613 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2614 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2615 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2616 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2617 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2618 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2619 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2620 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2621 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2622 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2623 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2624 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2625 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2626 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2627 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2628 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2629 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2630 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2631 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2632 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2633 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2634 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2635 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2636 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2637 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2638 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2639 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2640 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2641 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2642 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2643 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2644 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2645 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2646 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2647 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2648 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2649 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2650 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2651 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2652 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2653 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2654 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2655 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2656 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2657 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2658 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2659 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2660 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2661 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2662 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2663 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2664 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2665 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2666 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2667 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2668 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2669 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2670 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2671 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2672 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2673 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2674 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2675 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2676 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2677 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2678 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2679 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2680 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2681 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2682 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2683 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2684 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2685 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2686 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2687 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2688 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2689 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2690 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2691 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2692 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2693 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2694 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2695 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2696 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2697 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2698 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2699 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2700 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2701 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2702 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2703 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2704 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2705 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2706 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2707 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2708 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2709 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2710 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2711 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2712 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2713 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2714 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2715 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2716 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2717 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2718 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2719 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2720 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2721 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2722 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2723 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2724 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2725 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2726 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2727 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2728 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2729 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2730 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2731 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2732 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2733 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2734 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2735 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2736 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2737 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2738 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2739 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2740 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2741 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2742 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2743 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2744 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2745 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2746 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2747 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2748 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2749 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2750 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2751 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2752 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2753 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2754 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2755 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2756 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2757 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2758 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2759 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2760 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2761 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2762 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2763 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2764 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2765 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2766 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2767 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2768 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2769 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2770 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2771 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2772 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2773 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2774 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2775 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2776 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2777 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2778 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2779 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2780 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2781 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2782 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2783 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2784 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2785 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2786 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2787 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2788 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2789 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2790 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2791 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2792 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2793 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2794 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2795 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2796 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2797 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2798 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2799 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2800 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2801 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2802 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2803 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2804 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2805 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2806 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2807 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2808 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2809 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2810 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2811 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2812 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2813 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2814 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2815 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2816 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2817 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2818 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2819 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2820 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2821 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2822 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2823 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2824 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2825 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2826 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2827 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2828 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2829 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2830 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2831 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2832 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2833 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2834 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2835 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2836 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2837 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2838 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2839 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2840 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2841 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2842 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2843 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2844 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2845 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2846 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2847 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2848 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2849 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2850 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2851 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2852 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2853 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2854 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2855 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2856 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2857 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2858 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2859 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2860 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2861 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2862 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2863 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2864 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2865 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2866 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2867 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2868 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2869 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2870 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2871 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2872 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2873 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2874 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2875 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2876 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2877 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2878 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2879 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2880 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2881 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2882 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2883 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2884 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2885 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2886 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2887 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2888 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2889 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2890 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2891 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2892 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2893 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2894 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2895 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2896 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2897 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2898 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2899 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2900 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2901 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2902 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2903 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2904 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2905 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2906 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2907 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2908 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2909 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2910 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2911 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2912 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2913 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2914 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2915 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2916 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2917 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2918 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2919 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2920 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2921 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2922 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2923 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2924 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2925 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2926 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2927 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2928 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2929 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2930 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2931 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2932 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2933 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2934 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2935 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2936 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2937 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2938 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2939 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2940 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2941 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2942 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2943 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2944 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2945 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2946 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2947 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2948 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2949 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2950 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2951 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2952 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2953 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2954 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2955 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2956 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2957 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2958 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2959 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2960 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2961 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2962 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2963 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2964 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2965 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2966 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2967 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2968 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2969 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2970 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2971 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2972 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2973 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2974 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2975 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2976 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2977 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2978 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2979 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2980 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2981 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2982 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2983 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2984 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2985 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2986 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2987 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2988 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2989 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2990 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2991 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2992 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2993 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2994 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2995 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2996 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2997 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2998 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 2999 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3000 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3001 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3002 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3003 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3004 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3005 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3006 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3007 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3008 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3009 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3010 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3011 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3012 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3013 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3014 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3015 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3016 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3017 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3018 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3019 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3020 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3021 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3022 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3023 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3024 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3025 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3026 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3027 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3028 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3029 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3030 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3031 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3032 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3033 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3034 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3035 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3036 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3037 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3038 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3039 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3040 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3041 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3042 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3043 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3044 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3045 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3046 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3047 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3048 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3049 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3050 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3051 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3052 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3053 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3054 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3055 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3056 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3057 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3058 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3059 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3060 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3061 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3062 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3063 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3064 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3065 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3066 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3067 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3068 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3069 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3070 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3071 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3072 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3073 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3074 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3075 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3076 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3077 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3078 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3079 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3080 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3081 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3082 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3083 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3084 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3085 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3086 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3087 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3088 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3089 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3090 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3091 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3092 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3093 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3094 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3095 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3096 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3097 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3098 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3099 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3100 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3101 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3102 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3103 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3104 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3105 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3106 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3107 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3108 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3109 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3110 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3111 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3112 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3113 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3114 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3115 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3116 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3117 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3118 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3119 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3120 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3121 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3122 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3123 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3124 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3125 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3126 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3127 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3128 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3129 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3130 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3131 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3132 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3133 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3134 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3135 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3136 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3137 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3138 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3139 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3140 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3141 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3142 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3143 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3144 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3145 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3146 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3147 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3148 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3149 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3150 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3151 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3152 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3153 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3154 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3155 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3156 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3157 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3158 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3159 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3160 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3161 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3162 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3163 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3164 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3165 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3166 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3167 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3168 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3169 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3170 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3171 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3172 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3173 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3174 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3175 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3176 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3177 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3178 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3179 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3180 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3181 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3182 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3183 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3184 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3185 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3186 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3187 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3188 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3189 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3190 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3191 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3192 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3193 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3194 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3195 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3196 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3197 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3198 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3199 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3200 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3201 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3202 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3203 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3204 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3205 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3206 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3207 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3208 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3209 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3210 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3211 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3212 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3213 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3214 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3215 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3216 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3217 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3218 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3219 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3220 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3221 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3222 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3223 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3224 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3225 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3226 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3227 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3228 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3229 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3230 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3231 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3232 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3233 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3234 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3235 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3236 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3237 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3238 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3239 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3240 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3241 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3242 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3243 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3244 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3245 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3246 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3247 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3248 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3249 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3250 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3251 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3252 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3253 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3254 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3255 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3256 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3257 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3258 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3259 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3260 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3261 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3262 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3263 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3264 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3265 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3266 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3267 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3268 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3269 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3270 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3271 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3272 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3273 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3274 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3275 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3276 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3277 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3278 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3279 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3280 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3281 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3282 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3283 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3284 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3285 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3286 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3287 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3288 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3289 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3290 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3291 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3292 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3293 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3294 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3295 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3296 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3297 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3298 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3299 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3300 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3301 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3302 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3303 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3304 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3305 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3306 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3307 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3308 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3309 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3310 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3311 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3312 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3313 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3314 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3315 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3316 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3317 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3318 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3319 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3320 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3321 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3322 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3323 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3324 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3325 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3326 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3327 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3328 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3329 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3330 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3331 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3332 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3333 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3334 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3335 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3336 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3337 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3338 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3339 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3340 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3341 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3342 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3343 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3344 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3345 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3346 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3347 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3348 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3349 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3350 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3351 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3352 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3353 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3354 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3355 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3356 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3357 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3358 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3359 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3360 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3361 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3362 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3363 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3364 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3365 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3366 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3367 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3368 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3369 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3370 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3371 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3372 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3373 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3374 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3375 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3376 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3377 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3378 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3379 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3380 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3381 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3382 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3383 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3384 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3385 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3386 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3387 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3388 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3389 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3390 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3391 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3392 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3393 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3394 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3395 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3396 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3397 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3398 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3399 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3400 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3401 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3402 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3403 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3404 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3405 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3406 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3407 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3408 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3409 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3410 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3411 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3412 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3413 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3414 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3415 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3416 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3417 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3418 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3419 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3420 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3421 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3422 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3423 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3424 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3425 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3426 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3427 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3428 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3429 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3430 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3431 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3432 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3433 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3434 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3435 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3436 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3437 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3438 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3439 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3440 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3441 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3442 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3443 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3444 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3445 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3446 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3447 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3448 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3449 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3450 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3451 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3452 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3453 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3454 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3455 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3456 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3457 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3458 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3459 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3460 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3461 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3462 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3463 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3464 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3465 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3466 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3467 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3468 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3469 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3470 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3471 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3472 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3473 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3474 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3475 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3476 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3477 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3478 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3479 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3480 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3481 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3482 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3483 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3484 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3485 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3486 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3487 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3488 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3489 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3490 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3491 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3492 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3493 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3494 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3495 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3496 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3497 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3498 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3499 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3500 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3501 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3502 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3503 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3504 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3505 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3506 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3507 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3508 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3509 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3510 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3511 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3512 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3513 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3514 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3515 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3516 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3517 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3518 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3519 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3520 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3521 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3522 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3523 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3524 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3525 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3526 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3527 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3528 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3529 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3530 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3531 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3532 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3533 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3534 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3535 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3536 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3537 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3538 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3539 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3540 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3541 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3542 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3543 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3544 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3545 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3546 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3547 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3548 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3549 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3550 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3551 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3552 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3553 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3554 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3555 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3556 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3557 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3558 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3559 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3560 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3561 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3562 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3563 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3564 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3565 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3566 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3567 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3568 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3569 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3570 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3571 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3572 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3573 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3574 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3575 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3576 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3577 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3578 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3579 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3580 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3581 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3582 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3583 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3584 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3585 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3586 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3587 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3588 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3589 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3590 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3591 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3592 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3593 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3594 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3595 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3596 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3597 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3598 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3599 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3600 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3601 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3602 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3603 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3604 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3605 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3606 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3607 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3608 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3609 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3610 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3611 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3612 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3613 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3614 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3615 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3616 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3617 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3618 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3619 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3620 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3621 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3622 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3623 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3624 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3625 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3626 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3627 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3628 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3629 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3630 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3631 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3632 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3633 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3634 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3635 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3636 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3637 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3638 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3639 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3640 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3641 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3642 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3643 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3644 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3645 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3646 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3647 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3648 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3649 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3650 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3651 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3652 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3653 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3654 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3655 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3656 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3657 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3658 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3659 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3660 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3661 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3662 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3663 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3664 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3665 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3666 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3667 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3668 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3669 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3670 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3671 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3672 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3673 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3674 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3675 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3676 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3677 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3678 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3679 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3680 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3681 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3682 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3683 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3684 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3685 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3686 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3687 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3688 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3689 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3690 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3691 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3692 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3693 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3694 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3695 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3696 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3697 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3698 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3699 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3700 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3701 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3702 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3703 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3704 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3705 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3706 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3707 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3708 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3709 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3710 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3711 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3712 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3713 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3714 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3715 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3716 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3717 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3718 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3719 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3720 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3721 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3722 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3723 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3724 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3725 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3726 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3727 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3728 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3729 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3730 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3731 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3732 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3733 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3734 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3735 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3736 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3737 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3738 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3739 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3740 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3741 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3742 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3743 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3744 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3745 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3746 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3747 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3748 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3749 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3750 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3751 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3752 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3753 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3754 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3755 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3756 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3757 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3758 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3759 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3760 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3761 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3762 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3763 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3764 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3765 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3766 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3767 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3768 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3769 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3770 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3771 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3772 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3773 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3774 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3775 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3776 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3777 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3778 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3779 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3780 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3781 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3782 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3783 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3784 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3785 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3786 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3787 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3788 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3789 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3790 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3791 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3792 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3793 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3794 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3795 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3796 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3797 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3798 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3799 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3800 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3801 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3802 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3803 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3804 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3805 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3806 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3807 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3808 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3809 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3810 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3811 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3812 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3813 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3814 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3815 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3816 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3817 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3818 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3819 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3820 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3821 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3822 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3823 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3824 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3825 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3826 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3827 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3828 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3829 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3830 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3831 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3832 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3833 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3834 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3835 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3836 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3837 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3838 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3839 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3840 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3841 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3842 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3843 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3844 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3845 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3846 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3847 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3848 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3849 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3850 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3851 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3852 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3853 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3854 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3855 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3856 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3857 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3858 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3859 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3860 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3861 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3862 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3863 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3864 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3865 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3866 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3867 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3868 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3869 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3870 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3871 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3872 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3873 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3874 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3875 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3876 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3877 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3878 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3879 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3880 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3881 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3882 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3883 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3884 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3885 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3886 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3887 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3888 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3889 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3890 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3891 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3892 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3893 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3894 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3895 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3896 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3897 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3898 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3899 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3900 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3901 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3902 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3903 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3904 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3905 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3906 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3907 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3908 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3909 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3910 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3911 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3912 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3913 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3914 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3915 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3916 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3917 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3918 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3919 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3920 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3921 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3922 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3923 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3924 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3925 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3926 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3927 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3928 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3929 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3930 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3931 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3932 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3933 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3934 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3935 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3936 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3937 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3938 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3939 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3940 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3941 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3942 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3943 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3944 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3945 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3946 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3947 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3948 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3949 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3950 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3951 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3952 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3953 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3954 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3955 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3956 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3957 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3958 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3959 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3960 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3961 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3962 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3963 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3964 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3965 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3966 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3967 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3968 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3969 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3970 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3971 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3972 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3973 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3974 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3975 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3976 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3977 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3978 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3979 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3980 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3981 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3982 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3983 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3984 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3985 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3986 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3987 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3988 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3989 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3990 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3991 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3992 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3993 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3994 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3995 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3996 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3997 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3998 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 3999 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4000 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4001 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4002 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4003 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4004 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4005 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4006 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4007 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4008 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4009 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4010 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4011 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4012 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4013 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4014 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4015 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4016 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4017 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4018 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4019 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4020 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4021 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4022 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4023 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4024 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4025 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4026 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4027 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4028 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4029 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4030 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4031 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4032 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4033 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4034 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4035 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4036 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4037 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4038 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4039 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4040 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4041 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4042 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4043 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4044 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4045 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4046 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4047 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4048 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4049 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4050 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4051 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4052 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4053 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4054 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4055 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4056 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4057 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4058 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4059 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4060 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4061 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4062 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4063 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4064 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4065 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4066 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4067 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4068 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4069 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4070 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4071 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4072 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4073 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4074 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4075 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4076 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4077 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4078 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4079 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4080 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4081 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4082 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4083 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4084 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4085 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4086 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4087 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4088 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4089 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4090 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4091 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4092 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4093 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4094 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4095 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4096 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4097 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4098 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4099 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4100 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4101 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4102 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4103 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4104 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4105 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4106 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4107 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4108 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4109 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4110 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4111 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4112 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4113 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4114 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4115 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4116 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4117 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4118 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4119 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4120 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4121 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4122 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4123 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4124 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4125 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4126 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4127 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4128 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4129 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4130 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4131 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4132 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4133 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4134 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4135 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4136 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4137 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4138 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4139 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4140 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4141 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4142 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4143 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4144 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4145 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4146 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4147 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4148 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4149 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4150 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4151 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4152 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4153 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4154 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4155 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4156 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4157 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4158 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4159 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4160 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4161 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4162 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4163 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4164 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4165 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4166 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4167 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4168 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4169 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4170 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4171 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4172 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4173 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4174 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4175 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4176 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4177 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4178 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4179 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4180 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4181 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4182 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4183 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4184 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4185 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4186 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4187 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4188 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4189 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4190 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4191 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4192 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4193 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4194 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4195 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4196 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4197 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4198 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4199 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4200 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4201 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4202 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4203 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4204 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4205 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4206 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4207 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4208 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4209 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4210 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4211 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4212 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4213 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4214 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4215 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4216 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4217 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4218 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4219 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4220 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4221 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4222 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4223 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4224 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4225 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4226 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4227 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4228 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4229 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4230 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4231 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4232 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4233 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4234 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4235 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4236 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4237 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4238 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4239 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4240 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4241 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4242 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4243 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4244 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4245 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4246 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4247 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4248 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4249 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4250 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4251 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4252 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4253 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4254 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4255 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4256 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4257 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4258 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4259 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4260 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4261 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4262 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4263 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4264 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4265 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4266 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4267 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4268 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4269 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4270 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4271 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4272 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4273 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4274 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4275 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4276 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4277 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4278 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4279 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4280 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4281 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4282 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4283 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4284 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4285 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4286 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4287 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4288 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4289 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4290 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4291 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4292 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4293 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4294 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4295 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4296 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4297 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4298 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4299 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4300 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4301 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4302 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4303 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4304 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4305 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4306 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4307 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4308 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4309 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4310 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4311 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4312 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4313 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4314 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4315 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4316 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4317 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4318 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4319 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4320 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4321 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4322 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4323 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4324 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4325 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4326 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4327 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4328 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4329 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4330 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4331 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4332 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4333 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4334 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4335 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4336 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4337 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4338 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4339 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4340 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4341 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4342 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4343 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4344 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4345 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4346 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4347 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4348 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4349 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4350 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4351 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4352 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4353 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4354 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4355 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4356 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4357 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4358 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4359 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4360 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4361 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4362 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4363 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4364 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4365 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4366 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4367 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4368 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4369 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4370 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4371 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4372 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4373 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4374 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4375 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4376 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4377 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4378 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4379 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4380 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4381 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4382 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4383 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4384 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4385 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4386 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4387 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4388 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4389 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4390 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4391 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4392 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4393 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4394 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4395 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4396 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4397 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4398 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4399 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4400 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4401 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4402 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4403 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4404 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4405 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4406 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4407 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4408 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4409 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4410 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4411 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4412 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4413 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4414 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4415 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4416 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4417 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4418 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4419 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4420 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4421 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4422 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4423 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4424 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4425 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4426 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4427 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4428 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4429 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4430 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4431 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4432 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4433 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4434 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4435 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4436 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4437 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4438 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4439 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4440 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4441 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4442 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4443 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4444 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4445 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4446 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4447 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4448 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4449 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4450 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4451 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4452 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4453 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4454 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4455 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4456 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4457 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4458 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4459 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4460 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4461 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4462 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4463 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4464 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4465 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4466 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4467 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4468 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4469 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4470 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4471 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4472 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4473 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4474 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4475 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4476 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4477 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4478 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4479 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4480 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4481 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4482 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4483 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4484 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4485 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4486 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4487 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4488 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4489 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4490 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4491 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4492 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4493 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4494 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4495 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4496 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4497 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4498 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4499 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4500 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4501 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4502 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4503 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4504 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4505 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4506 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4507 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4508 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4509 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4510 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4511 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4512 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4513 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4514 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4515 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4516 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4517 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4518 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4519 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4520 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4521 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4522 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4523 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4524 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4525 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4526 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4527 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4528 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4529 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4530 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4531 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4532 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4533 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4534 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4535 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4536 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4537 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4538 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4539 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4540 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4541 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4542 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4543 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4544 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4545 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4546 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4547 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4548 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4549 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4550 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4551 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4552 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4553 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4554 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4555 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4556 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4557 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4558 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4559 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4560 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4561 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4562 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4563 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4564 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4565 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4566 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4567 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4568 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4569 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4570 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4571 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4572 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4573 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4574 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4575 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4576 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4577 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4578 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4579 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4580 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4581 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4582 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4583 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4584 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4585 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4586 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4587 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4588 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4589 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4590 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4591 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4592 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4593 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4594 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4595 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4596 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4597 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4598 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4599 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4600 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4601 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4602 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4603 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4604 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4605 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4606 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4607 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4608 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4609 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4610 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4611 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4612 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4613 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4614 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4615 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4616 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4617 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4618 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4619 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4620 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4621 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4622 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4623 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4624 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4625 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4626 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4627 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4628 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4629 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4630 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4631 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4632 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4633 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4634 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4635 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4636 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4637 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4638 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4639 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4640 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4641 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4642 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4643 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4644 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4645 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4646 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4647 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4648 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4649 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4650 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4651 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4652 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4653 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4654 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4655 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4656 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4657 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4658 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4659 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4660 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4661 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4662 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4663 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4664 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4665 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4666 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4667 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4668 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4669 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4670 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4671 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4672 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4673 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4674 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4675 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4676 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4677 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4678 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4679 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4680 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4681 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4682 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4683 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4684 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4685 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4686 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4687 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4688 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4689 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4690 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4691 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4692 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4693 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4694 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4695 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4696 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4697 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4698 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4699 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4700 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4701 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4702 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4703 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4704 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4705 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4706 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4707 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4708 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4709 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4710 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4711 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4712 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4713 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4714 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4715 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4716 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4717 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4718 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4719 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4720 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4721 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4722 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4723 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4724 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4725 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4726 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4727 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4728 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4729 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4730 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4731 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4732 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4733 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4734 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4735 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4736 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4737 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4738 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4739 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4740 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4741 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4742 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4743 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4744 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4745 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4746 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4747 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4748 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4749 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4750 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4751 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4752 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4753 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4754 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4755 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4756 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4757 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4758 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4759 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4760 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4761 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4762 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4763 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4764 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4765 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4766 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4767 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4768 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4769 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4770 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4771 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4772 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4773 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4774 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4775 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4776 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4777 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4778 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4779 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4780 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4781 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4782 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4783 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4784 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4785 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4786 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4787 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4788 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4789 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4790 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4791 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4792 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4793 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4794 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4795 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4796 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4797 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4798 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4799 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4800 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4801 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4802 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4803 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4804 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4805 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4806 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4807 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4808 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4809 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4810 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4811 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4812 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4813 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4814 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4815 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4816 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4817 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4818 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4819 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4820 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4821 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4822 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4823 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4824 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4825 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4826 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4827 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4828 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4829 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4830 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4831 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4832 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4833 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4834 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4835 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4836 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4837 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4838 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4839 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4840 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4841 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4842 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4843 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4844 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4845 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4846 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4847 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4848 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4849 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4850 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4851 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4852 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4853 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4854 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4855 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4856 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4857 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4858 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4859 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4860 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4861 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4862 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4863 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4864 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4865 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4866 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4867 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4868 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4869 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4870 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4871 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4872 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4873 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4874 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4875 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4876 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4877 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4878 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4879 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4880 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4881 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4882 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4883 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4884 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4885 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4886 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4887 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4888 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4889 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4890 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4891 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4892 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4893 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4894 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4895 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4896 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4897 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4898 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4899 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4900 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4901 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4902 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4903 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4904 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4905 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4906 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4907 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4908 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4909 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4910 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4911 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4912 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4913 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4914 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4915 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4916 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4917 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4918 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4919 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4920 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4921 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4922 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4923 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4924 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4925 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4926 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4927 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4928 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4929 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4930 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4931 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4932 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4933 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4934 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4935 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4936 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4937 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4938 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4939 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4940 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4941 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4942 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4943 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4944 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4945 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4946 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4947 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4948 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4949 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4950 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4951 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4952 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4953 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4954 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4955 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4956 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4957 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 4958 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4959 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4960 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4961 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4962 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4963 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4964 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4965 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4966 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4967 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4968 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4969 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4970 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4971 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4972 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4973 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4974 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4975 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4976 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4977 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4978 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4979 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4980 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4981 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4982 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4983 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4984 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4985 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4986 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4987 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4988 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4989 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4990 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4991 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4992 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4993 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4994 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4995 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4996 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4997 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4998 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 4999 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Iteration: 5000 Loss: Variable containing:\n",
      "nan\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEDCAYAAADOc0QpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f569fdd62d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = np.loadtxt('./data/20news-bydate/matlab/train.data')\n",
    "labels = np.loadtxt('./data/20news-bydate/matlab/train.label')\n",
    "labels -= 1\n",
    "data = restructure_data(data)\n",
    "data = standardize_data(data)\n",
    "Vocab_size = data.shape[1]\n",
    "\n",
    "train_loader,_= make_loaders(data,labels,1,batch_size=1)\n",
    "var_model = MLP_20(Vocab_size,20)\n",
    "optimizer = torch.optim.SGD(var_model.parameters(),lr=0.2,momentum=0.9)\n",
    "loss_func = nn.NLLLoss()\n",
    "model, losses = train_5000(var_model,train_loader,optimizer,loss_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1 Loss: 4.230446815490723\n",
      "Iteration: 2 Loss: nan\n",
      "Iteration: 3 Loss: nan\n",
      "Iteration: 4 Loss: nan\n",
      "Iteration: 5 Loss: nan\n",
      "Iteration: 6 Loss: nan\n",
      "Iteration: 7 Loss: nan\n",
      "Iteration: 8 Loss: nan\n",
      "Iteration: 9 Loss: nan\n",
      "Iteration: 10 Loss: nan\n",
      "Iteration: 11 Loss: nan\n",
      "Iteration: 12 Loss: nan\n",
      "Iteration: 13 Loss: nan\n",
      "Iteration: 14 Loss: nan\n",
      "Iteration: 15 Loss: nan\n",
      "Iteration: 16 Loss: nan\n",
      "Iteration: 17 Loss: nan\n",
      "Iteration: 18 Loss: nan\n",
      "Iteration: 19 Loss: nan\n",
      "Iteration: 20 Loss: nan\n",
      "Iteration: 21 Loss: nan\n",
      "Iteration: 22 Loss: nan\n",
      "Iteration: 23 Loss: nan\n",
      "Iteration: 24 Loss: nan\n",
      "Iteration: 25 Loss: nan\n",
      "Iteration: 26 Loss: nan\n",
      "Iteration: 27 Loss: nan\n",
      "Iteration: 28 Loss: nan\n",
      "Iteration: 29 Loss: nan\n",
      "Iteration: 30 Loss: nan\n",
      "Iteration: 31 Loss: nan\n",
      "Iteration: 32 Loss: nan\n",
      "Iteration: 33 Loss: nan\n",
      "Iteration: 34 Loss: nan\n",
      "Iteration: 35 Loss: nan\n",
      "Iteration: 36 Loss: nan\n",
      "Iteration: 37 Loss: nan\n",
      "Iteration: 38 Loss: nan\n",
      "Iteration: 39 Loss: nan\n",
      "Iteration: 40 Loss: nan\n",
      "Iteration: 41 Loss: nan\n",
      "Iteration: 42 Loss: nan\n",
      "Iteration: 43 Loss: nan\n",
      "Iteration: 44 Loss: nan\n",
      "Iteration: 45 Loss: nan\n",
      "Iteration: 46 Loss: nan\n",
      "Iteration: 47 Loss: nan\n",
      "Iteration: 48 Loss: nan\n",
      "Iteration: 49 Loss: nan\n",
      "Iteration: 50 Loss: nan\n",
      "Iteration: 51 Loss: nan\n",
      "Iteration: 52 Loss: nan\n",
      "Iteration: 53 Loss: nan\n",
      "Iteration: 54 Loss: nan\n",
      "Iteration: 55 Loss: nan\n",
      "Iteration: 56 Loss: nan\n",
      "Iteration: 57 Loss: nan\n",
      "Iteration: 58 Loss: nan\n",
      "Iteration: 59 Loss: nan\n",
      "Iteration: 60 Loss: nan\n",
      "Iteration: 61 Loss: nan\n",
      "Iteration: 62 Loss: nan\n",
      "Iteration: 63 Loss: nan\n",
      "Iteration: 64 Loss: nan\n",
      "Iteration: 65 Loss: nan\n",
      "Iteration: 66 Loss: nan\n",
      "Iteration: 67 Loss: nan\n",
      "Iteration: 68 Loss: nan\n",
      "Iteration: 69 Loss: nan\n",
      "Iteration: 70 Loss: nan\n",
      "Iteration: 71 Loss: nan\n",
      "Iteration: 72 Loss: nan\n",
      "Iteration: 73 Loss: nan\n",
      "Iteration: 74 Loss: nan\n",
      "Iteration: 75 Loss: nan\n",
      "Iteration: 76 Loss: nan\n",
      "Iteration: 77 Loss: nan\n",
      "Iteration: 78 Loss: nan\n",
      "Iteration: 79 Loss: nan\n",
      "Iteration: 80 Loss: nan\n",
      "Iteration: 81 Loss: nan\n",
      "Iteration: 82 Loss: nan\n",
      "Iteration: 83 Loss: nan\n",
      "Iteration: 84 Loss: nan\n",
      "Iteration: 85 Loss: nan\n",
      "Iteration: 86 Loss: nan\n",
      "Iteration: 87 Loss: nan\n",
      "Iteration: 88 Loss: nan\n",
      "Iteration: 89 Loss: nan\n",
      "Iteration: 90 Loss: nan\n",
      "Iteration: 91 Loss: nan\n",
      "Iteration: 92 Loss: nan\n",
      "Iteration: 93 Loss: nan\n",
      "Iteration: 94 Loss: nan\n",
      "Iteration: 95 Loss: nan\n",
      "Iteration: 96 Loss: nan\n",
      "Iteration: 97 Loss: nan\n",
      "Iteration: 98 Loss: nan\n",
      "Iteration: 99 Loss: nan\n",
      "Iteration: 100 Loss: nan\n",
      "Iteration: 101 Loss: nan\n",
      "Iteration: 102 Loss: nan\n",
      "Iteration: 103 Loss: nan\n",
      "Iteration: 104 Loss: nan\n",
      "Iteration: 105 Loss: nan\n",
      "Iteration: 106 Loss: nan\n",
      "Iteration: 107 Loss: nan\n",
      "Iteration: 108 Loss: nan\n",
      "Iteration: 109 Loss: nan\n",
      "Iteration: 110 Loss: nan\n",
      "Iteration: 111 Loss: nan\n",
      "Iteration: 112 Loss: nan\n",
      "Iteration: 113 Loss: nan\n",
      "Iteration: 114 Loss: nan\n",
      "Iteration: 115 Loss: nan\n",
      "Iteration: 116 Loss: nan\n",
      "Iteration: 117 Loss: nan\n",
      "Iteration: 118 Loss: nan\n",
      "Iteration: 119 Loss: nan\n",
      "Iteration: 120 Loss: nan\n",
      "Iteration: 121 Loss: nan\n",
      "Iteration: 122 Loss: nan\n",
      "Iteration: 123 Loss: nan\n",
      "Iteration: 124 Loss: nan\n",
      "Iteration: 125 Loss: nan\n",
      "Iteration: 126 Loss: nan\n",
      "Iteration: 127 Loss: nan\n",
      "Iteration: 128 Loss: nan\n",
      "Iteration: 129 Loss: nan\n",
      "Iteration: 130 Loss: nan\n",
      "Iteration: 131 Loss: nan\n",
      "Iteration: 132 Loss: nan\n",
      "Iteration: 133 Loss: nan\n",
      "Iteration: 134 Loss: nan\n",
      "Iteration: 135 Loss: nan\n",
      "Iteration: 136 Loss: nan\n",
      "Iteration: 137 Loss: nan\n",
      "Iteration: 138 Loss: nan\n",
      "Iteration: 139 Loss: nan\n",
      "Iteration: 140 Loss: nan\n",
      "Iteration: 141 Loss: nan\n",
      "Iteration: 142 Loss: nan\n",
      "Iteration: 143 Loss: nan\n",
      "Iteration: 144 Loss: nan\n",
      "Iteration: 145 Loss: nan\n",
      "Iteration: 146 Loss: nan\n",
      "Iteration: 147 Loss: nan\n",
      "Iteration: 148 Loss: nan\n",
      "Iteration: 149 Loss: nan\n",
      "Iteration: 150 Loss: nan\n",
      "Iteration: 151 Loss: nan\n",
      "Iteration: 152 Loss: nan\n",
      "Iteration: 153 Loss: nan\n",
      "Iteration: 154 Loss: nan\n",
      "Iteration: 155 Loss: nan\n",
      "Iteration: 156 Loss: nan\n",
      "Iteration: 157 Loss: nan\n",
      "Iteration: 158 Loss: nan\n",
      "Iteration: 159 Loss: nan\n",
      "Iteration: 160 Loss: nan\n",
      "Iteration: 161 Loss: nan\n",
      "Iteration: 162 Loss: nan\n",
      "Iteration: 163 Loss: nan\n",
      "Iteration: 164 Loss: nan\n",
      "Iteration: 165 Loss: nan\n",
      "Iteration: 166 Loss: nan\n",
      "Iteration: 167 Loss: nan\n",
      "Iteration: 168 Loss: nan\n",
      "Iteration: 169 Loss: nan\n",
      "Iteration: 170 Loss: nan\n",
      "Iteration: 171 Loss: nan\n",
      "Iteration: 172 Loss: nan\n",
      "Iteration: 173 Loss: nan\n",
      "Iteration: 174 Loss: nan\n",
      "Iteration: 175 Loss: nan\n",
      "Iteration: 176 Loss: nan\n",
      "Iteration: 177 Loss: nan\n",
      "Iteration: 178 Loss: nan\n",
      "Iteration: 179 Loss: nan\n",
      "Iteration: 180 Loss: nan\n",
      "Iteration: 181 Loss: nan\n",
      "Iteration: 182 Loss: nan\n",
      "Iteration: 183 Loss: nan\n",
      "Iteration: 184 Loss: nan\n",
      "Iteration: 185 Loss: nan\n",
      "Iteration: 186 Loss: nan\n",
      "Iteration: 187 Loss: nan\n",
      "Iteration: 188 Loss: nan\n",
      "Iteration: 189 Loss: nan\n",
      "Iteration: 190 Loss: nan\n",
      "Iteration: 191 Loss: nan\n",
      "Iteration: 192 Loss: nan\n",
      "Iteration: 193 Loss: nan\n",
      "Iteration: 194 Loss: nan\n",
      "Iteration: 195 Loss: nan\n",
      "Iteration: 196 Loss: nan\n",
      "Iteration: 197 Loss: nan\n",
      "Iteration: 198 Loss: nan\n",
      "Iteration: 199 Loss: nan\n",
      "Iteration: 200 Loss: nan\n",
      "Iteration: 201 Loss: nan\n",
      "Iteration: 202 Loss: nan\n",
      "Iteration: 203 Loss: nan\n",
      "Iteration: 204 Loss: nan\n",
      "Iteration: 205 Loss: nan\n",
      "Iteration: 206 Loss: nan\n",
      "Iteration: 207 Loss: nan\n",
      "Iteration: 208 Loss: nan\n",
      "Iteration: 209 Loss: nan\n",
      "Iteration: 210 Loss: nan\n",
      "Iteration: 211 Loss: nan\n",
      "Iteration: 212 Loss: nan\n",
      "Iteration: 213 Loss: nan\n",
      "Iteration: 214 Loss: nan\n",
      "Iteration: 215 Loss: nan\n",
      "Iteration: 216 Loss: nan\n",
      "Iteration: 217 Loss: nan\n",
      "Iteration: 218 Loss: nan\n",
      "Iteration: 219 Loss: nan\n",
      "Iteration: 220 Loss: nan\n",
      "Iteration: 221 Loss: nan\n",
      "Iteration: 222 Loss: nan\n",
      "Iteration: 223 Loss: nan\n",
      "Iteration: 224 Loss: nan\n",
      "Iteration: 225 Loss: nan\n",
      "Iteration: 226 Loss: nan\n",
      "Iteration: 227 Loss: nan\n",
      "Iteration: 228 Loss: nan\n",
      "Iteration: 229 Loss: nan\n",
      "Iteration: 230 Loss: nan\n",
      "Iteration: 231 Loss: nan\n",
      "Iteration: 232 Loss: nan\n",
      "Iteration: 233 Loss: nan\n",
      "Iteration: 234 Loss: nan\n",
      "Iteration: 235 Loss: nan\n",
      "Iteration: 236 Loss: nan\n",
      "Iteration: 237 Loss: nan\n",
      "Iteration: 238 Loss: nan\n",
      "Iteration: 239 Loss: nan\n",
      "Iteration: 240 Loss: nan\n",
      "Iteration: 241 Loss: nan\n",
      "Iteration: 242 Loss: nan\n",
      "Iteration: 243 Loss: nan\n",
      "Iteration: 244 Loss: nan\n",
      "Iteration: 245 Loss: nan\n",
      "Iteration: 246 Loss: nan\n",
      "Iteration: 247 Loss: nan\n",
      "Iteration: 248 Loss: nan\n",
      "Iteration: 249 Loss: nan\n",
      "Iteration: 250 Loss: nan\n",
      "Iteration: 251 Loss: nan\n",
      "Iteration: 252 Loss: nan\n",
      "Iteration: 253 Loss: nan\n",
      "Iteration: 254 Loss: nan\n",
      "Iteration: 255 Loss: nan\n",
      "Iteration: 256 Loss: nan\n",
      "Iteration: 257 Loss: nan\n",
      "Iteration: 258 Loss: nan\n",
      "Iteration: 259 Loss: nan\n",
      "Iteration: 260 Loss: nan\n",
      "Iteration: 261 Loss: nan\n",
      "Iteration: 262 Loss: nan\n",
      "Iteration: 263 Loss: nan\n",
      "Iteration: 264 Loss: nan\n",
      "Iteration: 265 Loss: nan\n",
      "Iteration: 266 Loss: nan\n",
      "Iteration: 267 Loss: nan\n",
      "Iteration: 268 Loss: nan\n",
      "Iteration: 269 Loss: nan\n",
      "Iteration: 270 Loss: nan\n",
      "Iteration: 271 Loss: nan\n",
      "Iteration: 272 Loss: nan\n",
      "Iteration: 273 Loss: nan\n",
      "Iteration: 274 Loss: nan\n",
      "Iteration: 275 Loss: nan\n",
      "Iteration: 276 Loss: nan\n",
      "Iteration: 277 Loss: nan\n",
      "Iteration: 278 Loss: nan\n",
      "Iteration: 279 Loss: nan\n",
      "Iteration: 280 Loss: nan\n",
      "Iteration: 281 Loss: nan\n",
      "Iteration: 282 Loss: nan\n",
      "Iteration: 283 Loss: nan\n",
      "Iteration: 284 Loss: nan\n",
      "Iteration: 285 Loss: nan\n",
      "Iteration: 286 Loss: nan\n",
      "Iteration: 287 Loss: nan\n",
      "Iteration: 288 Loss: nan\n",
      "Iteration: 289 Loss: nan\n",
      "Iteration: 290 Loss: nan\n",
      "Iteration: 291 Loss: nan\n",
      "Iteration: 292 Loss: nan\n",
      "Iteration: 293 Loss: nan\n",
      "Iteration: 294 Loss: nan\n",
      "Iteration: 295 Loss: nan\n",
      "Iteration: 296 Loss: nan\n",
      "Iteration: 297 Loss: nan\n",
      "Iteration: 298 Loss: nan\n",
      "Iteration: 299 Loss: nan\n",
      "Iteration: 300 Loss: nan\n",
      "Iteration: 301 Loss: nan\n",
      "Iteration: 302 Loss: nan\n",
      "Iteration: 303 Loss: nan\n",
      "Iteration: 304 Loss: nan\n",
      "Iteration: 305 Loss: nan\n",
      "Iteration: 306 Loss: nan\n",
      "Iteration: 307 Loss: nan\n",
      "Iteration: 308 Loss: nan\n",
      "Iteration: 309 Loss: nan\n",
      "Iteration: 310 Loss: nan\n",
      "Iteration: 311 Loss: nan\n",
      "Iteration: 312 Loss: nan\n",
      "Iteration: 313 Loss: nan\n",
      "Iteration: 314 Loss: nan\n",
      "Iteration: 315 Loss: nan\n",
      "Iteration: 316 Loss: nan\n",
      "Iteration: 317 Loss: nan\n",
      "Iteration: 318 Loss: nan\n",
      "Iteration: 319 Loss: nan\n",
      "Iteration: 320 Loss: nan\n",
      "Iteration: 321 Loss: nan\n",
      "Iteration: 322 Loss: nan\n",
      "Iteration: 323 Loss: nan\n",
      "Iteration: 324 Loss: nan\n",
      "Iteration: 325 Loss: nan\n",
      "Iteration: 326 Loss: nan\n",
      "Iteration: 327 Loss: nan\n",
      "Iteration: 328 Loss: nan\n",
      "Iteration: 329 Loss: nan\n",
      "Iteration: 330 Loss: nan\n",
      "Iteration: 331 Loss: nan\n",
      "Iteration: 332 Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 333 Loss: nan\n",
      "Iteration: 334 Loss: nan\n",
      "Iteration: 335 Loss: nan\n",
      "Iteration: 336 Loss: nan\n",
      "Iteration: 337 Loss: nan\n",
      "Iteration: 338 Loss: nan\n",
      "Iteration: 339 Loss: nan\n",
      "Iteration: 340 Loss: nan\n",
      "Iteration: 341 Loss: nan\n",
      "Iteration: 342 Loss: nan\n",
      "Iteration: 343 Loss: nan\n",
      "Iteration: 344 Loss: nan\n",
      "Iteration: 345 Loss: nan\n",
      "Iteration: 346 Loss: nan\n",
      "Iteration: 347 Loss: nan\n",
      "Iteration: 348 Loss: nan\n",
      "Iteration: 349 Loss: nan\n",
      "Iteration: 350 Loss: nan\n",
      "Iteration: 351 Loss: nan\n",
      "Iteration: 352 Loss: nan\n",
      "Iteration: 353 Loss: nan\n",
      "Iteration: 354 Loss: nan\n",
      "Iteration: 355 Loss: nan\n",
      "Iteration: 356 Loss: nan\n",
      "Iteration: 357 Loss: nan\n",
      "Iteration: 358 Loss: nan\n",
      "Iteration: 359 Loss: nan\n",
      "Iteration: 360 Loss: nan\n",
      "Iteration: 361 Loss: nan\n",
      "Iteration: 362 Loss: nan\n",
      "Iteration: 363 Loss: nan\n",
      "Iteration: 364 Loss: nan\n",
      "Iteration: 365 Loss: nan\n",
      "Iteration: 366 Loss: nan\n",
      "Iteration: 367 Loss: nan\n",
      "Iteration: 368 Loss: nan\n",
      "Iteration: 369 Loss: nan\n",
      "Iteration: 370 Loss: nan\n",
      "Iteration: 371 Loss: nan\n",
      "Iteration: 372 Loss: nan\n",
      "Iteration: 373 Loss: nan\n",
      "Iteration: 374 Loss: nan\n",
      "Iteration: 375 Loss: nan\n",
      "Iteration: 376 Loss: nan\n",
      "Iteration: 377 Loss: nan\n",
      "Iteration: 378 Loss: nan\n",
      "Iteration: 379 Loss: nan\n",
      "Iteration: 380 Loss: nan\n",
      "Iteration: 381 Loss: nan\n",
      "Iteration: 382 Loss: nan\n",
      "Iteration: 383 Loss: nan\n",
      "Iteration: 384 Loss: nan\n",
      "Iteration: 385 Loss: nan\n",
      "Iteration: 386 Loss: nan\n",
      "Iteration: 387 Loss: nan\n",
      "Iteration: 388 Loss: nan\n",
      "Iteration: 389 Loss: nan\n",
      "Iteration: 390 Loss: nan\n",
      "Iteration: 391 Loss: nan\n",
      "Iteration: 392 Loss: nan\n",
      "Iteration: 393 Loss: nan\n",
      "Iteration: 394 Loss: nan\n",
      "Iteration: 395 Loss: nan\n",
      "Iteration: 396 Loss: nan\n",
      "Iteration: 397 Loss: nan\n",
      "Iteration: 398 Loss: nan\n",
      "Iteration: 399 Loss: nan\n",
      "Iteration: 400 Loss: nan\n",
      "Iteration: 401 Loss: nan\n",
      "Iteration: 402 Loss: nan\n",
      "Iteration: 403 Loss: nan\n",
      "Iteration: 404 Loss: nan\n",
      "Iteration: 405 Loss: nan\n",
      "Iteration: 406 Loss: nan\n",
      "Iteration: 407 Loss: nan\n",
      "Iteration: 408 Loss: nan\n",
      "Iteration: 409 Loss: nan\n",
      "Iteration: 410 Loss: nan\n",
      "Iteration: 411 Loss: nan\n",
      "Iteration: 412 Loss: nan\n",
      "Iteration: 413 Loss: nan\n",
      "Iteration: 414 Loss: nan\n",
      "Iteration: 415 Loss: nan\n",
      "Iteration: 416 Loss: nan\n",
      "Iteration: 417 Loss: nan\n",
      "Iteration: 418 Loss: nan\n",
      "Iteration: 419 Loss: nan\n",
      "Iteration: 420 Loss: nan\n",
      "Iteration: 421 Loss: nan\n",
      "Iteration: 422 Loss: nan\n",
      "Iteration: 423 Loss: nan\n",
      "Iteration: 424 Loss: nan\n",
      "Iteration: 425 Loss: nan\n",
      "Iteration: 426 Loss: nan\n",
      "Iteration: 427 Loss: nan\n",
      "Iteration: 428 Loss: nan\n",
      "Iteration: 429 Loss: nan\n",
      "Iteration: 430 Loss: nan\n",
      "Iteration: 431 Loss: nan\n",
      "Iteration: 432 Loss: nan\n",
      "Iteration: 433 Loss: nan\n",
      "Iteration: 434 Loss: nan\n",
      "Iteration: 435 Loss: nan\n",
      "Iteration: 436 Loss: nan\n",
      "Iteration: 437 Loss: nan\n",
      "Iteration: 438 Loss: nan\n",
      "Iteration: 439 Loss: nan\n",
      "Iteration: 440 Loss: nan\n",
      "Iteration: 441 Loss: nan\n",
      "Iteration: 442 Loss: nan\n",
      "Iteration: 443 Loss: nan\n",
      "Iteration: 444 Loss: nan\n",
      "Iteration: 445 Loss: nan\n",
      "Iteration: 446 Loss: nan\n",
      "Iteration: 447 Loss: nan\n",
      "Iteration: 448 Loss: nan\n",
      "Iteration: 449 Loss: nan\n",
      "Iteration: 450 Loss: nan\n",
      "Iteration: 451 Loss: nan\n",
      "Iteration: 452 Loss: nan\n",
      "Iteration: 453 Loss: nan\n",
      "Iteration: 454 Loss: nan\n",
      "Iteration: 455 Loss: nan\n",
      "Iteration: 456 Loss: nan\n",
      "Iteration: 457 Loss: nan\n",
      "Iteration: 458 Loss: nan\n",
      "Iteration: 459 Loss: nan\n",
      "Iteration: 460 Loss: nan\n",
      "Iteration: 461 Loss: nan\n",
      "Iteration: 462 Loss: nan\n",
      "Iteration: 463 Loss: nan\n",
      "Iteration: 464 Loss: nan\n",
      "Iteration: 465 Loss: nan\n",
      "Iteration: 466 Loss: nan\n",
      "Iteration: 467 Loss: nan\n",
      "Iteration: 468 Loss: nan\n",
      "Iteration: 469 Loss: nan\n",
      "Iteration: 470 Loss: nan\n",
      "Iteration: 471 Loss: nan\n",
      "Iteration: 472 Loss: nan\n",
      "Iteration: 473 Loss: nan\n",
      "Iteration: 474 Loss: nan\n",
      "Iteration: 475 Loss: nan\n",
      "Iteration: 476 Loss: nan\n",
      "Iteration: 477 Loss: nan\n",
      "Iteration: 478 Loss: nan\n",
      "Iteration: 479 Loss: nan\n",
      "Iteration: 480 Loss: nan\n",
      "Iteration: 481 Loss: nan\n",
      "Iteration: 482 Loss: nan\n",
      "Iteration: 483 Loss: nan\n",
      "Iteration: 484 Loss: nan\n",
      "Iteration: 485 Loss: nan\n",
      "Iteration: 486 Loss: nan\n",
      "Iteration: 487 Loss: nan\n",
      "Iteration: 488 Loss: nan\n",
      "Iteration: 489 Loss: nan\n",
      "Iteration: 490 Loss: nan\n",
      "Iteration: 491 Loss: nan\n",
      "Iteration: 492 Loss: nan\n",
      "Iteration: 493 Loss: nan\n",
      "Iteration: 494 Loss: nan\n",
      "Iteration: 495 Loss: nan\n",
      "Iteration: 496 Loss: nan\n",
      "Iteration: 497 Loss: nan\n",
      "Iteration: 498 Loss: nan\n",
      "Iteration: 499 Loss: nan\n",
      "Iteration: 500 Loss: nan\n",
      "Iteration: 501 Loss: nan\n",
      "Iteration: 502 Loss: nan\n",
      "Iteration: 503 Loss: nan\n",
      "Iteration: 504 Loss: nan\n",
      "Iteration: 505 Loss: nan\n",
      "Iteration: 506 Loss: nan\n",
      "Iteration: 507 Loss: nan\n",
      "Iteration: 508 Loss: nan\n",
      "Iteration: 509 Loss: nan\n",
      "Iteration: 510 Loss: nan\n",
      "Iteration: 511 Loss: nan\n",
      "Iteration: 512 Loss: nan\n",
      "Iteration: 513 Loss: nan\n",
      "Iteration: 514 Loss: nan\n",
      "Iteration: 515 Loss: nan\n",
      "Iteration: 516 Loss: nan\n",
      "Iteration: 517 Loss: nan\n",
      "Iteration: 518 Loss: nan\n",
      "Iteration: 519 Loss: nan\n",
      "Iteration: 520 Loss: nan\n",
      "Iteration: 521 Loss: nan\n",
      "Iteration: 522 Loss: nan\n",
      "Iteration: 523 Loss: nan\n",
      "Iteration: 524 Loss: nan\n",
      "Iteration: 525 Loss: nan\n",
      "Iteration: 526 Loss: nan\n",
      "Iteration: 527 Loss: nan\n",
      "Iteration: 528 Loss: nan\n",
      "Iteration: 529 Loss: nan\n",
      "Iteration: 530 Loss: nan\n",
      "Iteration: 531 Loss: nan\n",
      "Iteration: 532 Loss: nan\n",
      "Iteration: 533 Loss: nan\n",
      "Iteration: 534 Loss: nan\n",
      "Iteration: 535 Loss: nan\n",
      "Iteration: 536 Loss: nan\n",
      "Iteration: 537 Loss: nan\n",
      "Iteration: 538 Loss: nan\n",
      "Iteration: 539 Loss: nan\n",
      "Iteration: 540 Loss: nan\n",
      "Iteration: 541 Loss: nan\n",
      "Iteration: 542 Loss: nan\n",
      "Iteration: 543 Loss: nan\n",
      "Iteration: 544 Loss: nan\n",
      "Iteration: 545 Loss: nan\n",
      "Iteration: 546 Loss: nan\n",
      "Iteration: 547 Loss: nan\n",
      "Iteration: 548 Loss: nan\n",
      "Iteration: 549 Loss: nan\n",
      "Iteration: 550 Loss: nan\n",
      "Iteration: 551 Loss: nan\n",
      "Iteration: 552 Loss: nan\n",
      "Iteration: 553 Loss: nan\n",
      "Iteration: 554 Loss: nan\n",
      "Iteration: 555 Loss: nan\n",
      "Iteration: 556 Loss: nan\n",
      "Iteration: 557 Loss: nan\n",
      "Iteration: 558 Loss: nan\n",
      "Iteration: 559 Loss: nan\n",
      "Iteration: 560 Loss: nan\n",
      "Iteration: 561 Loss: nan\n",
      "Iteration: 562 Loss: nan\n",
      "Iteration: 563 Loss: nan\n",
      "Iteration: 564 Loss: nan\n",
      "Iteration: 565 Loss: nan\n",
      "Iteration: 566 Loss: nan\n",
      "Iteration: 567 Loss: nan\n",
      "Iteration: 568 Loss: nan\n",
      "Iteration: 569 Loss: nan\n",
      "Iteration: 570 Loss: nan\n",
      "Iteration: 571 Loss: nan\n",
      "Iteration: 572 Loss: nan\n",
      "Iteration: 573 Loss: nan\n",
      "Iteration: 574 Loss: nan\n",
      "Iteration: 575 Loss: nan\n",
      "Iteration: 576 Loss: nan\n",
      "Iteration: 577 Loss: nan\n",
      "Iteration: 578 Loss: nan\n",
      "Iteration: 579 Loss: nan\n",
      "Iteration: 580 Loss: nan\n",
      "Iteration: 581 Loss: nan\n",
      "Iteration: 582 Loss: nan\n",
      "Iteration: 583 Loss: nan\n",
      "Iteration: 584 Loss: nan\n",
      "Iteration: 585 Loss: nan\n",
      "Iteration: 586 Loss: nan\n",
      "Iteration: 587 Loss: nan\n",
      "Iteration: 588 Loss: nan\n",
      "Iteration: 589 Loss: nan\n",
      "Iteration: 590 Loss: nan\n",
      "Iteration: 591 Loss: nan\n",
      "Iteration: 592 Loss: nan\n",
      "Iteration: 593 Loss: nan\n",
      "Iteration: 594 Loss: nan\n",
      "Iteration: 595 Loss: nan\n",
      "Iteration: 596 Loss: nan\n",
      "Iteration: 597 Loss: nan\n",
      "Iteration: 598 Loss: nan\n",
      "Iteration: 599 Loss: nan\n",
      "Iteration: 600 Loss: nan\n",
      "Iteration: 601 Loss: nan\n",
      "Iteration: 602 Loss: nan\n",
      "Iteration: 603 Loss: nan\n",
      "Iteration: 604 Loss: nan\n",
      "Iteration: 605 Loss: nan\n",
      "Iteration: 606 Loss: nan\n",
      "Iteration: 607 Loss: nan\n",
      "Iteration: 608 Loss: nan\n",
      "Iteration: 609 Loss: nan\n",
      "Iteration: 610 Loss: nan\n",
      "Iteration: 611 Loss: nan\n",
      "Iteration: 612 Loss: nan\n",
      "Iteration: 613 Loss: nan\n",
      "Iteration: 614 Loss: nan\n",
      "Iteration: 615 Loss: nan\n",
      "Iteration: 616 Loss: nan\n",
      "Iteration: 617 Loss: nan\n",
      "Iteration: 618 Loss: nan\n",
      "Iteration: 619 Loss: nan\n",
      "Iteration: 620 Loss: nan\n",
      "Iteration: 621 Loss: nan\n",
      "Iteration: 622 Loss: nan\n",
      "Iteration: 623 Loss: nan\n",
      "Iteration: 624 Loss: nan\n",
      "Iteration: 625 Loss: nan\n",
      "Iteration: 626 Loss: nan\n",
      "Iteration: 627 Loss: nan\n",
      "Iteration: 628 Loss: nan\n",
      "Iteration: 629 Loss: nan\n",
      "Iteration: 630 Loss: nan\n",
      "Iteration: 631 Loss: nan\n",
      "Iteration: 632 Loss: nan\n",
      "Iteration: 633 Loss: nan\n",
      "Iteration: 634 Loss: nan\n",
      "Iteration: 635 Loss: nan\n",
      "Iteration: 636 Loss: nan\n",
      "Iteration: 637 Loss: nan\n",
      "Iteration: 638 Loss: nan\n",
      "Iteration: 639 Loss: nan\n",
      "Iteration: 640 Loss: nan\n",
      "Iteration: 641 Loss: nan\n",
      "Iteration: 642 Loss: nan\n",
      "Iteration: 643 Loss: nan\n",
      "Iteration: 644 Loss: nan\n",
      "Iteration: 645 Loss: nan\n",
      "Iteration: 646 Loss: nan\n",
      "Iteration: 647 Loss: nan\n",
      "Iteration: 648 Loss: nan\n",
      "Iteration: 649 Loss: nan\n",
      "Iteration: 650 Loss: nan\n",
      "Iteration: 651 Loss: nan\n",
      "Iteration: 652 Loss: nan\n",
      "Iteration: 653 Loss: nan\n",
      "Iteration: 654 Loss: nan\n",
      "Iteration: 655 Loss: nan\n",
      "Iteration: 656 Loss: nan\n",
      "Iteration: 657 Loss: nan\n",
      "Iteration: 658 Loss: nan\n",
      "Iteration: 659 Loss: nan\n",
      "Iteration: 660 Loss: nan\n",
      "Iteration: 661 Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 662 Loss: nan\n",
      "Iteration: 663 Loss: nan\n",
      "Iteration: 664 Loss: nan\n",
      "Iteration: 665 Loss: nan\n",
      "Iteration: 666 Loss: nan\n",
      "Iteration: 667 Loss: nan\n",
      "Iteration: 668 Loss: nan\n",
      "Iteration: 669 Loss: nan\n",
      "Iteration: 670 Loss: nan\n",
      "Iteration: 671 Loss: nan\n",
      "Iteration: 672 Loss: nan\n",
      "Iteration: 673 Loss: nan\n",
      "Iteration: 674 Loss: nan\n",
      "Iteration: 675 Loss: nan\n",
      "Iteration: 676 Loss: nan\n",
      "Iteration: 677 Loss: nan\n",
      "Iteration: 678 Loss: nan\n",
      "Iteration: 679 Loss: nan\n",
      "Iteration: 680 Loss: nan\n",
      "Iteration: 681 Loss: nan\n",
      "Iteration: 682 Loss: nan\n",
      "Iteration: 683 Loss: nan\n",
      "Iteration: 684 Loss: nan\n",
      "Iteration: 685 Loss: nan\n",
      "Iteration: 686 Loss: nan\n",
      "Iteration: 687 Loss: nan\n",
      "Iteration: 688 Loss: nan\n",
      "Iteration: 689 Loss: nan\n",
      "Iteration: 690 Loss: nan\n",
      "Iteration: 691 Loss: nan\n",
      "Iteration: 692 Loss: nan\n",
      "Iteration: 693 Loss: nan\n",
      "Iteration: 694 Loss: nan\n",
      "Iteration: 695 Loss: nan\n",
      "Iteration: 696 Loss: nan\n",
      "Iteration: 697 Loss: nan\n",
      "Iteration: 698 Loss: nan\n",
      "Iteration: 699 Loss: nan\n",
      "Iteration: 700 Loss: nan\n",
      "Iteration: 701 Loss: nan\n",
      "Iteration: 702 Loss: nan\n",
      "Iteration: 703 Loss: nan\n",
      "Iteration: 704 Loss: nan\n",
      "Iteration: 705 Loss: nan\n",
      "Iteration: 706 Loss: nan\n",
      "Iteration: 707 Loss: nan\n",
      "Iteration: 708 Loss: nan\n",
      "Iteration: 709 Loss: nan\n",
      "Iteration: 710 Loss: nan\n",
      "Iteration: 711 Loss: nan\n",
      "Iteration: 712 Loss: nan\n",
      "Iteration: 713 Loss: nan\n",
      "Iteration: 714 Loss: nan\n",
      "Iteration: 715 Loss: nan\n",
      "Iteration: 716 Loss: nan\n",
      "Iteration: 717 Loss: nan\n",
      "Iteration: 718 Loss: nan\n",
      "Iteration: 719 Loss: nan\n",
      "Iteration: 720 Loss: nan\n",
      "Iteration: 721 Loss: nan\n",
      "Iteration: 722 Loss: nan\n",
      "Iteration: 723 Loss: nan\n",
      "Iteration: 724 Loss: nan\n",
      "Iteration: 725 Loss: nan\n",
      "Iteration: 726 Loss: nan\n",
      "Iteration: 727 Loss: nan\n",
      "Iteration: 728 Loss: nan\n",
      "Iteration: 729 Loss: nan\n",
      "Iteration: 730 Loss: nan\n",
      "Iteration: 731 Loss: nan\n",
      "Iteration: 732 Loss: nan\n",
      "Iteration: 733 Loss: nan\n",
      "Iteration: 734 Loss: nan\n",
      "Iteration: 735 Loss: nan\n",
      "Iteration: 736 Loss: nan\n",
      "Iteration: 737 Loss: nan\n",
      "Iteration: 738 Loss: nan\n",
      "Iteration: 739 Loss: nan\n",
      "Iteration: 740 Loss: nan\n",
      "Iteration: 741 Loss: nan\n",
      "Iteration: 742 Loss: nan\n",
      "Iteration: 743 Loss: nan\n",
      "Iteration: 744 Loss: nan\n",
      "Iteration: 745 Loss: nan\n",
      "Iteration: 746 Loss: nan\n",
      "Iteration: 747 Loss: nan\n",
      "Iteration: 748 Loss: nan\n",
      "Iteration: 749 Loss: nan\n",
      "Iteration: 750 Loss: nan\n",
      "Iteration: 751 Loss: nan\n",
      "Iteration: 752 Loss: nan\n",
      "Iteration: 753 Loss: nan\n",
      "Iteration: 754 Loss: nan\n",
      "Iteration: 755 Loss: nan\n",
      "Iteration: 756 Loss: nan\n",
      "Iteration: 757 Loss: nan\n",
      "Iteration: 758 Loss: nan\n",
      "Iteration: 759 Loss: nan\n",
      "Iteration: 760 Loss: nan\n",
      "Iteration: 761 Loss: nan\n",
      "Iteration: 762 Loss: nan\n",
      "Iteration: 763 Loss: nan\n",
      "Iteration: 764 Loss: nan\n",
      "Iteration: 765 Loss: nan\n",
      "Iteration: 766 Loss: nan\n",
      "Iteration: 767 Loss: nan\n",
      "Iteration: 768 Loss: nan\n",
      "Iteration: 769 Loss: nan\n",
      "Iteration: 770 Loss: nan\n",
      "Iteration: 771 Loss: nan\n",
      "Iteration: 772 Loss: nan\n",
      "Iteration: 773 Loss: nan\n",
      "Iteration: 774 Loss: nan\n",
      "Iteration: 775 Loss: nan\n",
      "Iteration: 776 Loss: nan\n",
      "Iteration: 777 Loss: nan\n",
      "Iteration: 778 Loss: nan\n",
      "Iteration: 779 Loss: nan\n",
      "Iteration: 780 Loss: nan\n",
      "Iteration: 781 Loss: nan\n",
      "Iteration: 782 Loss: nan\n",
      "Iteration: 783 Loss: nan\n",
      "Iteration: 784 Loss: nan\n",
      "Iteration: 785 Loss: nan\n",
      "Iteration: 786 Loss: nan\n",
      "Iteration: 787 Loss: nan\n",
      "Iteration: 788 Loss: nan\n",
      "Iteration: 789 Loss: nan\n",
      "Iteration: 790 Loss: nan\n",
      "Iteration: 791 Loss: nan\n",
      "Iteration: 792 Loss: nan\n",
      "Iteration: 793 Loss: nan\n",
      "Iteration: 794 Loss: nan\n",
      "Iteration: 795 Loss: nan\n",
      "Iteration: 796 Loss: nan\n",
      "Iteration: 797 Loss: nan\n",
      "Iteration: 798 Loss: nan\n",
      "Iteration: 799 Loss: nan\n",
      "Iteration: 800 Loss: nan\n",
      "Iteration: 801 Loss: nan\n",
      "Iteration: 802 Loss: nan\n",
      "Iteration: 803 Loss: nan\n",
      "Iteration: 804 Loss: nan\n",
      "Iteration: 805 Loss: nan\n",
      "Iteration: 806 Loss: nan\n",
      "Iteration: 807 Loss: nan\n",
      "Iteration: 808 Loss: nan\n",
      "Iteration: 809 Loss: nan\n",
      "Iteration: 810 Loss: nan\n",
      "Iteration: 811 Loss: nan\n",
      "Iteration: 812 Loss: nan\n",
      "Iteration: 813 Loss: nan\n",
      "Iteration: 814 Loss: nan\n",
      "Iteration: 815 Loss: nan\n",
      "Iteration: 816 Loss: nan\n",
      "Iteration: 817 Loss: nan\n",
      "Iteration: 818 Loss: nan\n",
      "Iteration: 819 Loss: nan\n",
      "Iteration: 820 Loss: nan\n",
      "Iteration: 821 Loss: nan\n",
      "Iteration: 822 Loss: nan\n",
      "Iteration: 823 Loss: nan\n",
      "Iteration: 824 Loss: nan\n",
      "Iteration: 825 Loss: nan\n",
      "Iteration: 826 Loss: nan\n",
      "Iteration: 827 Loss: nan\n",
      "Iteration: 828 Loss: nan\n",
      "Iteration: 829 Loss: nan\n",
      "Iteration: 830 Loss: nan\n",
      "Iteration: 831 Loss: nan\n",
      "Iteration: 832 Loss: nan\n",
      "Iteration: 833 Loss: nan\n",
      "Iteration: 834 Loss: nan\n",
      "Iteration: 835 Loss: nan\n",
      "Iteration: 836 Loss: nan\n",
      "Iteration: 837 Loss: nan\n",
      "Iteration: 838 Loss: nan\n",
      "Iteration: 839 Loss: nan\n",
      "Iteration: 840 Loss: nan\n",
      "Iteration: 841 Loss: nan\n",
      "Iteration: 842 Loss: nan\n",
      "Iteration: 843 Loss: nan\n",
      "Iteration: 844 Loss: nan\n",
      "Iteration: 845 Loss: nan\n",
      "Iteration: 846 Loss: nan\n",
      "Iteration: 847 Loss: nan\n",
      "Iteration: 848 Loss: nan\n",
      "Iteration: 849 Loss: nan\n",
      "Iteration: 850 Loss: nan\n",
      "Iteration: 851 Loss: nan\n",
      "Iteration: 852 Loss: nan\n",
      "Iteration: 853 Loss: nan\n",
      "Iteration: 854 Loss: nan\n",
      "Iteration: 855 Loss: nan\n",
      "Iteration: 856 Loss: nan\n",
      "Iteration: 857 Loss: nan\n",
      "Iteration: 858 Loss: nan\n",
      "Iteration: 859 Loss: nan\n",
      "Iteration: 860 Loss: nan\n",
      "Iteration: 861 Loss: nan\n",
      "Iteration: 862 Loss: nan\n",
      "Iteration: 863 Loss: nan\n",
      "Iteration: 864 Loss: nan\n",
      "Iteration: 865 Loss: nan\n",
      "Iteration: 866 Loss: nan\n",
      "Iteration: 867 Loss: nan\n",
      "Iteration: 868 Loss: nan\n",
      "Iteration: 869 Loss: nan\n",
      "Iteration: 870 Loss: nan\n",
      "Iteration: 871 Loss: nan\n",
      "Iteration: 872 Loss: nan\n",
      "Iteration: 873 Loss: nan\n",
      "Iteration: 874 Loss: nan\n",
      "Iteration: 875 Loss: nan\n",
      "Iteration: 876 Loss: nan\n",
      "Iteration: 877 Loss: nan\n",
      "Iteration: 878 Loss: nan\n",
      "Iteration: 879 Loss: nan\n",
      "Iteration: 880 Loss: nan\n",
      "Iteration: 881 Loss: nan\n",
      "Iteration: 882 Loss: nan\n",
      "Iteration: 883 Loss: nan\n",
      "Iteration: 884 Loss: nan\n",
      "Iteration: 885 Loss: nan\n",
      "Iteration: 886 Loss: nan\n",
      "Iteration: 887 Loss: nan\n",
      "Iteration: 888 Loss: nan\n",
      "Iteration: 889 Loss: nan\n",
      "Iteration: 890 Loss: nan\n",
      "Iteration: 891 Loss: nan\n",
      "Iteration: 892 Loss: nan\n",
      "Iteration: 893 Loss: nan\n",
      "Iteration: 894 Loss: nan\n",
      "Iteration: 895 Loss: nan\n",
      "Iteration: 896 Loss: nan\n",
      "Iteration: 897 Loss: nan\n",
      "Iteration: 898 Loss: nan\n",
      "Iteration: 899 Loss: nan\n",
      "Iteration: 900 Loss: nan\n",
      "Iteration: 901 Loss: nan\n",
      "Iteration: 902 Loss: nan\n",
      "Iteration: 903 Loss: nan\n",
      "Iteration: 904 Loss: nan\n",
      "Iteration: 905 Loss: nan\n",
      "Iteration: 906 Loss: nan\n",
      "Iteration: 907 Loss: nan\n",
      "Iteration: 908 Loss: nan\n",
      "Iteration: 909 Loss: nan\n",
      "Iteration: 910 Loss: nan\n",
      "Iteration: 911 Loss: nan\n",
      "Iteration: 912 Loss: nan\n",
      "Iteration: 913 Loss: nan\n",
      "Iteration: 914 Loss: nan\n",
      "Iteration: 915 Loss: nan\n",
      "Iteration: 916 Loss: nan\n",
      "Iteration: 917 Loss: nan\n",
      "Iteration: 918 Loss: nan\n",
      "Iteration: 919 Loss: nan\n",
      "Iteration: 920 Loss: nan\n",
      "Iteration: 921 Loss: nan\n",
      "Iteration: 922 Loss: nan\n",
      "Iteration: 923 Loss: nan\n",
      "Iteration: 924 Loss: nan\n",
      "Iteration: 925 Loss: nan\n",
      "Iteration: 926 Loss: nan\n",
      "Iteration: 927 Loss: nan\n",
      "Iteration: 928 Loss: nan\n",
      "Iteration: 929 Loss: nan\n",
      "Iteration: 930 Loss: nan\n",
      "Iteration: 931 Loss: nan\n",
      "Iteration: 932 Loss: nan\n",
      "Iteration: 933 Loss: nan\n",
      "Iteration: 934 Loss: nan\n",
      "Iteration: 935 Loss: nan\n",
      "Iteration: 936 Loss: nan\n",
      "Iteration: 937 Loss: nan\n",
      "Iteration: 938 Loss: nan\n",
      "Iteration: 939 Loss: nan\n",
      "Iteration: 940 Loss: nan\n",
      "Iteration: 941 Loss: nan\n",
      "Iteration: 942 Loss: nan\n",
      "Iteration: 943 Loss: nan\n",
      "Iteration: 944 Loss: nan\n",
      "Iteration: 945 Loss: nan\n",
      "Iteration: 946 Loss: nan\n",
      "Iteration: 947 Loss: nan\n",
      "Iteration: 948 Loss: nan\n",
      "Iteration: 949 Loss: nan\n",
      "Iteration: 950 Loss: nan\n",
      "Iteration: 951 Loss: nan\n",
      "Iteration: 952 Loss: nan\n",
      "Iteration: 953 Loss: nan\n",
      "Iteration: 954 Loss: nan\n",
      "Iteration: 955 Loss: nan\n",
      "Iteration: 956 Loss: nan\n",
      "Iteration: 957 Loss: nan\n",
      "Iteration: 958 Loss: nan\n",
      "Iteration: 959 Loss: nan\n",
      "Iteration: 960 Loss: nan\n",
      "Iteration: 961 Loss: nan\n",
      "Iteration: 962 Loss: nan\n",
      "Iteration: 963 Loss: nan\n",
      "Iteration: 964 Loss: nan\n",
      "Iteration: 965 Loss: nan\n",
      "Iteration: 966 Loss: nan\n",
      "Iteration: 967 Loss: nan\n",
      "Iteration: 968 Loss: nan\n",
      "Iteration: 969 Loss: nan\n",
      "Iteration: 970 Loss: nan\n",
      "Iteration: 971 Loss: nan\n",
      "Iteration: 972 Loss: nan\n",
      "Iteration: 973 Loss: nan\n",
      "Iteration: 974 Loss: nan\n",
      "Iteration: 975 Loss: nan\n",
      "Iteration: 976 Loss: nan\n",
      "Iteration: 977 Loss: nan\n",
      "Iteration: 978 Loss: nan\n",
      "Iteration: 979 Loss: nan\n",
      "Iteration: 980 Loss: nan\n",
      "Iteration: 981 Loss: nan\n",
      "Iteration: 982 Loss: nan\n",
      "Iteration: 983 Loss: nan\n",
      "Iteration: 984 Loss: nan\n",
      "Iteration: 985 Loss: nan\n",
      "Iteration: 986 Loss: nan\n",
      "Iteration: 987 Loss: nan\n",
      "Iteration: 988 Loss: nan\n",
      "Iteration: 989 Loss: nan\n",
      "Iteration: 990 Loss: nan\n",
      "Iteration: 991 Loss: nan\n",
      "Iteration: 992 Loss: nan\n",
      "Iteration: 993 Loss: nan\n",
      "Iteration: 994 Loss: nan\n",
      "Iteration: 995 Loss: nan\n",
      "Iteration: 996 Loss: nan\n",
      "Iteration: 997 Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 998 Loss: nan\n",
      "Iteration: 999 Loss: nan\n",
      "Iteration: 1000 Loss: nan\n",
      "Iteration: 1001 Loss: nan\n",
      "Iteration: 1002 Loss: nan\n",
      "Iteration: 1003 Loss: nan\n",
      "Iteration: 1004 Loss: nan\n",
      "Iteration: 1005 Loss: nan\n",
      "Iteration: 1006 Loss: nan\n",
      "Iteration: 1007 Loss: nan\n",
      "Iteration: 1008 Loss: nan\n",
      "Iteration: 1009 Loss: nan\n",
      "Iteration: 1010 Loss: nan\n",
      "Iteration: 1011 Loss: nan\n",
      "Iteration: 1012 Loss: nan\n",
      "Iteration: 1013 Loss: nan\n",
      "Iteration: 1014 Loss: nan\n",
      "Iteration: 1015 Loss: nan\n",
      "Iteration: 1016 Loss: nan\n",
      "Iteration: 1017 Loss: nan\n",
      "Iteration: 1018 Loss: nan\n",
      "Iteration: 1019 Loss: nan\n",
      "Iteration: 1020 Loss: nan\n",
      "Iteration: 1021 Loss: nan\n",
      "Iteration: 1022 Loss: nan\n",
      "Iteration: 1023 Loss: nan\n",
      "Iteration: 1024 Loss: nan\n",
      "Iteration: 1025 Loss: nan\n",
      "Iteration: 1026 Loss: nan\n",
      "Iteration: 1027 Loss: nan\n",
      "Iteration: 1028 Loss: nan\n",
      "Iteration: 1029 Loss: nan\n",
      "Iteration: 1030 Loss: nan\n",
      "Iteration: 1031 Loss: nan\n",
      "Iteration: 1032 Loss: nan\n",
      "Iteration: 1033 Loss: nan\n",
      "Iteration: 1034 Loss: nan\n",
      "Iteration: 1035 Loss: nan\n",
      "Iteration: 1036 Loss: nan\n",
      "Iteration: 1037 Loss: nan\n",
      "Iteration: 1038 Loss: nan\n",
      "Iteration: 1039 Loss: nan\n",
      "Iteration: 1040 Loss: nan\n",
      "Iteration: 1041 Loss: nan\n",
      "Iteration: 1042 Loss: nan\n",
      "Iteration: 1043 Loss: nan\n",
      "Iteration: 1044 Loss: nan\n",
      "Iteration: 1045 Loss: nan\n",
      "Iteration: 1046 Loss: nan\n",
      "Iteration: 1047 Loss: nan\n",
      "Iteration: 1048 Loss: nan\n",
      "Iteration: 1049 Loss: nan\n",
      "Iteration: 1050 Loss: nan\n",
      "Iteration: 1051 Loss: nan\n",
      "Iteration: 1052 Loss: nan\n",
      "Iteration: 1053 Loss: nan\n",
      "Iteration: 1054 Loss: nan\n",
      "Iteration: 1055 Loss: nan\n",
      "Iteration: 1056 Loss: nan\n",
      "Iteration: 1057 Loss: nan\n",
      "Iteration: 1058 Loss: nan\n",
      "Iteration: 1059 Loss: nan\n",
      "Iteration: 1060 Loss: nan\n",
      "Iteration: 1061 Loss: nan\n",
      "Iteration: 1062 Loss: nan\n",
      "Iteration: 1063 Loss: nan\n",
      "Iteration: 1064 Loss: nan\n",
      "Iteration: 1065 Loss: nan\n",
      "Iteration: 1066 Loss: nan\n",
      "Iteration: 1067 Loss: nan\n",
      "Iteration: 1068 Loss: nan\n",
      "Iteration: 1069 Loss: nan\n",
      "Iteration: 1070 Loss: nan\n",
      "Iteration: 1071 Loss: nan\n",
      "Iteration: 1072 Loss: nan\n",
      "Iteration: 1073 Loss: nan\n",
      "Iteration: 1074 Loss: nan\n",
      "Iteration: 1075 Loss: nan\n",
      "Iteration: 1076 Loss: nan\n",
      "Iteration: 1077 Loss: nan\n",
      "Iteration: 1078 Loss: nan\n",
      "Iteration: 1079 Loss: nan\n",
      "Iteration: 1080 Loss: nan\n",
      "Iteration: 1081 Loss: nan\n",
      "Iteration: 1082 Loss: nan\n",
      "Iteration: 1083 Loss: nan\n",
      "Iteration: 1084 Loss: nan\n",
      "Iteration: 1085 Loss: nan\n",
      "Iteration: 1086 Loss: nan\n",
      "Iteration: 1087 Loss: nan\n",
      "Iteration: 1088 Loss: nan\n",
      "Iteration: 1089 Loss: nan\n",
      "Iteration: 1090 Loss: nan\n",
      "Iteration: 1091 Loss: nan\n",
      "Iteration: 1092 Loss: nan\n",
      "Iteration: 1093 Loss: nan\n",
      "Iteration: 1094 Loss: nan\n",
      "Iteration: 1095 Loss: nan\n",
      "Iteration: 1096 Loss: nan\n",
      "Iteration: 1097 Loss: nan\n",
      "Iteration: 1098 Loss: nan\n",
      "Iteration: 1099 Loss: nan\n",
      "Iteration: 1100 Loss: nan\n",
      "Iteration: 1101 Loss: nan\n",
      "Iteration: 1102 Loss: nan\n",
      "Iteration: 1103 Loss: nan\n",
      "Iteration: 1104 Loss: nan\n",
      "Iteration: 1105 Loss: nan\n",
      "Iteration: 1106 Loss: nan\n",
      "Iteration: 1107 Loss: nan\n",
      "Iteration: 1108 Loss: nan\n",
      "Iteration: 1109 Loss: nan\n",
      "Iteration: 1110 Loss: nan\n",
      "Iteration: 1111 Loss: nan\n",
      "Iteration: 1112 Loss: nan\n",
      "Iteration: 1113 Loss: nan\n",
      "Iteration: 1114 Loss: nan\n",
      "Iteration: 1115 Loss: nan\n",
      "Iteration: 1116 Loss: nan\n",
      "Iteration: 1117 Loss: nan\n",
      "Iteration: 1118 Loss: nan\n",
      "Iteration: 1119 Loss: nan\n",
      "Iteration: 1120 Loss: nan\n",
      "Iteration: 1121 Loss: nan\n",
      "Iteration: 1122 Loss: nan\n",
      "Iteration: 1123 Loss: nan\n",
      "Iteration: 1124 Loss: nan\n",
      "Iteration: 1125 Loss: nan\n",
      "Iteration: 1126 Loss: nan\n",
      "Iteration: 1127 Loss: nan\n",
      "Iteration: 1128 Loss: nan\n",
      "Iteration: 1129 Loss: nan\n",
      "Iteration: 1130 Loss: nan\n",
      "Iteration: 1131 Loss: nan\n",
      "Iteration: 1132 Loss: nan\n",
      "Iteration: 1133 Loss: nan\n",
      "Iteration: 1134 Loss: nan\n",
      "Iteration: 1135 Loss: nan\n",
      "Iteration: 1136 Loss: nan\n",
      "Iteration: 1137 Loss: nan\n",
      "Iteration: 1138 Loss: nan\n",
      "Iteration: 1139 Loss: nan\n",
      "Iteration: 1140 Loss: nan\n",
      "Iteration: 1141 Loss: nan\n",
      "Iteration: 1142 Loss: nan\n",
      "Iteration: 1143 Loss: nan\n",
      "Iteration: 1144 Loss: nan\n",
      "Iteration: 1145 Loss: nan\n",
      "Iteration: 1146 Loss: nan\n",
      "Iteration: 1147 Loss: nan\n",
      "Iteration: 1148 Loss: nan\n",
      "Iteration: 1149 Loss: nan\n",
      "Iteration: 1150 Loss: nan\n",
      "Iteration: 1151 Loss: nan\n",
      "Iteration: 1152 Loss: nan\n",
      "Iteration: 1153 Loss: nan\n",
      "Iteration: 1154 Loss: nan\n",
      "Iteration: 1155 Loss: nan\n",
      "Iteration: 1156 Loss: nan\n",
      "Iteration: 1157 Loss: nan\n",
      "Iteration: 1158 Loss: nan\n",
      "Iteration: 1159 Loss: nan\n",
      "Iteration: 1160 Loss: nan\n",
      "Iteration: 1161 Loss: nan\n",
      "Iteration: 1162 Loss: nan\n",
      "Iteration: 1163 Loss: nan\n",
      "Iteration: 1164 Loss: nan\n",
      "Iteration: 1165 Loss: nan\n",
      "Iteration: 1166 Loss: nan\n",
      "Iteration: 1167 Loss: nan\n",
      "Iteration: 1168 Loss: nan\n",
      "Iteration: 1169 Loss: nan\n",
      "Iteration: 1170 Loss: nan\n",
      "Iteration: 1171 Loss: nan\n",
      "Iteration: 1172 Loss: nan\n",
      "Iteration: 1173 Loss: nan\n",
      "Iteration: 1174 Loss: nan\n",
      "Iteration: 1175 Loss: nan\n",
      "Iteration: 1176 Loss: nan\n",
      "Iteration: 1177 Loss: nan\n",
      "Iteration: 1178 Loss: nan\n",
      "Iteration: 1179 Loss: nan\n",
      "Iteration: 1180 Loss: nan\n",
      "Iteration: 1181 Loss: nan\n",
      "Iteration: 1182 Loss: nan\n",
      "Iteration: 1183 Loss: nan\n",
      "Iteration: 1184 Loss: nan\n",
      "Iteration: 1185 Loss: nan\n",
      "Iteration: 1186 Loss: nan\n",
      "Iteration: 1187 Loss: nan\n",
      "Iteration: 1188 Loss: nan\n",
      "Iteration: 1189 Loss: nan\n",
      "Iteration: 1190 Loss: nan\n",
      "Iteration: 1191 Loss: nan\n",
      "Iteration: 1192 Loss: nan\n",
      "Iteration: 1193 Loss: nan\n",
      "Iteration: 1194 Loss: nan\n",
      "Iteration: 1195 Loss: nan\n",
      "Iteration: 1196 Loss: nan\n",
      "Iteration: 1197 Loss: nan\n",
      "Iteration: 1198 Loss: nan\n",
      "Iteration: 1199 Loss: nan\n",
      "Iteration: 1200 Loss: nan\n",
      "Iteration: 1201 Loss: nan\n",
      "Iteration: 1202 Loss: nan\n",
      "Iteration: 1203 Loss: nan\n",
      "Iteration: 1204 Loss: nan\n",
      "Iteration: 1205 Loss: nan\n",
      "Iteration: 1206 Loss: nan\n",
      "Iteration: 1207 Loss: nan\n",
      "Iteration: 1208 Loss: nan\n",
      "Iteration: 1209 Loss: nan\n",
      "Iteration: 1210 Loss: nan\n",
      "Iteration: 1211 Loss: nan\n",
      "Iteration: 1212 Loss: nan\n",
      "Iteration: 1213 Loss: nan\n",
      "Iteration: 1214 Loss: nan\n",
      "Iteration: 1215 Loss: nan\n",
      "Iteration: 1216 Loss: nan\n",
      "Iteration: 1217 Loss: nan\n",
      "Iteration: 1218 Loss: nan\n",
      "Iteration: 1219 Loss: nan\n",
      "Iteration: 1220 Loss: nan\n",
      "Iteration: 1221 Loss: nan\n",
      "Iteration: 1222 Loss: nan\n",
      "Iteration: 1223 Loss: nan\n",
      "Iteration: 1224 Loss: nan\n",
      "Iteration: 1225 Loss: nan\n",
      "Iteration: 1226 Loss: nan\n",
      "Iteration: 1227 Loss: nan\n",
      "Iteration: 1228 Loss: nan\n",
      "Iteration: 1229 Loss: nan\n",
      "Iteration: 1230 Loss: nan\n",
      "Iteration: 1231 Loss: nan\n",
      "Iteration: 1232 Loss: nan\n",
      "Iteration: 1233 Loss: nan\n",
      "Iteration: 1234 Loss: nan\n",
      "Iteration: 1235 Loss: nan\n",
      "Iteration: 1236 Loss: nan\n",
      "Iteration: 1237 Loss: nan\n",
      "Iteration: 1238 Loss: nan\n",
      "Iteration: 1239 Loss: nan\n",
      "Iteration: 1240 Loss: nan\n",
      "Iteration: 1241 Loss: nan\n",
      "Iteration: 1242 Loss: nan\n",
      "Iteration: 1243 Loss: nan\n",
      "Iteration: 1244 Loss: nan\n",
      "Iteration: 1245 Loss: nan\n",
      "Iteration: 1246 Loss: nan\n",
      "Iteration: 1247 Loss: nan\n",
      "Iteration: 1248 Loss: nan\n",
      "Iteration: 1249 Loss: nan\n",
      "Iteration: 1250 Loss: nan\n",
      "Iteration: 1251 Loss: nan\n",
      "Iteration: 1252 Loss: nan\n",
      "Iteration: 1253 Loss: nan\n",
      "Iteration: 1254 Loss: nan\n",
      "Iteration: 1255 Loss: nan\n",
      "Iteration: 1256 Loss: nan\n",
      "Iteration: 1257 Loss: nan\n",
      "Iteration: 1258 Loss: nan\n",
      "Iteration: 1259 Loss: nan\n",
      "Iteration: 1260 Loss: nan\n",
      "Iteration: 1261 Loss: nan\n",
      "Iteration: 1262 Loss: nan\n",
      "Iteration: 1263 Loss: nan\n",
      "Iteration: 1264 Loss: nan\n",
      "Iteration: 1265 Loss: nan\n",
      "Iteration: 1266 Loss: nan\n",
      "Iteration: 1267 Loss: nan\n",
      "Iteration: 1268 Loss: nan\n",
      "Iteration: 1269 Loss: nan\n",
      "Iteration: 1270 Loss: nan\n",
      "Iteration: 1271 Loss: nan\n",
      "Iteration: 1272 Loss: nan\n",
      "Iteration: 1273 Loss: nan\n",
      "Iteration: 1274 Loss: nan\n",
      "Iteration: 1275 Loss: nan\n",
      "Iteration: 1276 Loss: nan\n",
      "Iteration: 1277 Loss: nan\n",
      "Iteration: 1278 Loss: nan\n",
      "Iteration: 1279 Loss: nan\n",
      "Iteration: 1280 Loss: nan\n",
      "Iteration: 1281 Loss: nan\n",
      "Iteration: 1282 Loss: nan\n",
      "Iteration: 1283 Loss: nan\n",
      "Iteration: 1284 Loss: nan\n",
      "Iteration: 1285 Loss: nan\n",
      "Iteration: 1286 Loss: nan\n",
      "Iteration: 1287 Loss: nan\n",
      "Iteration: 1288 Loss: nan\n",
      "Iteration: 1289 Loss: nan\n",
      "Iteration: 1290 Loss: nan\n",
      "Iteration: 1291 Loss: nan\n",
      "Iteration: 1292 Loss: nan\n",
      "Iteration: 1293 Loss: nan\n",
      "Iteration: 1294 Loss: nan\n",
      "Iteration: 1295 Loss: nan\n",
      "Iteration: 1296 Loss: nan\n",
      "Iteration: 1297 Loss: nan\n",
      "Iteration: 1298 Loss: nan\n",
      "Iteration: 1299 Loss: nan\n",
      "Iteration: 1300 Loss: nan\n",
      "Iteration: 1301 Loss: nan\n",
      "Iteration: 1302 Loss: nan\n",
      "Iteration: 1303 Loss: nan\n",
      "Iteration: 1304 Loss: nan\n",
      "Iteration: 1305 Loss: nan\n",
      "Iteration: 1306 Loss: nan\n",
      "Iteration: 1307 Loss: nan\n",
      "Iteration: 1308 Loss: nan\n",
      "Iteration: 1309 Loss: nan\n",
      "Iteration: 1310 Loss: nan\n",
      "Iteration: 1311 Loss: nan\n",
      "Iteration: 1312 Loss: nan\n",
      "Iteration: 1313 Loss: nan\n",
      "Iteration: 1314 Loss: nan\n",
      "Iteration: 1315 Loss: nan\n",
      "Iteration: 1316 Loss: nan\n",
      "Iteration: 1317 Loss: nan\n",
      "Iteration: 1318 Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1319 Loss: nan\n",
      "Iteration: 1320 Loss: nan\n",
      "Iteration: 1321 Loss: nan\n",
      "Iteration: 1322 Loss: nan\n",
      "Iteration: 1323 Loss: nan\n",
      "Iteration: 1324 Loss: nan\n",
      "Iteration: 1325 Loss: nan\n",
      "Iteration: 1326 Loss: nan\n",
      "Iteration: 1327 Loss: nan\n",
      "Iteration: 1328 Loss: nan\n",
      "Iteration: 1329 Loss: nan\n",
      "Iteration: 1330 Loss: nan\n",
      "Iteration: 1331 Loss: nan\n",
      "Iteration: 1332 Loss: nan\n",
      "Iteration: 1333 Loss: nan\n",
      "Iteration: 1334 Loss: nan\n",
      "Iteration: 1335 Loss: nan\n",
      "Iteration: 1336 Loss: nan\n",
      "Iteration: 1337 Loss: nan\n",
      "Iteration: 1338 Loss: nan\n",
      "Iteration: 1339 Loss: nan\n",
      "Iteration: 1340 Loss: nan\n",
      "Iteration: 1341 Loss: nan\n",
      "Iteration: 1342 Loss: nan\n",
      "Iteration: 1343 Loss: nan\n",
      "Iteration: 1344 Loss: nan\n",
      "Iteration: 1345 Loss: nan\n",
      "Iteration: 1346 Loss: nan\n",
      "Iteration: 1347 Loss: nan\n",
      "Iteration: 1348 Loss: nan\n",
      "Iteration: 1349 Loss: nan\n",
      "Iteration: 1350 Loss: nan\n",
      "Iteration: 1351 Loss: nan\n",
      "Iteration: 1352 Loss: nan\n",
      "Iteration: 1353 Loss: nan\n",
      "Iteration: 1354 Loss: nan\n",
      "Iteration: 1355 Loss: nan\n",
      "Iteration: 1356 Loss: nan\n",
      "Iteration: 1357 Loss: nan\n",
      "Iteration: 1358 Loss: nan\n",
      "Iteration: 1359 Loss: nan\n",
      "Iteration: 1360 Loss: nan\n",
      "Iteration: 1361 Loss: nan\n",
      "Iteration: 1362 Loss: nan\n",
      "Iteration: 1363 Loss: nan\n",
      "Iteration: 1364 Loss: nan\n",
      "Iteration: 1365 Loss: nan\n",
      "Iteration: 1366 Loss: nan\n",
      "Iteration: 1367 Loss: nan\n",
      "Iteration: 1368 Loss: nan\n",
      "Iteration: 1369 Loss: nan\n",
      "Iteration: 1370 Loss: nan\n",
      "Iteration: 1371 Loss: nan\n",
      "Iteration: 1372 Loss: nan\n",
      "Iteration: 1373 Loss: nan\n",
      "Iteration: 1374 Loss: nan\n",
      "Iteration: 1375 Loss: nan\n",
      "Iteration: 1376 Loss: nan\n",
      "Iteration: 1377 Loss: nan\n",
      "Iteration: 1378 Loss: nan\n",
      "Iteration: 1379 Loss: nan\n",
      "Iteration: 1380 Loss: nan\n",
      "Iteration: 1381 Loss: nan\n",
      "Iteration: 1382 Loss: nan\n",
      "Iteration: 1383 Loss: nan\n",
      "Iteration: 1384 Loss: nan\n",
      "Iteration: 1385 Loss: nan\n",
      "Iteration: 1386 Loss: nan\n",
      "Iteration: 1387 Loss: nan\n",
      "Iteration: 1388 Loss: nan\n",
      "Iteration: 1389 Loss: nan\n",
      "Iteration: 1390 Loss: nan\n",
      "Iteration: 1391 Loss: nan\n",
      "Iteration: 1392 Loss: nan\n",
      "Iteration: 1393 Loss: nan\n",
      "Iteration: 1394 Loss: nan\n",
      "Iteration: 1395 Loss: nan\n",
      "Iteration: 1396 Loss: nan\n",
      "Iteration: 1397 Loss: nan\n",
      "Iteration: 1398 Loss: nan\n",
      "Iteration: 1399 Loss: nan\n",
      "Iteration: 1400 Loss: nan\n",
      "Iteration: 1401 Loss: nan\n",
      "Iteration: 1402 Loss: nan\n",
      "Iteration: 1403 Loss: nan\n",
      "Iteration: 1404 Loss: nan\n",
      "Iteration: 1405 Loss: nan\n",
      "Iteration: 1406 Loss: nan\n",
      "Iteration: 1407 Loss: nan\n",
      "Iteration: 1408 Loss: nan\n",
      "Iteration: 1409 Loss: nan\n",
      "Iteration: 1410 Loss: nan\n",
      "Iteration: 1411 Loss: nan\n",
      "Iteration: 1412 Loss: nan\n",
      "Iteration: 1413 Loss: nan\n",
      "Iteration: 1414 Loss: nan\n",
      "Iteration: 1415 Loss: nan\n",
      "Iteration: 1416 Loss: nan\n",
      "Iteration: 1417 Loss: nan\n",
      "Iteration: 1418 Loss: nan\n",
      "Iteration: 1419 Loss: nan\n",
      "Iteration: 1420 Loss: nan\n",
      "Iteration: 1421 Loss: nan\n",
      "Iteration: 1422 Loss: nan\n",
      "Iteration: 1423 Loss: nan\n",
      "Iteration: 1424 Loss: nan\n",
      "Iteration: 1425 Loss: nan\n",
      "Iteration: 1426 Loss: nan\n",
      "Iteration: 1427 Loss: nan\n",
      "Iteration: 1428 Loss: nan\n",
      "Iteration: 1429 Loss: nan\n",
      "Iteration: 1430 Loss: nan\n",
      "Iteration: 1431 Loss: nan\n",
      "Iteration: 1432 Loss: nan\n",
      "Iteration: 1433 Loss: nan\n",
      "Iteration: 1434 Loss: nan\n",
      "Iteration: 1435 Loss: nan\n",
      "Iteration: 1436 Loss: nan\n",
      "Iteration: 1437 Loss: nan\n",
      "Iteration: 1438 Loss: nan\n",
      "Iteration: 1439 Loss: nan\n",
      "Iteration: 1440 Loss: nan\n",
      "Iteration: 1441 Loss: nan\n",
      "Iteration: 1442 Loss: nan\n",
      "Iteration: 1443 Loss: nan\n",
      "Iteration: 1444 Loss: nan\n",
      "Iteration: 1445 Loss: nan\n",
      "Iteration: 1446 Loss: nan\n",
      "Iteration: 1447 Loss: nan\n",
      "Iteration: 1448 Loss: nan\n",
      "Iteration: 1449 Loss: nan\n",
      "Iteration: 1450 Loss: nan\n",
      "Iteration: 1451 Loss: nan\n",
      "Iteration: 1452 Loss: nan\n",
      "Iteration: 1453 Loss: nan\n",
      "Iteration: 1454 Loss: nan\n",
      "Iteration: 1455 Loss: nan\n",
      "Iteration: 1456 Loss: nan\n",
      "Iteration: 1457 Loss: nan\n",
      "Iteration: 1458 Loss: nan\n",
      "Iteration: 1459 Loss: nan\n",
      "Iteration: 1460 Loss: nan\n",
      "Iteration: 1461 Loss: nan\n",
      "Iteration: 1462 Loss: nan\n",
      "Iteration: 1463 Loss: nan\n",
      "Iteration: 1464 Loss: nan\n",
      "Iteration: 1465 Loss: nan\n",
      "Iteration: 1466 Loss: nan\n",
      "Iteration: 1467 Loss: nan\n",
      "Iteration: 1468 Loss: nan\n",
      "Iteration: 1469 Loss: nan\n",
      "Iteration: 1470 Loss: nan\n",
      "Iteration: 1471 Loss: nan\n",
      "Iteration: 1472 Loss: nan\n",
      "Iteration: 1473 Loss: nan\n",
      "Iteration: 1474 Loss: nan\n",
      "Iteration: 1475 Loss: nan\n",
      "Iteration: 1476 Loss: nan\n",
      "Iteration: 1477 Loss: nan\n",
      "Iteration: 1478 Loss: nan\n",
      "Iteration: 1479 Loss: nan\n",
      "Iteration: 1480 Loss: nan\n",
      "Iteration: 1481 Loss: nan\n",
      "Iteration: 1482 Loss: nan\n",
      "Iteration: 1483 Loss: nan\n",
      "Iteration: 1484 Loss: nan\n",
      "Iteration: 1485 Loss: nan\n",
      "Iteration: 1486 Loss: nan\n",
      "Iteration: 1487 Loss: nan\n",
      "Iteration: 1488 Loss: nan\n",
      "Iteration: 1489 Loss: nan\n",
      "Iteration: 1490 Loss: nan\n",
      "Iteration: 1491 Loss: nan\n",
      "Iteration: 1492 Loss: nan\n",
      "Iteration: 1493 Loss: nan\n",
      "Iteration: 1494 Loss: nan\n",
      "Iteration: 1495 Loss: nan\n",
      "Iteration: 1496 Loss: nan\n",
      "Iteration: 1497 Loss: nan\n",
      "Iteration: 1498 Loss: nan\n",
      "Iteration: 1499 Loss: nan\n",
      "Iteration: 1500 Loss: nan\n",
      "Iteration: 1501 Loss: nan\n",
      "Iteration: 1502 Loss: nan\n",
      "Iteration: 1503 Loss: nan\n",
      "Iteration: 1504 Loss: nan\n",
      "Iteration: 1505 Loss: nan\n",
      "Iteration: 1506 Loss: nan\n",
      "Iteration: 1507 Loss: nan\n",
      "Iteration: 1508 Loss: nan\n",
      "Iteration: 1509 Loss: nan\n",
      "Iteration: 1510 Loss: nan\n",
      "Iteration: 1511 Loss: nan\n",
      "Iteration: 1512 Loss: nan\n",
      "Iteration: 1513 Loss: nan\n",
      "Iteration: 1514 Loss: nan\n",
      "Iteration: 1515 Loss: nan\n",
      "Iteration: 1516 Loss: nan\n",
      "Iteration: 1517 Loss: nan\n",
      "Iteration: 1518 Loss: nan\n",
      "Iteration: 1519 Loss: nan\n",
      "Iteration: 1520 Loss: nan\n",
      "Iteration: 1521 Loss: nan\n",
      "Iteration: 1522 Loss: nan\n",
      "Iteration: 1523 Loss: nan\n",
      "Iteration: 1524 Loss: nan\n",
      "Iteration: 1525 Loss: nan\n",
      "Iteration: 1526 Loss: nan\n",
      "Iteration: 1527 Loss: nan\n",
      "Iteration: 1528 Loss: nan\n",
      "Iteration: 1529 Loss: nan\n",
      "Iteration: 1530 Loss: nan\n",
      "Iteration: 1531 Loss: nan\n",
      "Iteration: 1532 Loss: nan\n",
      "Iteration: 1533 Loss: nan\n",
      "Iteration: 1534 Loss: nan\n",
      "Iteration: 1535 Loss: nan\n",
      "Iteration: 1536 Loss: nan\n",
      "Iteration: 1537 Loss: nan\n",
      "Iteration: 1538 Loss: nan\n",
      "Iteration: 1539 Loss: nan\n",
      "Iteration: 1540 Loss: nan\n",
      "Iteration: 1541 Loss: nan\n",
      "Iteration: 1542 Loss: nan\n",
      "Iteration: 1543 Loss: nan\n",
      "Iteration: 1544 Loss: nan\n",
      "Iteration: 1545 Loss: nan\n",
      "Iteration: 1546 Loss: nan\n",
      "Iteration: 1547 Loss: nan\n",
      "Iteration: 1548 Loss: nan\n",
      "Iteration: 1549 Loss: nan\n",
      "Iteration: 1550 Loss: nan\n",
      "Iteration: 1551 Loss: nan\n",
      "Iteration: 1552 Loss: nan\n",
      "Iteration: 1553 Loss: nan\n",
      "Iteration: 1554 Loss: nan\n",
      "Iteration: 1555 Loss: nan\n",
      "Iteration: 1556 Loss: nan\n",
      "Iteration: 1557 Loss: nan\n",
      "Iteration: 1558 Loss: nan\n",
      "Iteration: 1559 Loss: nan\n",
      "Iteration: 1560 Loss: nan\n",
      "Iteration: 1561 Loss: nan\n",
      "Iteration: 1562 Loss: nan\n",
      "Iteration: 1563 Loss: nan\n",
      "Iteration: 1564 Loss: nan\n",
      "Iteration: 1565 Loss: nan\n",
      "Iteration: 1566 Loss: nan\n",
      "Iteration: 1567 Loss: nan\n",
      "Iteration: 1568 Loss: nan\n",
      "Iteration: 1569 Loss: nan\n",
      "Iteration: 1570 Loss: nan\n",
      "Iteration: 1571 Loss: nan\n",
      "Iteration: 1572 Loss: nan\n",
      "Iteration: 1573 Loss: nan\n",
      "Iteration: 1574 Loss: nan\n",
      "Iteration: 1575 Loss: nan\n",
      "Iteration: 1576 Loss: nan\n",
      "Iteration: 1577 Loss: nan\n",
      "Iteration: 1578 Loss: nan\n",
      "Iteration: 1579 Loss: nan\n",
      "Iteration: 1580 Loss: nan\n",
      "Iteration: 1581 Loss: nan\n",
      "Iteration: 1582 Loss: nan\n",
      "Iteration: 1583 Loss: nan\n",
      "Iteration: 1584 Loss: nan\n",
      "Iteration: 1585 Loss: nan\n",
      "Iteration: 1586 Loss: nan\n",
      "Iteration: 1587 Loss: nan\n",
      "Iteration: 1588 Loss: nan\n",
      "Iteration: 1589 Loss: nan\n",
      "Iteration: 1590 Loss: nan\n",
      "Iteration: 1591 Loss: nan\n",
      "Iteration: 1592 Loss: nan\n",
      "Iteration: 1593 Loss: nan\n",
      "Iteration: 1594 Loss: nan\n",
      "Iteration: 1595 Loss: nan\n",
      "Iteration: 1596 Loss: nan\n",
      "Iteration: 1597 Loss: nan\n",
      "Iteration: 1598 Loss: nan\n",
      "Iteration: 1599 Loss: nan\n",
      "Iteration: 1600 Loss: nan\n",
      "Iteration: 1601 Loss: nan\n",
      "Iteration: 1602 Loss: nan\n",
      "Iteration: 1603 Loss: nan\n",
      "Iteration: 1604 Loss: nan\n",
      "Iteration: 1605 Loss: nan\n",
      "Iteration: 1606 Loss: nan\n",
      "Iteration: 1607 Loss: nan\n",
      "Iteration: 1608 Loss: nan\n",
      "Iteration: 1609 Loss: nan\n",
      "Iteration: 1610 Loss: nan\n",
      "Iteration: 1611 Loss: nan\n",
      "Iteration: 1612 Loss: nan\n",
      "Iteration: 1613 Loss: nan\n",
      "Iteration: 1614 Loss: nan\n",
      "Iteration: 1615 Loss: nan\n",
      "Iteration: 1616 Loss: nan\n",
      "Iteration: 1617 Loss: nan\n",
      "Iteration: 1618 Loss: nan\n",
      "Iteration: 1619 Loss: nan\n",
      "Iteration: 1620 Loss: nan\n",
      "Iteration: 1621 Loss: nan\n",
      "Iteration: 1622 Loss: nan\n",
      "Iteration: 1623 Loss: nan\n",
      "Iteration: 1624 Loss: nan\n",
      "Iteration: 1625 Loss: nan\n",
      "Iteration: 1626 Loss: nan\n",
      "Iteration: 1627 Loss: nan\n",
      "Iteration: 1628 Loss: nan\n",
      "Iteration: 1629 Loss: nan\n",
      "Iteration: 1630 Loss: nan\n",
      "Iteration: 1631 Loss: nan\n",
      "Iteration: 1632 Loss: nan\n",
      "Iteration: 1633 Loss: nan\n",
      "Iteration: 1634 Loss: nan\n",
      "Iteration: 1635 Loss: nan\n",
      "Iteration: 1636 Loss: nan\n",
      "Iteration: 1637 Loss: nan\n",
      "Iteration: 1638 Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1639 Loss: nan\n",
      "Iteration: 1640 Loss: nan\n",
      "Iteration: 1641 Loss: nan\n",
      "Iteration: 1642 Loss: nan\n",
      "Iteration: 1643 Loss: nan\n",
      "Iteration: 1644 Loss: nan\n",
      "Iteration: 1645 Loss: nan\n",
      "Iteration: 1646 Loss: nan\n",
      "Iteration: 1647 Loss: nan\n",
      "Iteration: 1648 Loss: nan\n",
      "Iteration: 1649 Loss: nan\n",
      "Iteration: 1650 Loss: nan\n",
      "Iteration: 1651 Loss: nan\n",
      "Iteration: 1652 Loss: nan\n",
      "Iteration: 1653 Loss: nan\n",
      "Iteration: 1654 Loss: nan\n",
      "Iteration: 1655 Loss: nan\n",
      "Iteration: 1656 Loss: nan\n",
      "Iteration: 1657 Loss: nan\n",
      "Iteration: 1658 Loss: nan\n",
      "Iteration: 1659 Loss: nan\n",
      "Iteration: 1660 Loss: nan\n",
      "Iteration: 1661 Loss: nan\n",
      "Iteration: 1662 Loss: nan\n",
      "Iteration: 1663 Loss: nan\n",
      "Iteration: 1664 Loss: nan\n",
      "Iteration: 1665 Loss: nan\n",
      "Iteration: 1666 Loss: nan\n",
      "Iteration: 1667 Loss: nan\n",
      "Iteration: 1668 Loss: nan\n",
      "Iteration: 1669 Loss: nan\n",
      "Iteration: 1670 Loss: nan\n",
      "Iteration: 1671 Loss: nan\n",
      "Iteration: 1672 Loss: nan\n",
      "Iteration: 1673 Loss: nan\n",
      "Iteration: 1674 Loss: nan\n",
      "Iteration: 1675 Loss: nan\n",
      "Iteration: 1676 Loss: nan\n",
      "Iteration: 1677 Loss: nan\n",
      "Iteration: 1678 Loss: nan\n",
      "Iteration: 1679 Loss: nan\n",
      "Iteration: 1680 Loss: nan\n",
      "Iteration: 1681 Loss: nan\n",
      "Iteration: 1682 Loss: nan\n",
      "Iteration: 1683 Loss: nan\n",
      "Iteration: 1684 Loss: nan\n",
      "Iteration: 1685 Loss: nan\n",
      "Iteration: 1686 Loss: nan\n",
      "Iteration: 1687 Loss: nan\n",
      "Iteration: 1688 Loss: nan\n",
      "Iteration: 1689 Loss: nan\n",
      "Iteration: 1690 Loss: nan\n",
      "Iteration: 1691 Loss: nan\n",
      "Iteration: 1692 Loss: nan\n",
      "Iteration: 1693 Loss: nan\n",
      "Iteration: 1694 Loss: nan\n",
      "Iteration: 1695 Loss: nan\n",
      "Iteration: 1696 Loss: nan\n",
      "Iteration: 1697 Loss: nan\n",
      "Iteration: 1698 Loss: nan\n",
      "Iteration: 1699 Loss: nan\n",
      "Iteration: 1700 Loss: nan\n",
      "Iteration: 1701 Loss: nan\n",
      "Iteration: 1702 Loss: nan\n",
      "Iteration: 1703 Loss: nan\n",
      "Iteration: 1704 Loss: nan\n",
      "Iteration: 1705 Loss: nan\n",
      "Iteration: 1706 Loss: nan\n",
      "Iteration: 1707 Loss: nan\n",
      "Iteration: 1708 Loss: nan\n",
      "Iteration: 1709 Loss: nan\n",
      "Iteration: 1710 Loss: nan\n",
      "Iteration: 1711 Loss: nan\n",
      "Iteration: 1712 Loss: nan\n",
      "Iteration: 1713 Loss: nan\n",
      "Iteration: 1714 Loss: nan\n",
      "Iteration: 1715 Loss: nan\n",
      "Iteration: 1716 Loss: nan\n",
      "Iteration: 1717 Loss: nan\n",
      "Iteration: 1718 Loss: nan\n",
      "Iteration: 1719 Loss: nan\n",
      "Iteration: 1720 Loss: nan\n",
      "Iteration: 1721 Loss: nan\n",
      "Iteration: 1722 Loss: nan\n",
      "Iteration: 1723 Loss: nan\n",
      "Iteration: 1724 Loss: nan\n",
      "Iteration: 1725 Loss: nan\n",
      "Iteration: 1726 Loss: nan\n",
      "Iteration: 1727 Loss: nan\n",
      "Iteration: 1728 Loss: nan\n",
      "Iteration: 1729 Loss: nan\n",
      "Iteration: 1730 Loss: nan\n",
      "Iteration: 1731 Loss: nan\n",
      "Iteration: 1732 Loss: nan\n",
      "Iteration: 1733 Loss: nan\n",
      "Iteration: 1734 Loss: nan\n",
      "Iteration: 1735 Loss: nan\n",
      "Iteration: 1736 Loss: nan\n",
      "Iteration: 1737 Loss: nan\n",
      "Iteration: 1738 Loss: nan\n",
      "Iteration: 1739 Loss: nan\n",
      "Iteration: 1740 Loss: nan\n",
      "Iteration: 1741 Loss: nan\n",
      "Iteration: 1742 Loss: nan\n",
      "Iteration: 1743 Loss: nan\n",
      "Iteration: 1744 Loss: nan\n",
      "Iteration: 1745 Loss: nan\n",
      "Iteration: 1746 Loss: nan\n",
      "Iteration: 1747 Loss: nan\n",
      "Iteration: 1748 Loss: nan\n",
      "Iteration: 1749 Loss: nan\n",
      "Iteration: 1750 Loss: nan\n",
      "Iteration: 1751 Loss: nan\n",
      "Iteration: 1752 Loss: nan\n",
      "Iteration: 1753 Loss: nan\n",
      "Iteration: 1754 Loss: nan\n",
      "Iteration: 1755 Loss: nan\n",
      "Iteration: 1756 Loss: nan\n",
      "Iteration: 1757 Loss: nan\n",
      "Iteration: 1758 Loss: nan\n",
      "Iteration: 1759 Loss: nan\n",
      "Iteration: 1760 Loss: nan\n",
      "Iteration: 1761 Loss: nan\n",
      "Iteration: 1762 Loss: nan\n",
      "Iteration: 1763 Loss: nan\n",
      "Iteration: 1764 Loss: nan\n",
      "Iteration: 1765 Loss: nan\n",
      "Iteration: 1766 Loss: nan\n",
      "Iteration: 1767 Loss: nan\n",
      "Iteration: 1768 Loss: nan\n",
      "Iteration: 1769 Loss: nan\n",
      "Iteration: 1770 Loss: nan\n",
      "Iteration: 1771 Loss: nan\n",
      "Iteration: 1772 Loss: nan\n",
      "Iteration: 1773 Loss: nan\n",
      "Iteration: 1774 Loss: nan\n",
      "Iteration: 1775 Loss: nan\n",
      "Iteration: 1776 Loss: nan\n",
      "Iteration: 1777 Loss: nan\n",
      "Iteration: 1778 Loss: nan\n",
      "Iteration: 1779 Loss: nan\n",
      "Iteration: 1780 Loss: nan\n",
      "Iteration: 1781 Loss: nan\n",
      "Iteration: 1782 Loss: nan\n",
      "Iteration: 1783 Loss: nan\n",
      "Iteration: 1784 Loss: nan\n",
      "Iteration: 1785 Loss: nan\n",
      "Iteration: 1786 Loss: nan\n",
      "Iteration: 1787 Loss: nan\n",
      "Iteration: 1788 Loss: nan\n",
      "Iteration: 1789 Loss: nan\n",
      "Iteration: 1790 Loss: nan\n",
      "Iteration: 1791 Loss: nan\n",
      "Iteration: 1792 Loss: nan\n",
      "Iteration: 1793 Loss: nan\n",
      "Iteration: 1794 Loss: nan\n",
      "Iteration: 1795 Loss: nan\n",
      "Iteration: 1796 Loss: nan\n",
      "Iteration: 1797 Loss: nan\n",
      "Iteration: 1798 Loss: nan\n",
      "Iteration: 1799 Loss: nan\n",
      "Iteration: 1800 Loss: nan\n",
      "Iteration: 1801 Loss: nan\n",
      "Iteration: 1802 Loss: nan\n",
      "Iteration: 1803 Loss: nan\n",
      "Iteration: 1804 Loss: nan\n",
      "Iteration: 1805 Loss: nan\n",
      "Iteration: 1806 Loss: nan\n",
      "Iteration: 1807 Loss: nan\n",
      "Iteration: 1808 Loss: nan\n",
      "Iteration: 1809 Loss: nan\n",
      "Iteration: 1810 Loss: nan\n",
      "Iteration: 1811 Loss: nan\n",
      "Iteration: 1812 Loss: nan\n",
      "Iteration: 1813 Loss: nan\n",
      "Iteration: 1814 Loss: nan\n",
      "Iteration: 1815 Loss: nan\n",
      "Iteration: 1816 Loss: nan\n",
      "Iteration: 1817 Loss: nan\n",
      "Iteration: 1818 Loss: nan\n",
      "Iteration: 1819 Loss: nan\n",
      "Iteration: 1820 Loss: nan\n",
      "Iteration: 1821 Loss: nan\n",
      "Iteration: 1822 Loss: nan\n",
      "Iteration: 1823 Loss: nan\n",
      "Iteration: 1824 Loss: nan\n",
      "Iteration: 1825 Loss: nan\n",
      "Iteration: 1826 Loss: nan\n",
      "Iteration: 1827 Loss: nan\n",
      "Iteration: 1828 Loss: nan\n",
      "Iteration: 1829 Loss: nan\n",
      "Iteration: 1830 Loss: nan\n",
      "Iteration: 1831 Loss: nan\n",
      "Iteration: 1832 Loss: nan\n",
      "Iteration: 1833 Loss: nan\n",
      "Iteration: 1834 Loss: nan\n",
      "Iteration: 1835 Loss: nan\n",
      "Iteration: 1836 Loss: nan\n",
      "Iteration: 1837 Loss: nan\n",
      "Iteration: 1838 Loss: nan\n",
      "Iteration: 1839 Loss: nan\n",
      "Iteration: 1840 Loss: nan\n",
      "Iteration: 1841 Loss: nan\n",
      "Iteration: 1842 Loss: nan\n",
      "Iteration: 1843 Loss: nan\n",
      "Iteration: 1844 Loss: nan\n",
      "Iteration: 1845 Loss: nan\n",
      "Iteration: 1846 Loss: nan\n",
      "Iteration: 1847 Loss: nan\n",
      "Iteration: 1848 Loss: nan\n",
      "Iteration: 1849 Loss: nan\n",
      "Iteration: 1850 Loss: nan\n",
      "Iteration: 1851 Loss: nan\n",
      "Iteration: 1852 Loss: nan\n",
      "Iteration: 1853 Loss: nan\n",
      "Iteration: 1854 Loss: nan\n",
      "Iteration: 1855 Loss: nan\n",
      "Iteration: 1856 Loss: nan\n",
      "Iteration: 1857 Loss: nan\n",
      "Iteration: 1858 Loss: nan\n",
      "Iteration: 1859 Loss: nan\n",
      "Iteration: 1860 Loss: nan\n",
      "Iteration: 1861 Loss: nan\n",
      "Iteration: 1862 Loss: nan\n",
      "Iteration: 1863 Loss: nan\n",
      "Iteration: 1864 Loss: nan\n",
      "Iteration: 1865 Loss: nan\n",
      "Iteration: 1866 Loss: nan\n",
      "Iteration: 1867 Loss: nan\n",
      "Iteration: 1868 Loss: nan\n",
      "Iteration: 1869 Loss: nan\n",
      "Iteration: 1870 Loss: nan\n",
      "Iteration: 1871 Loss: nan\n",
      "Iteration: 1872 Loss: nan\n",
      "Iteration: 1873 Loss: nan\n",
      "Iteration: 1874 Loss: nan\n",
      "Iteration: 1875 Loss: nan\n",
      "Iteration: 1876 Loss: nan\n",
      "Iteration: 1877 Loss: nan\n",
      "Iteration: 1878 Loss: nan\n",
      "Iteration: 1879 Loss: nan\n",
      "Iteration: 1880 Loss: nan\n",
      "Iteration: 1881 Loss: nan\n",
      "Iteration: 1882 Loss: nan\n",
      "Iteration: 1883 Loss: nan\n",
      "Iteration: 1884 Loss: nan\n",
      "Iteration: 1885 Loss: nan\n",
      "Iteration: 1886 Loss: nan\n",
      "Iteration: 1887 Loss: nan\n",
      "Iteration: 1888 Loss: nan\n",
      "Iteration: 1889 Loss: nan\n",
      "Iteration: 1890 Loss: nan\n",
      "Iteration: 1891 Loss: nan\n",
      "Iteration: 1892 Loss: nan\n",
      "Iteration: 1893 Loss: nan\n",
      "Iteration: 1894 Loss: nan\n",
      "Iteration: 1895 Loss: nan\n",
      "Iteration: 1896 Loss: nan\n",
      "Iteration: 1897 Loss: nan\n",
      "Iteration: 1898 Loss: nan\n",
      "Iteration: 1899 Loss: nan\n",
      "Iteration: 1900 Loss: nan\n",
      "Iteration: 1901 Loss: nan\n",
      "Iteration: 1902 Loss: nan\n",
      "Iteration: 1903 Loss: nan\n",
      "Iteration: 1904 Loss: nan\n",
      "Iteration: 1905 Loss: nan\n",
      "Iteration: 1906 Loss: nan\n",
      "Iteration: 1907 Loss: nan\n",
      "Iteration: 1908 Loss: nan\n",
      "Iteration: 1909 Loss: nan\n",
      "Iteration: 1910 Loss: nan\n",
      "Iteration: 1911 Loss: nan\n",
      "Iteration: 1912 Loss: nan\n",
      "Iteration: 1913 Loss: nan\n",
      "Iteration: 1914 Loss: nan\n",
      "Iteration: 1915 Loss: nan\n",
      "Iteration: 1916 Loss: nan\n",
      "Iteration: 1917 Loss: nan\n",
      "Iteration: 1918 Loss: nan\n",
      "Iteration: 1919 Loss: nan\n",
      "Iteration: 1920 Loss: nan\n",
      "Iteration: 1921 Loss: nan\n",
      "Iteration: 1922 Loss: nan\n",
      "Iteration: 1923 Loss: nan\n",
      "Iteration: 1924 Loss: nan\n",
      "Iteration: 1925 Loss: nan\n",
      "Iteration: 1926 Loss: nan\n",
      "Iteration: 1927 Loss: nan\n",
      "Iteration: 1928 Loss: nan\n",
      "Iteration: 1929 Loss: nan\n",
      "Iteration: 1930 Loss: nan\n",
      "Iteration: 1931 Loss: nan\n",
      "Iteration: 1932 Loss: nan\n",
      "Iteration: 1933 Loss: nan\n",
      "Iteration: 1934 Loss: nan\n",
      "Iteration: 1935 Loss: nan\n",
      "Iteration: 1936 Loss: nan\n",
      "Iteration: 1937 Loss: nan\n",
      "Iteration: 1938 Loss: nan\n",
      "Iteration: 1939 Loss: nan\n",
      "Iteration: 1940 Loss: nan\n",
      "Iteration: 1941 Loss: nan\n",
      "Iteration: 1942 Loss: nan\n",
      "Iteration: 1943 Loss: nan\n",
      "Iteration: 1944 Loss: nan\n",
      "Iteration: 1945 Loss: nan\n",
      "Iteration: 1946 Loss: nan\n",
      "Iteration: 1947 Loss: nan\n",
      "Iteration: 1948 Loss: nan\n",
      "Iteration: 1949 Loss: nan\n",
      "Iteration: 1950 Loss: nan\n",
      "Iteration: 1951 Loss: nan\n",
      "Iteration: 1952 Loss: nan\n",
      "Iteration: 1953 Loss: nan\n",
      "Iteration: 1954 Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1955 Loss: nan\n",
      "Iteration: 1956 Loss: nan\n",
      "Iteration: 1957 Loss: nan\n",
      "Iteration: 1958 Loss: nan\n",
      "Iteration: 1959 Loss: nan\n",
      "Iteration: 1960 Loss: nan\n",
      "Iteration: 1961 Loss: nan\n",
      "Iteration: 1962 Loss: nan\n",
      "Iteration: 1963 Loss: nan\n",
      "Iteration: 1964 Loss: nan\n",
      "Iteration: 1965 Loss: nan\n",
      "Iteration: 1966 Loss: nan\n",
      "Iteration: 1967 Loss: nan\n",
      "Iteration: 1968 Loss: nan\n",
      "Iteration: 1969 Loss: nan\n",
      "Iteration: 1970 Loss: nan\n",
      "Iteration: 1971 Loss: nan\n",
      "Iteration: 1972 Loss: nan\n",
      "Iteration: 1973 Loss: nan\n",
      "Iteration: 1974 Loss: nan\n",
      "Iteration: 1975 Loss: nan\n",
      "Iteration: 1976 Loss: nan\n",
      "Iteration: 1977 Loss: nan\n",
      "Iteration: 1978 Loss: nan\n",
      "Iteration: 1979 Loss: nan\n",
      "Iteration: 1980 Loss: nan\n",
      "Iteration: 1981 Loss: nan\n",
      "Iteration: 1982 Loss: nan\n",
      "Iteration: 1983 Loss: nan\n",
      "Iteration: 1984 Loss: nan\n",
      "Iteration: 1985 Loss: nan\n",
      "Iteration: 1986 Loss: nan\n",
      "Iteration: 1987 Loss: nan\n",
      "Iteration: 1988 Loss: nan\n",
      "Iteration: 1989 Loss: nan\n",
      "Iteration: 1990 Loss: nan\n",
      "Iteration: 1991 Loss: nan\n",
      "Iteration: 1992 Loss: nan\n",
      "Iteration: 1993 Loss: nan\n",
      "Iteration: 1994 Loss: nan\n",
      "Iteration: 1995 Loss: nan\n",
      "Iteration: 1996 Loss: nan\n",
      "Iteration: 1997 Loss: nan\n",
      "Iteration: 1998 Loss: nan\n",
      "Iteration: 1999 Loss: nan\n",
      "Iteration: 2000 Loss: nan\n",
      "Iteration: 2001 Loss: nan\n",
      "Iteration: 2002 Loss: nan\n",
      "Iteration: 2003 Loss: nan\n",
      "Iteration: 2004 Loss: nan\n",
      "Iteration: 2005 Loss: nan\n",
      "Iteration: 2006 Loss: nan\n",
      "Iteration: 2007 Loss: nan\n",
      "Iteration: 2008 Loss: nan\n",
      "Iteration: 2009 Loss: nan\n",
      "Iteration: 2010 Loss: nan\n",
      "Iteration: 2011 Loss: nan\n",
      "Iteration: 2012 Loss: nan\n",
      "Iteration: 2013 Loss: nan\n",
      "Iteration: 2014 Loss: nan\n",
      "Iteration: 2015 Loss: nan\n",
      "Iteration: 2016 Loss: nan\n",
      "Iteration: 2017 Loss: nan\n",
      "Iteration: 2018 Loss: nan\n",
      "Iteration: 2019 Loss: nan\n",
      "Iteration: 2020 Loss: nan\n",
      "Iteration: 2021 Loss: nan\n",
      "Iteration: 2022 Loss: nan\n",
      "Iteration: 2023 Loss: nan\n",
      "Iteration: 2024 Loss: nan\n",
      "Iteration: 2025 Loss: nan\n",
      "Iteration: 2026 Loss: nan\n",
      "Iteration: 2027 Loss: nan\n",
      "Iteration: 2028 Loss: nan\n",
      "Iteration: 2029 Loss: nan\n",
      "Iteration: 2030 Loss: nan\n",
      "Iteration: 2031 Loss: nan\n",
      "Iteration: 2032 Loss: nan\n",
      "Iteration: 2033 Loss: nan\n",
      "Iteration: 2034 Loss: nan\n",
      "Iteration: 2035 Loss: nan\n",
      "Iteration: 2036 Loss: nan\n",
      "Iteration: 2037 Loss: nan\n",
      "Iteration: 2038 Loss: nan\n",
      "Iteration: 2039 Loss: nan\n",
      "Iteration: 2040 Loss: nan\n",
      "Iteration: 2041 Loss: nan\n",
      "Iteration: 2042 Loss: nan\n",
      "Iteration: 2043 Loss: nan\n",
      "Iteration: 2044 Loss: nan\n",
      "Iteration: 2045 Loss: nan\n",
      "Iteration: 2046 Loss: nan\n",
      "Iteration: 2047 Loss: nan\n",
      "Iteration: 2048 Loss: nan\n",
      "Iteration: 2049 Loss: nan\n",
      "Iteration: 2050 Loss: nan\n",
      "Iteration: 2051 Loss: nan\n",
      "Iteration: 2052 Loss: nan\n",
      "Iteration: 2053 Loss: nan\n",
      "Iteration: 2054 Loss: nan\n",
      "Iteration: 2055 Loss: nan\n",
      "Iteration: 2056 Loss: nan\n",
      "Iteration: 2057 Loss: nan\n",
      "Iteration: 2058 Loss: nan\n",
      "Iteration: 2059 Loss: nan\n",
      "Iteration: 2060 Loss: nan\n",
      "Iteration: 2061 Loss: nan\n",
      "Iteration: 2062 Loss: nan\n",
      "Iteration: 2063 Loss: nan\n",
      "Iteration: 2064 Loss: nan\n",
      "Iteration: 2065 Loss: nan\n",
      "Iteration: 2066 Loss: nan\n",
      "Iteration: 2067 Loss: nan\n",
      "Iteration: 2068 Loss: nan\n",
      "Iteration: 2069 Loss: nan\n",
      "Iteration: 2070 Loss: nan\n",
      "Iteration: 2071 Loss: nan\n",
      "Iteration: 2072 Loss: nan\n",
      "Iteration: 2073 Loss: nan\n",
      "Iteration: 2074 Loss: nan\n",
      "Iteration: 2075 Loss: nan\n",
      "Iteration: 2076 Loss: nan\n",
      "Iteration: 2077 Loss: nan\n",
      "Iteration: 2078 Loss: nan\n",
      "Iteration: 2079 Loss: nan\n",
      "Iteration: 2080 Loss: nan\n",
      "Iteration: 2081 Loss: nan\n",
      "Iteration: 2082 Loss: nan\n",
      "Iteration: 2083 Loss: nan\n",
      "Iteration: 2084 Loss: nan\n",
      "Iteration: 2085 Loss: nan\n",
      "Iteration: 2086 Loss: nan\n",
      "Iteration: 2087 Loss: nan\n",
      "Iteration: 2088 Loss: nan\n",
      "Iteration: 2089 Loss: nan\n",
      "Iteration: 2090 Loss: nan\n",
      "Iteration: 2091 Loss: nan\n",
      "Iteration: 2092 Loss: nan\n",
      "Iteration: 2093 Loss: nan\n",
      "Iteration: 2094 Loss: nan\n",
      "Iteration: 2095 Loss: nan\n",
      "Iteration: 2096 Loss: nan\n",
      "Iteration: 2097 Loss: nan\n",
      "Iteration: 2098 Loss: nan\n",
      "Iteration: 2099 Loss: nan\n",
      "Iteration: 2100 Loss: nan\n",
      "Iteration: 2101 Loss: nan\n",
      "Iteration: 2102 Loss: nan\n",
      "Iteration: 2103 Loss: nan\n",
      "Iteration: 2104 Loss: nan\n",
      "Iteration: 2105 Loss: nan\n",
      "Iteration: 2106 Loss: nan\n",
      "Iteration: 2107 Loss: nan\n",
      "Iteration: 2108 Loss: nan\n",
      "Iteration: 2109 Loss: nan\n",
      "Iteration: 2110 Loss: nan\n",
      "Iteration: 2111 Loss: nan\n",
      "Iteration: 2112 Loss: nan\n",
      "Iteration: 2113 Loss: nan\n",
      "Iteration: 2114 Loss: nan\n",
      "Iteration: 2115 Loss: nan\n",
      "Iteration: 2116 Loss: nan\n",
      "Iteration: 2117 Loss: nan\n",
      "Iteration: 2118 Loss: nan\n",
      "Iteration: 2119 Loss: nan\n",
      "Iteration: 2120 Loss: nan\n",
      "Iteration: 2121 Loss: nan\n",
      "Iteration: 2122 Loss: nan\n",
      "Iteration: 2123 Loss: nan\n",
      "Iteration: 2124 Loss: nan\n",
      "Iteration: 2125 Loss: nan\n",
      "Iteration: 2126 Loss: nan\n",
      "Iteration: 2127 Loss: nan\n",
      "Iteration: 2128 Loss: nan\n",
      "Iteration: 2129 Loss: nan\n",
      "Iteration: 2130 Loss: nan\n",
      "Iteration: 2131 Loss: nan\n",
      "Iteration: 2132 Loss: nan\n",
      "Iteration: 2133 Loss: nan\n",
      "Iteration: 2134 Loss: nan\n",
      "Iteration: 2135 Loss: nan\n",
      "Iteration: 2136 Loss: nan\n",
      "Iteration: 2137 Loss: nan\n",
      "Iteration: 2138 Loss: nan\n",
      "Iteration: 2139 Loss: nan\n",
      "Iteration: 2140 Loss: nan\n",
      "Iteration: 2141 Loss: nan\n",
      "Iteration: 2142 Loss: nan\n",
      "Iteration: 2143 Loss: nan\n",
      "Iteration: 2144 Loss: nan\n",
      "Iteration: 2145 Loss: nan\n",
      "Iteration: 2146 Loss: nan\n",
      "Iteration: 2147 Loss: nan\n",
      "Iteration: 2148 Loss: nan\n",
      "Iteration: 2149 Loss: nan\n",
      "Iteration: 2150 Loss: nan\n",
      "Iteration: 2151 Loss: nan\n",
      "Iteration: 2152 Loss: nan\n",
      "Iteration: 2153 Loss: nan\n",
      "Iteration: 2154 Loss: nan\n",
      "Iteration: 2155 Loss: nan\n",
      "Iteration: 2156 Loss: nan\n",
      "Iteration: 2157 Loss: nan\n",
      "Iteration: 2158 Loss: nan\n",
      "Iteration: 2159 Loss: nan\n",
      "Iteration: 2160 Loss: nan\n",
      "Iteration: 2161 Loss: nan\n",
      "Iteration: 2162 Loss: nan\n",
      "Iteration: 2163 Loss: nan\n",
      "Iteration: 2164 Loss: nan\n",
      "Iteration: 2165 Loss: nan\n",
      "Iteration: 2166 Loss: nan\n",
      "Iteration: 2167 Loss: nan\n",
      "Iteration: 2168 Loss: nan\n",
      "Iteration: 2169 Loss: nan\n",
      "Iteration: 2170 Loss: nan\n",
      "Iteration: 2171 Loss: nan\n",
      "Iteration: 2172 Loss: nan\n",
      "Iteration: 2173 Loss: nan\n",
      "Iteration: 2174 Loss: nan\n",
      "Iteration: 2175 Loss: nan\n",
      "Iteration: 2176 Loss: nan\n",
      "Iteration: 2177 Loss: nan\n",
      "Iteration: 2178 Loss: nan\n",
      "Iteration: 2179 Loss: nan\n",
      "Iteration: 2180 Loss: nan\n",
      "Iteration: 2181 Loss: nan\n",
      "Iteration: 2182 Loss: nan\n",
      "Iteration: 2183 Loss: nan\n",
      "Iteration: 2184 Loss: nan\n",
      "Iteration: 2185 Loss: nan\n",
      "Iteration: 2186 Loss: nan\n",
      "Iteration: 2187 Loss: nan\n",
      "Iteration: 2188 Loss: nan\n",
      "Iteration: 2189 Loss: nan\n",
      "Iteration: 2190 Loss: nan\n",
      "Iteration: 2191 Loss: nan\n",
      "Iteration: 2192 Loss: nan\n",
      "Iteration: 2193 Loss: nan\n",
      "Iteration: 2194 Loss: nan\n",
      "Iteration: 2195 Loss: nan\n",
      "Iteration: 2196 Loss: nan\n",
      "Iteration: 2197 Loss: nan\n",
      "Iteration: 2198 Loss: nan\n",
      "Iteration: 2199 Loss: nan\n",
      "Iteration: 2200 Loss: nan\n",
      "Iteration: 2201 Loss: nan\n",
      "Iteration: 2202 Loss: nan\n",
      "Iteration: 2203 Loss: nan\n",
      "Iteration: 2204 Loss: nan\n",
      "Iteration: 2205 Loss: nan\n",
      "Iteration: 2206 Loss: nan\n",
      "Iteration: 2207 Loss: nan\n",
      "Iteration: 2208 Loss: nan\n",
      "Iteration: 2209 Loss: nan\n",
      "Iteration: 2210 Loss: nan\n",
      "Iteration: 2211 Loss: nan\n",
      "Iteration: 2212 Loss: nan\n",
      "Iteration: 2213 Loss: nan\n",
      "Iteration: 2214 Loss: nan\n",
      "Iteration: 2215 Loss: nan\n",
      "Iteration: 2216 Loss: nan\n",
      "Iteration: 2217 Loss: nan\n",
      "Iteration: 2218 Loss: nan\n",
      "Iteration: 2219 Loss: nan\n",
      "Iteration: 2220 Loss: nan\n",
      "Iteration: 2221 Loss: nan\n",
      "Iteration: 2222 Loss: nan\n",
      "Iteration: 2223 Loss: nan\n",
      "Iteration: 2224 Loss: nan\n",
      "Iteration: 2225 Loss: nan\n",
      "Iteration: 2226 Loss: nan\n",
      "Iteration: 2227 Loss: nan\n",
      "Iteration: 2228 Loss: nan\n",
      "Iteration: 2229 Loss: nan\n",
      "Iteration: 2230 Loss: nan\n",
      "Iteration: 2231 Loss: nan\n",
      "Iteration: 2232 Loss: nan\n",
      "Iteration: 2233 Loss: nan\n",
      "Iteration: 2234 Loss: nan\n",
      "Iteration: 2235 Loss: nan\n",
      "Iteration: 2236 Loss: nan\n",
      "Iteration: 2237 Loss: nan\n",
      "Iteration: 2238 Loss: nan\n",
      "Iteration: 2239 Loss: nan\n",
      "Iteration: 2240 Loss: nan\n",
      "Iteration: 2241 Loss: nan\n",
      "Iteration: 2242 Loss: nan\n",
      "Iteration: 2243 Loss: nan\n",
      "Iteration: 2244 Loss: nan\n",
      "Iteration: 2245 Loss: nan\n",
      "Iteration: 2246 Loss: nan\n",
      "Iteration: 2247 Loss: nan\n",
      "Iteration: 2248 Loss: nan\n",
      "Iteration: 2249 Loss: nan\n",
      "Iteration: 2250 Loss: nan\n",
      "Iteration: 2251 Loss: nan\n",
      "Iteration: 2252 Loss: nan\n",
      "Iteration: 2253 Loss: nan\n",
      "Iteration: 2254 Loss: nan\n",
      "Iteration: 2255 Loss: nan\n",
      "Iteration: 2256 Loss: nan\n",
      "Iteration: 2257 Loss: nan\n",
      "Iteration: 2258 Loss: nan\n",
      "Iteration: 2259 Loss: nan\n",
      "Iteration: 2260 Loss: nan\n",
      "Iteration: 2261 Loss: nan\n",
      "Iteration: 2262 Loss: nan\n",
      "Iteration: 2263 Loss: nan\n",
      "Iteration: 2264 Loss: nan\n",
      "Iteration: 2265 Loss: nan\n",
      "Iteration: 2266 Loss: nan\n",
      "Iteration: 2267 Loss: nan\n",
      "Iteration: 2268 Loss: nan\n",
      "Iteration: 2269 Loss: nan\n",
      "Iteration: 2270 Loss: nan\n",
      "Iteration: 2271 Loss: nan\n",
      "Iteration: 2272 Loss: nan\n",
      "Iteration: 2273 Loss: nan\n",
      "Iteration: 2274 Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2275 Loss: nan\n",
      "Iteration: 2276 Loss: nan\n",
      "Iteration: 2277 Loss: nan\n",
      "Iteration: 2278 Loss: nan\n",
      "Iteration: 2279 Loss: nan\n",
      "Iteration: 2280 Loss: nan\n",
      "Iteration: 2281 Loss: nan\n",
      "Iteration: 2282 Loss: nan\n",
      "Iteration: 2283 Loss: nan\n",
      "Iteration: 2284 Loss: nan\n",
      "Iteration: 2285 Loss: nan\n",
      "Iteration: 2286 Loss: nan\n",
      "Iteration: 2287 Loss: nan\n",
      "Iteration: 2288 Loss: nan\n",
      "Iteration: 2289 Loss: nan\n",
      "Iteration: 2290 Loss: nan\n",
      "Iteration: 2291 Loss: nan\n",
      "Iteration: 2292 Loss: nan\n",
      "Iteration: 2293 Loss: nan\n",
      "Iteration: 2294 Loss: nan\n",
      "Iteration: 2295 Loss: nan\n",
      "Iteration: 2296 Loss: nan\n",
      "Iteration: 2297 Loss: nan\n",
      "Iteration: 2298 Loss: nan\n",
      "Iteration: 2299 Loss: nan\n",
      "Iteration: 2300 Loss: nan\n",
      "Iteration: 2301 Loss: nan\n",
      "Iteration: 2302 Loss: nan\n",
      "Iteration: 2303 Loss: nan\n",
      "Iteration: 2304 Loss: nan\n",
      "Iteration: 2305 Loss: nan\n",
      "Iteration: 2306 Loss: nan\n",
      "Iteration: 2307 Loss: nan\n",
      "Iteration: 2308 Loss: nan\n",
      "Iteration: 2309 Loss: nan\n",
      "Iteration: 2310 Loss: nan\n",
      "Iteration: 2311 Loss: nan\n",
      "Iteration: 2312 Loss: nan\n",
      "Iteration: 2313 Loss: nan\n",
      "Iteration: 2314 Loss: nan\n",
      "Iteration: 2315 Loss: nan\n",
      "Iteration: 2316 Loss: nan\n",
      "Iteration: 2317 Loss: nan\n",
      "Iteration: 2318 Loss: nan\n",
      "Iteration: 2319 Loss: nan\n",
      "Iteration: 2320 Loss: nan\n",
      "Iteration: 2321 Loss: nan\n",
      "Iteration: 2322 Loss: nan\n",
      "Iteration: 2323 Loss: nan\n",
      "Iteration: 2324 Loss: nan\n",
      "Iteration: 2325 Loss: nan\n",
      "Iteration: 2326 Loss: nan\n",
      "Iteration: 2327 Loss: nan\n",
      "Iteration: 2328 Loss: nan\n",
      "Iteration: 2329 Loss: nan\n",
      "Iteration: 2330 Loss: nan\n",
      "Iteration: 2331 Loss: nan\n",
      "Iteration: 2332 Loss: nan\n",
      "Iteration: 2333 Loss: nan\n",
      "Iteration: 2334 Loss: nan\n",
      "Iteration: 2335 Loss: nan\n",
      "Iteration: 2336 Loss: nan\n",
      "Iteration: 2337 Loss: nan\n",
      "Iteration: 2338 Loss: nan\n",
      "Iteration: 2339 Loss: nan\n",
      "Iteration: 2340 Loss: nan\n",
      "Iteration: 2341 Loss: nan\n",
      "Iteration: 2342 Loss: nan\n",
      "Iteration: 2343 Loss: nan\n",
      "Iteration: 2344 Loss: nan\n",
      "Iteration: 2345 Loss: nan\n",
      "Iteration: 2346 Loss: nan\n",
      "Iteration: 2347 Loss: nan\n",
      "Iteration: 2348 Loss: nan\n",
      "Iteration: 2349 Loss: nan\n",
      "Iteration: 2350 Loss: nan\n",
      "Iteration: 2351 Loss: nan\n",
      "Iteration: 2352 Loss: nan\n",
      "Iteration: 2353 Loss: nan\n",
      "Iteration: 2354 Loss: nan\n",
      "Iteration: 2355 Loss: nan\n",
      "Iteration: 2356 Loss: nan\n",
      "Iteration: 2357 Loss: nan\n",
      "Iteration: 2358 Loss: nan\n",
      "Iteration: 2359 Loss: nan\n",
      "Iteration: 2360 Loss: nan\n",
      "Iteration: 2361 Loss: nan\n",
      "Iteration: 2362 Loss: nan\n",
      "Iteration: 2363 Loss: nan\n",
      "Iteration: 2364 Loss: nan\n",
      "Iteration: 2365 Loss: nan\n",
      "Iteration: 2366 Loss: nan\n",
      "Iteration: 2367 Loss: nan\n",
      "Iteration: 2368 Loss: nan\n",
      "Iteration: 2369 Loss: nan\n",
      "Iteration: 2370 Loss: nan\n",
      "Iteration: 2371 Loss: nan\n",
      "Iteration: 2372 Loss: nan\n",
      "Iteration: 2373 Loss: nan\n",
      "Iteration: 2374 Loss: nan\n",
      "Iteration: 2375 Loss: nan\n",
      "Iteration: 2376 Loss: nan\n",
      "Iteration: 2377 Loss: nan\n",
      "Iteration: 2378 Loss: nan\n",
      "Iteration: 2379 Loss: nan\n",
      "Iteration: 2380 Loss: nan\n",
      "Iteration: 2381 Loss: nan\n",
      "Iteration: 2382 Loss: nan\n",
      "Iteration: 2383 Loss: nan\n",
      "Iteration: 2384 Loss: nan\n",
      "Iteration: 2385 Loss: nan\n",
      "Iteration: 2386 Loss: nan\n",
      "Iteration: 2387 Loss: nan\n",
      "Iteration: 2388 Loss: nan\n",
      "Iteration: 2389 Loss: nan\n",
      "Iteration: 2390 Loss: nan\n",
      "Iteration: 2391 Loss: nan\n",
      "Iteration: 2392 Loss: nan\n",
      "Iteration: 2393 Loss: nan\n",
      "Iteration: 2394 Loss: nan\n",
      "Iteration: 2395 Loss: nan\n",
      "Iteration: 2396 Loss: nan\n",
      "Iteration: 2397 Loss: nan\n",
      "Iteration: 2398 Loss: nan\n",
      "Iteration: 2399 Loss: nan\n",
      "Iteration: 2400 Loss: nan\n",
      "Iteration: 2401 Loss: nan\n",
      "Iteration: 2402 Loss: nan\n",
      "Iteration: 2403 Loss: nan\n",
      "Iteration: 2404 Loss: nan\n",
      "Iteration: 2405 Loss: nan\n",
      "Iteration: 2406 Loss: nan\n",
      "Iteration: 2407 Loss: nan\n",
      "Iteration: 2408 Loss: nan\n",
      "Iteration: 2409 Loss: nan\n",
      "Iteration: 2410 Loss: nan\n",
      "Iteration: 2411 Loss: nan\n",
      "Iteration: 2412 Loss: nan\n",
      "Iteration: 2413 Loss: nan\n",
      "Iteration: 2414 Loss: nan\n",
      "Iteration: 2415 Loss: nan\n",
      "Iteration: 2416 Loss: nan\n",
      "Iteration: 2417 Loss: nan\n",
      "Iteration: 2418 Loss: nan\n",
      "Iteration: 2419 Loss: nan\n",
      "Iteration: 2420 Loss: nan\n",
      "Iteration: 2421 Loss: nan\n",
      "Iteration: 2422 Loss: nan\n",
      "Iteration: 2423 Loss: nan\n",
      "Iteration: 2424 Loss: nan\n",
      "Iteration: 2425 Loss: nan\n",
      "Iteration: 2426 Loss: nan\n",
      "Iteration: 2427 Loss: nan\n",
      "Iteration: 2428 Loss: nan\n",
      "Iteration: 2429 Loss: nan\n",
      "Iteration: 2430 Loss: nan\n",
      "Iteration: 2431 Loss: nan\n",
      "Iteration: 2432 Loss: nan\n",
      "Iteration: 2433 Loss: nan\n",
      "Iteration: 2434 Loss: nan\n",
      "Iteration: 2435 Loss: nan\n",
      "Iteration: 2436 Loss: nan\n",
      "Iteration: 2437 Loss: nan\n",
      "Iteration: 2438 Loss: nan\n",
      "Iteration: 2439 Loss: nan\n",
      "Iteration: 2440 Loss: nan\n",
      "Iteration: 2441 Loss: nan\n",
      "Iteration: 2442 Loss: nan\n",
      "Iteration: 2443 Loss: nan\n",
      "Iteration: 2444 Loss: nan\n",
      "Iteration: 2445 Loss: nan\n",
      "Iteration: 2446 Loss: nan\n",
      "Iteration: 2447 Loss: nan\n",
      "Iteration: 2448 Loss: nan\n",
      "Iteration: 2449 Loss: nan\n",
      "Iteration: 2450 Loss: nan\n",
      "Iteration: 2451 Loss: nan\n",
      "Iteration: 2452 Loss: nan\n",
      "Iteration: 2453 Loss: nan\n",
      "Iteration: 2454 Loss: nan\n",
      "Iteration: 2455 Loss: nan\n",
      "Iteration: 2456 Loss: nan\n",
      "Iteration: 2457 Loss: nan\n",
      "Iteration: 2458 Loss: nan\n",
      "Iteration: 2459 Loss: nan\n",
      "Iteration: 2460 Loss: nan\n",
      "Iteration: 2461 Loss: nan\n",
      "Iteration: 2462 Loss: nan\n",
      "Iteration: 2463 Loss: nan\n",
      "Iteration: 2464 Loss: nan\n",
      "Iteration: 2465 Loss: nan\n",
      "Iteration: 2466 Loss: nan\n",
      "Iteration: 2467 Loss: nan\n",
      "Iteration: 2468 Loss: nan\n",
      "Iteration: 2469 Loss: nan\n",
      "Iteration: 2470 Loss: nan\n",
      "Iteration: 2471 Loss: nan\n",
      "Iteration: 2472 Loss: nan\n",
      "Iteration: 2473 Loss: nan\n",
      "Iteration: 2474 Loss: nan\n",
      "Iteration: 2475 Loss: nan\n",
      "Iteration: 2476 Loss: nan\n",
      "Iteration: 2477 Loss: nan\n",
      "Iteration: 2478 Loss: nan\n",
      "Iteration: 2479 Loss: nan\n",
      "Iteration: 2480 Loss: nan\n",
      "Iteration: 2481 Loss: nan\n",
      "Iteration: 2482 Loss: nan\n",
      "Iteration: 2483 Loss: nan\n",
      "Iteration: 2484 Loss: nan\n",
      "Iteration: 2485 Loss: nan\n",
      "Iteration: 2486 Loss: nan\n",
      "Iteration: 2487 Loss: nan\n",
      "Iteration: 2488 Loss: nan\n",
      "Iteration: 2489 Loss: nan\n",
      "Iteration: 2490 Loss: nan\n",
      "Iteration: 2491 Loss: nan\n",
      "Iteration: 2492 Loss: nan\n",
      "Iteration: 2493 Loss: nan\n",
      "Iteration: 2494 Loss: nan\n",
      "Iteration: 2495 Loss: nan\n",
      "Iteration: 2496 Loss: nan\n",
      "Iteration: 2497 Loss: nan\n",
      "Iteration: 2498 Loss: nan\n",
      "Iteration: 2499 Loss: nan\n",
      "Iteration: 2500 Loss: nan\n",
      "Iteration: 2501 Loss: nan\n",
      "Iteration: 2502 Loss: nan\n",
      "Iteration: 2503 Loss: nan\n",
      "Iteration: 2504 Loss: nan\n",
      "Iteration: 2505 Loss: nan\n",
      "Iteration: 2506 Loss: nan\n",
      "Iteration: 2507 Loss: nan\n",
      "Iteration: 2508 Loss: nan\n",
      "Iteration: 2509 Loss: nan\n",
      "Iteration: 2510 Loss: nan\n",
      "Iteration: 2511 Loss: nan\n",
      "Iteration: 2512 Loss: nan\n",
      "Iteration: 2513 Loss: nan\n",
      "Iteration: 2514 Loss: nan\n",
      "Iteration: 2515 Loss: nan\n",
      "Iteration: 2516 Loss: nan\n",
      "Iteration: 2517 Loss: nan\n",
      "Iteration: 2518 Loss: nan\n",
      "Iteration: 2519 Loss: nan\n",
      "Iteration: 2520 Loss: nan\n",
      "Iteration: 2521 Loss: nan\n",
      "Iteration: 2522 Loss: nan\n",
      "Iteration: 2523 Loss: nan\n",
      "Iteration: 2524 Loss: nan\n",
      "Iteration: 2525 Loss: nan\n",
      "Iteration: 2526 Loss: nan\n",
      "Iteration: 2527 Loss: nan\n",
      "Iteration: 2528 Loss: nan\n",
      "Iteration: 2529 Loss: nan\n",
      "Iteration: 2530 Loss: nan\n",
      "Iteration: 2531 Loss: nan\n",
      "Iteration: 2532 Loss: nan\n",
      "Iteration: 2533 Loss: nan\n",
      "Iteration: 2534 Loss: nan\n",
      "Iteration: 2535 Loss: nan\n",
      "Iteration: 2536 Loss: nan\n",
      "Iteration: 2537 Loss: nan\n",
      "Iteration: 2538 Loss: nan\n",
      "Iteration: 2539 Loss: nan\n",
      "Iteration: 2540 Loss: nan\n",
      "Iteration: 2541 Loss: nan\n",
      "Iteration: 2542 Loss: nan\n",
      "Iteration: 2543 Loss: nan\n",
      "Iteration: 2544 Loss: nan\n",
      "Iteration: 2545 Loss: nan\n",
      "Iteration: 2546 Loss: nan\n",
      "Iteration: 2547 Loss: nan\n",
      "Iteration: 2548 Loss: nan\n",
      "Iteration: 2549 Loss: nan\n",
      "Iteration: 2550 Loss: nan\n",
      "Iteration: 2551 Loss: nan\n",
      "Iteration: 2552 Loss: nan\n",
      "Iteration: 2553 Loss: nan\n",
      "Iteration: 2554 Loss: nan\n",
      "Iteration: 2555 Loss: nan\n",
      "Iteration: 2556 Loss: nan\n",
      "Iteration: 2557 Loss: nan\n",
      "Iteration: 2558 Loss: nan\n",
      "Iteration: 2559 Loss: nan\n",
      "Iteration: 2560 Loss: nan\n",
      "Iteration: 2561 Loss: nan\n",
      "Iteration: 2562 Loss: nan\n",
      "Iteration: 2563 Loss: nan\n",
      "Iteration: 2564 Loss: nan\n",
      "Iteration: 2565 Loss: nan\n",
      "Iteration: 2566 Loss: nan\n",
      "Iteration: 2567 Loss: nan\n",
      "Iteration: 2568 Loss: nan\n",
      "Iteration: 2569 Loss: nan\n",
      "Iteration: 2570 Loss: nan\n",
      "Iteration: 2571 Loss: nan\n",
      "Iteration: 2572 Loss: nan\n",
      "Iteration: 2573 Loss: nan\n",
      "Iteration: 2574 Loss: nan\n",
      "Iteration: 2575 Loss: nan\n",
      "Iteration: 2576 Loss: nan\n",
      "Iteration: 2577 Loss: nan\n",
      "Iteration: 2578 Loss: nan\n",
      "Iteration: 2579 Loss: nan\n",
      "Iteration: 2580 Loss: nan\n",
      "Iteration: 2581 Loss: nan\n",
      "Iteration: 2582 Loss: nan\n",
      "Iteration: 2583 Loss: nan\n",
      "Iteration: 2584 Loss: nan\n",
      "Iteration: 2585 Loss: nan\n",
      "Iteration: 2586 Loss: nan\n",
      "Iteration: 2587 Loss: nan\n",
      "Iteration: 2588 Loss: nan\n",
      "Iteration: 2589 Loss: nan\n",
      "Iteration: 2590 Loss: nan\n",
      "Iteration: 2591 Loss: nan\n",
      "Iteration: 2592 Loss: nan\n",
      "Iteration: 2593 Loss: nan\n",
      "Iteration: 2594 Loss: nan\n",
      "Iteration: 2595 Loss: nan\n",
      "Iteration: 2596 Loss: nan\n",
      "Iteration: 2597 Loss: nan\n",
      "Iteration: 2598 Loss: nan\n",
      "Iteration: 2599 Loss: nan\n",
      "Iteration: 2600 Loss: nan\n",
      "Iteration: 2601 Loss: nan\n",
      "Iteration: 2602 Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2603 Loss: nan\n",
      "Iteration: 2604 Loss: nan\n",
      "Iteration: 2605 Loss: nan\n",
      "Iteration: 2606 Loss: nan\n",
      "Iteration: 2607 Loss: nan\n",
      "Iteration: 2608 Loss: nan\n",
      "Iteration: 2609 Loss: nan\n",
      "Iteration: 2610 Loss: nan\n",
      "Iteration: 2611 Loss: nan\n",
      "Iteration: 2612 Loss: nan\n",
      "Iteration: 2613 Loss: nan\n",
      "Iteration: 2614 Loss: nan\n",
      "Iteration: 2615 Loss: nan\n",
      "Iteration: 2616 Loss: nan\n",
      "Iteration: 2617 Loss: nan\n",
      "Iteration: 2618 Loss: nan\n",
      "Iteration: 2619 Loss: nan\n",
      "Iteration: 2620 Loss: nan\n",
      "Iteration: 2621 Loss: nan\n",
      "Iteration: 2622 Loss: nan\n",
      "Iteration: 2623 Loss: nan\n",
      "Iteration: 2624 Loss: nan\n",
      "Iteration: 2625 Loss: nan\n",
      "Iteration: 2626 Loss: nan\n",
      "Iteration: 2627 Loss: nan\n",
      "Iteration: 2628 Loss: nan\n",
      "Iteration: 2629 Loss: nan\n",
      "Iteration: 2630 Loss: nan\n",
      "Iteration: 2631 Loss: nan\n",
      "Iteration: 2632 Loss: nan\n",
      "Iteration: 2633 Loss: nan\n",
      "Iteration: 2634 Loss: nan\n",
      "Iteration: 2635 Loss: nan\n",
      "Iteration: 2636 Loss: nan\n",
      "Iteration: 2637 Loss: nan\n",
      "Iteration: 2638 Loss: nan\n",
      "Iteration: 2639 Loss: nan\n",
      "Iteration: 2640 Loss: nan\n",
      "Iteration: 2641 Loss: nan\n",
      "Iteration: 2642 Loss: nan\n",
      "Iteration: 2643 Loss: nan\n",
      "Iteration: 2644 Loss: nan\n",
      "Iteration: 2645 Loss: nan\n",
      "Iteration: 2646 Loss: nan\n",
      "Iteration: 2647 Loss: nan\n",
      "Iteration: 2648 Loss: nan\n",
      "Iteration: 2649 Loss: nan\n",
      "Iteration: 2650 Loss: nan\n",
      "Iteration: 2651 Loss: nan\n",
      "Iteration: 2652 Loss: nan\n",
      "Iteration: 2653 Loss: nan\n",
      "Iteration: 2654 Loss: nan\n",
      "Iteration: 2655 Loss: nan\n",
      "Iteration: 2656 Loss: nan\n",
      "Iteration: 2657 Loss: nan\n",
      "Iteration: 2658 Loss: nan\n",
      "Iteration: 2659 Loss: nan\n",
      "Iteration: 2660 Loss: nan\n",
      "Iteration: 2661 Loss: nan\n",
      "Iteration: 2662 Loss: nan\n",
      "Iteration: 2663 Loss: nan\n",
      "Iteration: 2664 Loss: nan\n",
      "Iteration: 2665 Loss: nan\n",
      "Iteration: 2666 Loss: nan\n",
      "Iteration: 2667 Loss: nan\n",
      "Iteration: 2668 Loss: nan\n",
      "Iteration: 2669 Loss: nan\n",
      "Iteration: 2670 Loss: nan\n",
      "Iteration: 2671 Loss: nan\n",
      "Iteration: 2672 Loss: nan\n",
      "Iteration: 2673 Loss: nan\n",
      "Iteration: 2674 Loss: nan\n",
      "Iteration: 2675 Loss: nan\n",
      "Iteration: 2676 Loss: nan\n",
      "Iteration: 2677 Loss: nan\n",
      "Iteration: 2678 Loss: nan\n",
      "Iteration: 2679 Loss: nan\n",
      "Iteration: 2680 Loss: nan\n",
      "Iteration: 2681 Loss: nan\n",
      "Iteration: 2682 Loss: nan\n",
      "Iteration: 2683 Loss: nan\n",
      "Iteration: 2684 Loss: nan\n",
      "Iteration: 2685 Loss: nan\n",
      "Iteration: 2686 Loss: nan\n",
      "Iteration: 2687 Loss: nan\n",
      "Iteration: 2688 Loss: nan\n",
      "Iteration: 2689 Loss: nan\n",
      "Iteration: 2690 Loss: nan\n",
      "Iteration: 2691 Loss: nan\n",
      "Iteration: 2692 Loss: nan\n",
      "Iteration: 2693 Loss: nan\n",
      "Iteration: 2694 Loss: nan\n",
      "Iteration: 2695 Loss: nan\n",
      "Iteration: 2696 Loss: nan\n",
      "Iteration: 2697 Loss: nan\n",
      "Iteration: 2698 Loss: nan\n",
      "Iteration: 2699 Loss: nan\n",
      "Iteration: 2700 Loss: nan\n",
      "Iteration: 2701 Loss: nan\n",
      "Iteration: 2702 Loss: nan\n",
      "Iteration: 2703 Loss: nan\n",
      "Iteration: 2704 Loss: nan\n",
      "Iteration: 2705 Loss: nan\n",
      "Iteration: 2706 Loss: nan\n",
      "Iteration: 2707 Loss: nan\n",
      "Iteration: 2708 Loss: nan\n",
      "Iteration: 2709 Loss: nan\n",
      "Iteration: 2710 Loss: nan\n",
      "Iteration: 2711 Loss: nan\n",
      "Iteration: 2712 Loss: nan\n",
      "Iteration: 2713 Loss: nan\n",
      "Iteration: 2714 Loss: nan\n",
      "Iteration: 2715 Loss: nan\n",
      "Iteration: 2716 Loss: nan\n",
      "Iteration: 2717 Loss: nan\n",
      "Iteration: 2718 Loss: nan\n",
      "Iteration: 2719 Loss: nan\n",
      "Iteration: 2720 Loss: nan\n",
      "Iteration: 2721 Loss: nan\n",
      "Iteration: 2722 Loss: nan\n",
      "Iteration: 2723 Loss: nan\n",
      "Iteration: 2724 Loss: nan\n",
      "Iteration: 2725 Loss: nan\n",
      "Iteration: 2726 Loss: nan\n",
      "Iteration: 2727 Loss: nan\n",
      "Iteration: 2728 Loss: nan\n",
      "Iteration: 2729 Loss: nan\n",
      "Iteration: 2730 Loss: nan\n",
      "Iteration: 2731 Loss: nan\n",
      "Iteration: 2732 Loss: nan\n",
      "Iteration: 2733 Loss: nan\n",
      "Iteration: 2734 Loss: nan\n",
      "Iteration: 2735 Loss: nan\n",
      "Iteration: 2736 Loss: nan\n",
      "Iteration: 2737 Loss: nan\n",
      "Iteration: 2738 Loss: nan\n",
      "Iteration: 2739 Loss: nan\n",
      "Iteration: 2740 Loss: nan\n",
      "Iteration: 2741 Loss: nan\n",
      "Iteration: 2742 Loss: nan\n",
      "Iteration: 2743 Loss: nan\n",
      "Iteration: 2744 Loss: nan\n",
      "Iteration: 2745 Loss: nan\n",
      "Iteration: 2746 Loss: nan\n",
      "Iteration: 2747 Loss: nan\n",
      "Iteration: 2748 Loss: nan\n",
      "Iteration: 2749 Loss: nan\n",
      "Iteration: 2750 Loss: nan\n",
      "Iteration: 2751 Loss: nan\n",
      "Iteration: 2752 Loss: nan\n",
      "Iteration: 2753 Loss: nan\n",
      "Iteration: 2754 Loss: nan\n",
      "Iteration: 2755 Loss: nan\n",
      "Iteration: 2756 Loss: nan\n",
      "Iteration: 2757 Loss: nan\n",
      "Iteration: 2758 Loss: nan\n",
      "Iteration: 2759 Loss: nan\n",
      "Iteration: 2760 Loss: nan\n",
      "Iteration: 2761 Loss: nan\n",
      "Iteration: 2762 Loss: nan\n",
      "Iteration: 2763 Loss: nan\n",
      "Iteration: 2764 Loss: nan\n",
      "Iteration: 2765 Loss: nan\n",
      "Iteration: 2766 Loss: nan\n",
      "Iteration: 2767 Loss: nan\n",
      "Iteration: 2768 Loss: nan\n",
      "Iteration: 2769 Loss: nan\n",
      "Iteration: 2770 Loss: nan\n",
      "Iteration: 2771 Loss: nan\n",
      "Iteration: 2772 Loss: nan\n",
      "Iteration: 2773 Loss: nan\n",
      "Iteration: 2774 Loss: nan\n",
      "Iteration: 2775 Loss: nan\n",
      "Iteration: 2776 Loss: nan\n",
      "Iteration: 2777 Loss: nan\n",
      "Iteration: 2778 Loss: nan\n",
      "Iteration: 2779 Loss: nan\n",
      "Iteration: 2780 Loss: nan\n",
      "Iteration: 2781 Loss: nan\n",
      "Iteration: 2782 Loss: nan\n",
      "Iteration: 2783 Loss: nan\n",
      "Iteration: 2784 Loss: nan\n",
      "Iteration: 2785 Loss: nan\n",
      "Iteration: 2786 Loss: nan\n",
      "Iteration: 2787 Loss: nan\n",
      "Iteration: 2788 Loss: nan\n",
      "Iteration: 2789 Loss: nan\n",
      "Iteration: 2790 Loss: nan\n",
      "Iteration: 2791 Loss: nan\n",
      "Iteration: 2792 Loss: nan\n",
      "Iteration: 2793 Loss: nan\n",
      "Iteration: 2794 Loss: nan\n",
      "Iteration: 2795 Loss: nan\n",
      "Iteration: 2796 Loss: nan\n",
      "Iteration: 2797 Loss: nan\n",
      "Iteration: 2798 Loss: nan\n",
      "Iteration: 2799 Loss: nan\n",
      "Iteration: 2800 Loss: nan\n",
      "Iteration: 2801 Loss: nan\n",
      "Iteration: 2802 Loss: nan\n",
      "Iteration: 2803 Loss: nan\n",
      "Iteration: 2804 Loss: nan\n",
      "Iteration: 2805 Loss: nan\n",
      "Iteration: 2806 Loss: nan\n",
      "Iteration: 2807 Loss: nan\n",
      "Iteration: 2808 Loss: nan\n",
      "Iteration: 2809 Loss: nan\n",
      "Iteration: 2810 Loss: nan\n",
      "Iteration: 2811 Loss: nan\n",
      "Iteration: 2812 Loss: nan\n",
      "Iteration: 2813 Loss: nan\n",
      "Iteration: 2814 Loss: nan\n",
      "Iteration: 2815 Loss: nan\n",
      "Iteration: 2816 Loss: nan\n",
      "Iteration: 2817 Loss: nan\n",
      "Iteration: 2818 Loss: nan\n",
      "Iteration: 2819 Loss: nan\n",
      "Iteration: 2820 Loss: nan\n",
      "Iteration: 2821 Loss: nan\n",
      "Iteration: 2822 Loss: nan\n",
      "Iteration: 2823 Loss: nan\n",
      "Iteration: 2824 Loss: nan\n",
      "Iteration: 2825 Loss: nan\n",
      "Iteration: 2826 Loss: nan\n",
      "Iteration: 2827 Loss: nan\n",
      "Iteration: 2828 Loss: nan\n",
      "Iteration: 2829 Loss: nan\n",
      "Iteration: 2830 Loss: nan\n",
      "Iteration: 2831 Loss: nan\n",
      "Iteration: 2832 Loss: nan\n",
      "Iteration: 2833 Loss: nan\n",
      "Iteration: 2834 Loss: nan\n",
      "Iteration: 2835 Loss: nan\n",
      "Iteration: 2836 Loss: nan\n",
      "Iteration: 2837 Loss: nan\n",
      "Iteration: 2838 Loss: nan\n",
      "Iteration: 2839 Loss: nan\n",
      "Iteration: 2840 Loss: nan\n",
      "Iteration: 2841 Loss: nan\n",
      "Iteration: 2842 Loss: nan\n",
      "Iteration: 2843 Loss: nan\n",
      "Iteration: 2844 Loss: nan\n",
      "Iteration: 2845 Loss: nan\n",
      "Iteration: 2846 Loss: nan\n",
      "Iteration: 2847 Loss: nan\n",
      "Iteration: 2848 Loss: nan\n",
      "Iteration: 2849 Loss: nan\n",
      "Iteration: 2850 Loss: nan\n",
      "Iteration: 2851 Loss: nan\n",
      "Iteration: 2852 Loss: nan\n",
      "Iteration: 2853 Loss: nan\n",
      "Iteration: 2854 Loss: nan\n",
      "Iteration: 2855 Loss: nan\n",
      "Iteration: 2856 Loss: nan\n",
      "Iteration: 2857 Loss: nan\n",
      "Iteration: 2858 Loss: nan\n",
      "Iteration: 2859 Loss: nan\n",
      "Iteration: 2860 Loss: nan\n",
      "Iteration: 2861 Loss: nan\n",
      "Iteration: 2862 Loss: nan\n",
      "Iteration: 2863 Loss: nan\n",
      "Iteration: 2864 Loss: nan\n",
      "Iteration: 2865 Loss: nan\n",
      "Iteration: 2866 Loss: nan\n",
      "Iteration: 2867 Loss: nan\n",
      "Iteration: 2868 Loss: nan\n",
      "Iteration: 2869 Loss: nan\n",
      "Iteration: 2870 Loss: nan\n",
      "Iteration: 2871 Loss: nan\n",
      "Iteration: 2872 Loss: nan\n",
      "Iteration: 2873 Loss: nan\n",
      "Iteration: 2874 Loss: nan\n",
      "Iteration: 2875 Loss: nan\n",
      "Iteration: 2876 Loss: nan\n",
      "Iteration: 2877 Loss: nan\n",
      "Iteration: 2878 Loss: nan\n",
      "Iteration: 2879 Loss: nan\n",
      "Iteration: 2880 Loss: nan\n",
      "Iteration: 2881 Loss: nan\n",
      "Iteration: 2882 Loss: nan\n",
      "Iteration: 2883 Loss: nan\n",
      "Iteration: 2884 Loss: nan\n",
      "Iteration: 2885 Loss: nan\n",
      "Iteration: 2886 Loss: nan\n",
      "Iteration: 2887 Loss: nan\n",
      "Iteration: 2888 Loss: nan\n",
      "Iteration: 2889 Loss: nan\n",
      "Iteration: 2890 Loss: nan\n",
      "Iteration: 2891 Loss: nan\n",
      "Iteration: 2892 Loss: nan\n",
      "Iteration: 2893 Loss: nan\n",
      "Iteration: 2894 Loss: nan\n",
      "Iteration: 2895 Loss: nan\n",
      "Iteration: 2896 Loss: nan\n",
      "Iteration: 2897 Loss: nan\n",
      "Iteration: 2898 Loss: nan\n",
      "Iteration: 2899 Loss: nan\n",
      "Iteration: 2900 Loss: nan\n",
      "Iteration: 2901 Loss: nan\n",
      "Iteration: 2902 Loss: nan\n",
      "Iteration: 2903 Loss: nan\n",
      "Iteration: 2904 Loss: nan\n",
      "Iteration: 2905 Loss: nan\n",
      "Iteration: 2906 Loss: nan\n",
      "Iteration: 2907 Loss: nan\n",
      "Iteration: 2908 Loss: nan\n",
      "Iteration: 2909 Loss: nan\n",
      "Iteration: 2910 Loss: nan\n",
      "Iteration: 2911 Loss: nan\n",
      "Iteration: 2912 Loss: nan\n",
      "Iteration: 2913 Loss: nan\n",
      "Iteration: 2914 Loss: nan\n",
      "Iteration: 2915 Loss: nan\n",
      "Iteration: 2916 Loss: nan\n",
      "Iteration: 2917 Loss: nan\n",
      "Iteration: 2918 Loss: nan\n",
      "Iteration: 2919 Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2920 Loss: nan\n",
      "Iteration: 2921 Loss: nan\n",
      "Iteration: 2922 Loss: nan\n",
      "Iteration: 2923 Loss: nan\n",
      "Iteration: 2924 Loss: nan\n",
      "Iteration: 2925 Loss: nan\n",
      "Iteration: 2926 Loss: nan\n",
      "Iteration: 2927 Loss: nan\n",
      "Iteration: 2928 Loss: nan\n",
      "Iteration: 2929 Loss: nan\n",
      "Iteration: 2930 Loss: nan\n",
      "Iteration: 2931 Loss: nan\n",
      "Iteration: 2932 Loss: nan\n",
      "Iteration: 2933 Loss: nan\n",
      "Iteration: 2934 Loss: nan\n",
      "Iteration: 2935 Loss: nan\n",
      "Iteration: 2936 Loss: nan\n",
      "Iteration: 2937 Loss: nan\n",
      "Iteration: 2938 Loss: nan\n",
      "Iteration: 2939 Loss: nan\n",
      "Iteration: 2940 Loss: nan\n",
      "Iteration: 2941 Loss: nan\n",
      "Iteration: 2942 Loss: nan\n",
      "Iteration: 2943 Loss: nan\n",
      "Iteration: 2944 Loss: nan\n",
      "Iteration: 2945 Loss: nan\n",
      "Iteration: 2946 Loss: nan\n",
      "Iteration: 2947 Loss: nan\n",
      "Iteration: 2948 Loss: nan\n",
      "Iteration: 2949 Loss: nan\n",
      "Iteration: 2950 Loss: nan\n",
      "Iteration: 2951 Loss: nan\n",
      "Iteration: 2952 Loss: nan\n",
      "Iteration: 2953 Loss: nan\n",
      "Iteration: 2954 Loss: nan\n",
      "Iteration: 2955 Loss: nan\n",
      "Iteration: 2956 Loss: nan\n",
      "Iteration: 2957 Loss: nan\n",
      "Iteration: 2958 Loss: nan\n",
      "Iteration: 2959 Loss: nan\n",
      "Iteration: 2960 Loss: nan\n",
      "Iteration: 2961 Loss: nan\n",
      "Iteration: 2962 Loss: nan\n",
      "Iteration: 2963 Loss: nan\n",
      "Iteration: 2964 Loss: nan\n",
      "Iteration: 2965 Loss: nan\n",
      "Iteration: 2966 Loss: nan\n",
      "Iteration: 2967 Loss: nan\n",
      "Iteration: 2968 Loss: nan\n",
      "Iteration: 2969 Loss: nan\n",
      "Iteration: 2970 Loss: nan\n",
      "Iteration: 2971 Loss: nan\n",
      "Iteration: 2972 Loss: nan\n",
      "Iteration: 2973 Loss: nan\n",
      "Iteration: 2974 Loss: nan\n",
      "Iteration: 2975 Loss: nan\n",
      "Iteration: 2976 Loss: nan\n",
      "Iteration: 2977 Loss: nan\n",
      "Iteration: 2978 Loss: nan\n",
      "Iteration: 2979 Loss: nan\n",
      "Iteration: 2980 Loss: nan\n",
      "Iteration: 2981 Loss: nan\n",
      "Iteration: 2982 Loss: nan\n",
      "Iteration: 2983 Loss: nan\n",
      "Iteration: 2984 Loss: nan\n",
      "Iteration: 2985 Loss: nan\n",
      "Iteration: 2986 Loss: nan\n",
      "Iteration: 2987 Loss: nan\n",
      "Iteration: 2988 Loss: nan\n",
      "Iteration: 2989 Loss: nan\n",
      "Iteration: 2990 Loss: nan\n",
      "Iteration: 2991 Loss: nan\n",
      "Iteration: 2992 Loss: nan\n",
      "Iteration: 2993 Loss: nan\n",
      "Iteration: 2994 Loss: nan\n",
      "Iteration: 2995 Loss: nan\n",
      "Iteration: 2996 Loss: nan\n",
      "Iteration: 2997 Loss: nan\n",
      "Iteration: 2998 Loss: nan\n",
      "Iteration: 2999 Loss: nan\n",
      "Iteration: 3000 Loss: nan\n",
      "Iteration: 3001 Loss: nan\n",
      "Iteration: 3002 Loss: nan\n",
      "Iteration: 3003 Loss: nan\n",
      "Iteration: 3004 Loss: nan\n",
      "Iteration: 3005 Loss: nan\n",
      "Iteration: 3006 Loss: nan\n",
      "Iteration: 3007 Loss: nan\n",
      "Iteration: 3008 Loss: nan\n",
      "Iteration: 3009 Loss: nan\n",
      "Iteration: 3010 Loss: nan\n",
      "Iteration: 3011 Loss: nan\n",
      "Iteration: 3012 Loss: nan\n",
      "Iteration: 3013 Loss: nan\n",
      "Iteration: 3014 Loss: nan\n",
      "Iteration: 3015 Loss: nan\n",
      "Iteration: 3016 Loss: nan\n",
      "Iteration: 3017 Loss: nan\n",
      "Iteration: 3018 Loss: nan\n",
      "Iteration: 3019 Loss: nan\n",
      "Iteration: 3020 Loss: nan\n",
      "Iteration: 3021 Loss: nan\n",
      "Iteration: 3022 Loss: nan\n",
      "Iteration: 3023 Loss: nan\n",
      "Iteration: 3024 Loss: nan\n",
      "Iteration: 3025 Loss: nan\n",
      "Iteration: 3026 Loss: nan\n",
      "Iteration: 3027 Loss: nan\n",
      "Iteration: 3028 Loss: nan\n",
      "Iteration: 3029 Loss: nan\n",
      "Iteration: 3030 Loss: nan\n",
      "Iteration: 3031 Loss: nan\n",
      "Iteration: 3032 Loss: nan\n",
      "Iteration: 3033 Loss: nan\n",
      "Iteration: 3034 Loss: nan\n",
      "Iteration: 3035 Loss: nan\n",
      "Iteration: 3036 Loss: nan\n",
      "Iteration: 3037 Loss: nan\n",
      "Iteration: 3038 Loss: nan\n",
      "Iteration: 3039 Loss: nan\n",
      "Iteration: 3040 Loss: nan\n",
      "Iteration: 3041 Loss: nan\n",
      "Iteration: 3042 Loss: nan\n",
      "Iteration: 3043 Loss: nan\n",
      "Iteration: 3044 Loss: nan\n",
      "Iteration: 3045 Loss: nan\n",
      "Iteration: 3046 Loss: nan\n",
      "Iteration: 3047 Loss: nan\n",
      "Iteration: 3048 Loss: nan\n",
      "Iteration: 3049 Loss: nan\n",
      "Iteration: 3050 Loss: nan\n",
      "Iteration: 3051 Loss: nan\n",
      "Iteration: 3052 Loss: nan\n",
      "Iteration: 3053 Loss: nan\n",
      "Iteration: 3054 Loss: nan\n",
      "Iteration: 3055 Loss: nan\n",
      "Iteration: 3056 Loss: nan\n",
      "Iteration: 3057 Loss: nan\n",
      "Iteration: 3058 Loss: nan\n",
      "Iteration: 3059 Loss: nan\n",
      "Iteration: 3060 Loss: nan\n",
      "Iteration: 3061 Loss: nan\n",
      "Iteration: 3062 Loss: nan\n",
      "Iteration: 3063 Loss: nan\n",
      "Iteration: 3064 Loss: nan\n",
      "Iteration: 3065 Loss: nan\n",
      "Iteration: 3066 Loss: nan\n",
      "Iteration: 3067 Loss: nan\n",
      "Iteration: 3068 Loss: nan\n",
      "Iteration: 3069 Loss: nan\n",
      "Iteration: 3070 Loss: nan\n",
      "Iteration: 3071 Loss: nan\n",
      "Iteration: 3072 Loss: nan\n",
      "Iteration: 3073 Loss: nan\n",
      "Iteration: 3074 Loss: nan\n",
      "Iteration: 3075 Loss: nan\n",
      "Iteration: 3076 Loss: nan\n",
      "Iteration: 3077 Loss: nan\n",
      "Iteration: 3078 Loss: nan\n",
      "Iteration: 3079 Loss: nan\n",
      "Iteration: 3080 Loss: nan\n",
      "Iteration: 3081 Loss: nan\n",
      "Iteration: 3082 Loss: nan\n",
      "Iteration: 3083 Loss: nan\n",
      "Iteration: 3084 Loss: nan\n",
      "Iteration: 3085 Loss: nan\n",
      "Iteration: 3086 Loss: nan\n",
      "Iteration: 3087 Loss: nan\n",
      "Iteration: 3088 Loss: nan\n",
      "Iteration: 3089 Loss: nan\n",
      "Iteration: 3090 Loss: nan\n",
      "Iteration: 3091 Loss: nan\n",
      "Iteration: 3092 Loss: nan\n",
      "Iteration: 3093 Loss: nan\n",
      "Iteration: 3094 Loss: nan\n",
      "Iteration: 3095 Loss: nan\n",
      "Iteration: 3096 Loss: nan\n",
      "Iteration: 3097 Loss: nan\n",
      "Iteration: 3098 Loss: nan\n",
      "Iteration: 3099 Loss: nan\n",
      "Iteration: 3100 Loss: nan\n",
      "Iteration: 3101 Loss: nan\n",
      "Iteration: 3102 Loss: nan\n",
      "Iteration: 3103 Loss: nan\n",
      "Iteration: 3104 Loss: nan\n",
      "Iteration: 3105 Loss: nan\n",
      "Iteration: 3106 Loss: nan\n",
      "Iteration: 3107 Loss: nan\n",
      "Iteration: 3108 Loss: nan\n",
      "Iteration: 3109 Loss: nan\n",
      "Iteration: 3110 Loss: nan\n",
      "Iteration: 3111 Loss: nan\n",
      "Iteration: 3112 Loss: nan\n",
      "Iteration: 3113 Loss: nan\n",
      "Iteration: 3114 Loss: nan\n",
      "Iteration: 3115 Loss: nan\n",
      "Iteration: 3116 Loss: nan\n",
      "Iteration: 3117 Loss: nan\n",
      "Iteration: 3118 Loss: nan\n",
      "Iteration: 3119 Loss: nan\n",
      "Iteration: 3120 Loss: nan\n",
      "Iteration: 3121 Loss: nan\n",
      "Iteration: 3122 Loss: nan\n",
      "Iteration: 3123 Loss: nan\n",
      "Iteration: 3124 Loss: nan\n",
      "Iteration: 3125 Loss: nan\n",
      "Iteration: 3126 Loss: nan\n",
      "Iteration: 3127 Loss: nan\n",
      "Iteration: 3128 Loss: nan\n",
      "Iteration: 3129 Loss: nan\n",
      "Iteration: 3130 Loss: nan\n",
      "Iteration: 3131 Loss: nan\n",
      "Iteration: 3132 Loss: nan\n",
      "Iteration: 3133 Loss: nan\n",
      "Iteration: 3134 Loss: nan\n",
      "Iteration: 3135 Loss: nan\n",
      "Iteration: 3136 Loss: nan\n",
      "Iteration: 3137 Loss: nan\n",
      "Iteration: 3138 Loss: nan\n",
      "Iteration: 3139 Loss: nan\n",
      "Iteration: 3140 Loss: nan\n",
      "Iteration: 3141 Loss: nan\n",
      "Iteration: 3142 Loss: nan\n",
      "Iteration: 3143 Loss: nan\n",
      "Iteration: 3144 Loss: nan\n",
      "Iteration: 3145 Loss: nan\n",
      "Iteration: 3146 Loss: nan\n",
      "Iteration: 3147 Loss: nan\n",
      "Iteration: 3148 Loss: nan\n",
      "Iteration: 3149 Loss: nan\n",
      "Iteration: 3150 Loss: nan\n",
      "Iteration: 3151 Loss: nan\n",
      "Iteration: 3152 Loss: nan\n",
      "Iteration: 3153 Loss: nan\n",
      "Iteration: 3154 Loss: nan\n",
      "Iteration: 3155 Loss: nan\n",
      "Iteration: 3156 Loss: nan\n",
      "Iteration: 3157 Loss: nan\n",
      "Iteration: 3158 Loss: nan\n",
      "Iteration: 3159 Loss: nan\n",
      "Iteration: 3160 Loss: nan\n",
      "Iteration: 3161 Loss: nan\n",
      "Iteration: 3162 Loss: nan\n",
      "Iteration: 3163 Loss: nan\n",
      "Iteration: 3164 Loss: nan\n",
      "Iteration: 3165 Loss: nan\n",
      "Iteration: 3166 Loss: nan\n",
      "Iteration: 3167 Loss: nan\n",
      "Iteration: 3168 Loss: nan\n",
      "Iteration: 3169 Loss: nan\n",
      "Iteration: 3170 Loss: nan\n",
      "Iteration: 3171 Loss: nan\n",
      "Iteration: 3172 Loss: nan\n",
      "Iteration: 3173 Loss: nan\n",
      "Iteration: 3174 Loss: nan\n",
      "Iteration: 3175 Loss: nan\n",
      "Iteration: 3176 Loss: nan\n",
      "Iteration: 3177 Loss: nan\n",
      "Iteration: 3178 Loss: nan\n",
      "Iteration: 3179 Loss: nan\n",
      "Iteration: 3180 Loss: nan\n",
      "Iteration: 3181 Loss: nan\n",
      "Iteration: 3182 Loss: nan\n",
      "Iteration: 3183 Loss: nan\n",
      "Iteration: 3184 Loss: nan\n",
      "Iteration: 3185 Loss: nan\n",
      "Iteration: 3186 Loss: nan\n",
      "Iteration: 3187 Loss: nan\n",
      "Iteration: 3188 Loss: nan\n",
      "Iteration: 3189 Loss: nan\n",
      "Iteration: 3190 Loss: nan\n",
      "Iteration: 3191 Loss: nan\n",
      "Iteration: 3192 Loss: nan\n",
      "Iteration: 3193 Loss: nan\n",
      "Iteration: 3194 Loss: nan\n",
      "Iteration: 3195 Loss: nan\n",
      "Iteration: 3196 Loss: nan\n",
      "Iteration: 3197 Loss: nan\n",
      "Iteration: 3198 Loss: nan\n",
      "Iteration: 3199 Loss: nan\n",
      "Iteration: 3200 Loss: nan\n",
      "Iteration: 3201 Loss: nan\n",
      "Iteration: 3202 Loss: nan\n",
      "Iteration: 3203 Loss: nan\n",
      "Iteration: 3204 Loss: nan\n",
      "Iteration: 3205 Loss: nan\n",
      "Iteration: 3206 Loss: nan\n",
      "Iteration: 3207 Loss: nan\n",
      "Iteration: 3208 Loss: nan\n",
      "Iteration: 3209 Loss: nan\n",
      "Iteration: 3210 Loss: nan\n",
      "Iteration: 3211 Loss: nan\n",
      "Iteration: 3212 Loss: nan\n",
      "Iteration: 3213 Loss: nan\n",
      "Iteration: 3214 Loss: nan\n",
      "Iteration: 3215 Loss: nan\n",
      "Iteration: 3216 Loss: nan\n",
      "Iteration: 3217 Loss: nan\n",
      "Iteration: 3218 Loss: nan\n",
      "Iteration: 3219 Loss: nan\n",
      "Iteration: 3220 Loss: nan\n",
      "Iteration: 3221 Loss: nan\n",
      "Iteration: 3222 Loss: nan\n",
      "Iteration: 3223 Loss: nan\n",
      "Iteration: 3224 Loss: nan\n",
      "Iteration: 3225 Loss: nan\n",
      "Iteration: 3226 Loss: nan\n",
      "Iteration: 3227 Loss: nan\n",
      "Iteration: 3228 Loss: nan\n",
      "Iteration: 3229 Loss: nan\n",
      "Iteration: 3230 Loss: nan\n",
      "Iteration: 3231 Loss: nan\n",
      "Iteration: 3232 Loss: nan\n",
      "Iteration: 3233 Loss: nan\n",
      "Iteration: 3234 Loss: nan\n",
      "Iteration: 3235 Loss: nan\n",
      "Iteration: 3236 Loss: nan\n",
      "Iteration: 3237 Loss: nan\n",
      "Iteration: 3238 Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3239 Loss: nan\n",
      "Iteration: 3240 Loss: nan\n",
      "Iteration: 3241 Loss: nan\n",
      "Iteration: 3242 Loss: nan\n",
      "Iteration: 3243 Loss: nan\n",
      "Iteration: 3244 Loss: nan\n",
      "Iteration: 3245 Loss: nan\n",
      "Iteration: 3246 Loss: nan\n",
      "Iteration: 3247 Loss: nan\n",
      "Iteration: 3248 Loss: nan\n",
      "Iteration: 3249 Loss: nan\n",
      "Iteration: 3250 Loss: nan\n",
      "Iteration: 3251 Loss: nan\n",
      "Iteration: 3252 Loss: nan\n",
      "Iteration: 3253 Loss: nan\n",
      "Iteration: 3254 Loss: nan\n",
      "Iteration: 3255 Loss: nan\n",
      "Iteration: 3256 Loss: nan\n",
      "Iteration: 3257 Loss: nan\n",
      "Iteration: 3258 Loss: nan\n",
      "Iteration: 3259 Loss: nan\n",
      "Iteration: 3260 Loss: nan\n",
      "Iteration: 3261 Loss: nan\n",
      "Iteration: 3262 Loss: nan\n",
      "Iteration: 3263 Loss: nan\n",
      "Iteration: 3264 Loss: nan\n",
      "Iteration: 3265 Loss: nan\n",
      "Iteration: 3266 Loss: nan\n",
      "Iteration: 3267 Loss: nan\n",
      "Iteration: 3268 Loss: nan\n",
      "Iteration: 3269 Loss: nan\n",
      "Iteration: 3270 Loss: nan\n",
      "Iteration: 3271 Loss: nan\n",
      "Iteration: 3272 Loss: nan\n",
      "Iteration: 3273 Loss: nan\n",
      "Iteration: 3274 Loss: nan\n",
      "Iteration: 3275 Loss: nan\n",
      "Iteration: 3276 Loss: nan\n",
      "Iteration: 3277 Loss: nan\n",
      "Iteration: 3278 Loss: nan\n",
      "Iteration: 3279 Loss: nan\n",
      "Iteration: 3280 Loss: nan\n",
      "Iteration: 3281 Loss: nan\n",
      "Iteration: 3282 Loss: nan\n",
      "Iteration: 3283 Loss: nan\n",
      "Iteration: 3284 Loss: nan\n",
      "Iteration: 3285 Loss: nan\n",
      "Iteration: 3286 Loss: nan\n",
      "Iteration: 3287 Loss: nan\n",
      "Iteration: 3288 Loss: nan\n",
      "Iteration: 3289 Loss: nan\n",
      "Iteration: 3290 Loss: nan\n",
      "Iteration: 3291 Loss: nan\n",
      "Iteration: 3292 Loss: nan\n",
      "Iteration: 3293 Loss: nan\n",
      "Iteration: 3294 Loss: nan\n",
      "Iteration: 3295 Loss: nan\n",
      "Iteration: 3296 Loss: nan\n",
      "Iteration: 3297 Loss: nan\n",
      "Iteration: 3298 Loss: nan\n",
      "Iteration: 3299 Loss: nan\n",
      "Iteration: 3300 Loss: nan\n",
      "Iteration: 3301 Loss: nan\n",
      "Iteration: 3302 Loss: nan\n",
      "Iteration: 3303 Loss: nan\n",
      "Iteration: 3304 Loss: nan\n",
      "Iteration: 3305 Loss: nan\n",
      "Iteration: 3306 Loss: nan\n",
      "Iteration: 3307 Loss: nan\n",
      "Iteration: 3308 Loss: nan\n",
      "Iteration: 3309 Loss: nan\n",
      "Iteration: 3310 Loss: nan\n",
      "Iteration: 3311 Loss: nan\n",
      "Iteration: 3312 Loss: nan\n",
      "Iteration: 3313 Loss: nan\n",
      "Iteration: 3314 Loss: nan\n",
      "Iteration: 3315 Loss: nan\n",
      "Iteration: 3316 Loss: nan\n",
      "Iteration: 3317 Loss: nan\n",
      "Iteration: 3318 Loss: nan\n",
      "Iteration: 3319 Loss: nan\n",
      "Iteration: 3320 Loss: nan\n",
      "Iteration: 3321 Loss: nan\n",
      "Iteration: 3322 Loss: nan\n",
      "Iteration: 3323 Loss: nan\n",
      "Iteration: 3324 Loss: nan\n",
      "Iteration: 3325 Loss: nan\n",
      "Iteration: 3326 Loss: nan\n",
      "Iteration: 3327 Loss: nan\n",
      "Iteration: 3328 Loss: nan\n",
      "Iteration: 3329 Loss: nan\n",
      "Iteration: 3330 Loss: nan\n",
      "Iteration: 3331 Loss: nan\n",
      "Iteration: 3332 Loss: nan\n",
      "Iteration: 3333 Loss: nan\n",
      "Iteration: 3334 Loss: nan\n",
      "Iteration: 3335 Loss: nan\n",
      "Iteration: 3336 Loss: nan\n",
      "Iteration: 3337 Loss: nan\n",
      "Iteration: 3338 Loss: nan\n",
      "Iteration: 3339 Loss: nan\n",
      "Iteration: 3340 Loss: nan\n",
      "Iteration: 3341 Loss: nan\n",
      "Iteration: 3342 Loss: nan\n",
      "Iteration: 3343 Loss: nan\n",
      "Iteration: 3344 Loss: nan\n",
      "Iteration: 3345 Loss: nan\n",
      "Iteration: 3346 Loss: nan\n",
      "Iteration: 3347 Loss: nan\n",
      "Iteration: 3348 Loss: nan\n",
      "Iteration: 3349 Loss: nan\n",
      "Iteration: 3350 Loss: nan\n",
      "Iteration: 3351 Loss: nan\n",
      "Iteration: 3352 Loss: nan\n",
      "Iteration: 3353 Loss: nan\n",
      "Iteration: 3354 Loss: nan\n",
      "Iteration: 3355 Loss: nan\n",
      "Iteration: 3356 Loss: nan\n",
      "Iteration: 3357 Loss: nan\n",
      "Iteration: 3358 Loss: nan\n",
      "Iteration: 3359 Loss: nan\n",
      "Iteration: 3360 Loss: nan\n",
      "Iteration: 3361 Loss: nan\n",
      "Iteration: 3362 Loss: nan\n",
      "Iteration: 3363 Loss: nan\n",
      "Iteration: 3364 Loss: nan\n",
      "Iteration: 3365 Loss: nan\n",
      "Iteration: 3366 Loss: nan\n",
      "Iteration: 3367 Loss: nan\n",
      "Iteration: 3368 Loss: nan\n",
      "Iteration: 3369 Loss: nan\n",
      "Iteration: 3370 Loss: nan\n",
      "Iteration: 3371 Loss: nan\n",
      "Iteration: 3372 Loss: nan\n",
      "Iteration: 3373 Loss: nan\n",
      "Iteration: 3374 Loss: nan\n",
      "Iteration: 3375 Loss: nan\n",
      "Iteration: 3376 Loss: nan\n",
      "Iteration: 3377 Loss: nan\n",
      "Iteration: 3378 Loss: nan\n",
      "Iteration: 3379 Loss: nan\n",
      "Iteration: 3380 Loss: nan\n",
      "Iteration: 3381 Loss: nan\n",
      "Iteration: 3382 Loss: nan\n",
      "Iteration: 3383 Loss: nan\n",
      "Iteration: 3384 Loss: nan\n",
      "Iteration: 3385 Loss: nan\n",
      "Iteration: 3386 Loss: nan\n",
      "Iteration: 3387 Loss: nan\n",
      "Iteration: 3388 Loss: nan\n",
      "Iteration: 3389 Loss: nan\n",
      "Iteration: 3390 Loss: nan\n",
      "Iteration: 3391 Loss: nan\n",
      "Iteration: 3392 Loss: nan\n",
      "Iteration: 3393 Loss: nan\n",
      "Iteration: 3394 Loss: nan\n",
      "Iteration: 3395 Loss: nan\n",
      "Iteration: 3396 Loss: nan\n",
      "Iteration: 3397 Loss: nan\n",
      "Iteration: 3398 Loss: nan\n",
      "Iteration: 3399 Loss: nan\n",
      "Iteration: 3400 Loss: nan\n",
      "Iteration: 3401 Loss: nan\n",
      "Iteration: 3402 Loss: nan\n",
      "Iteration: 3403 Loss: nan\n",
      "Iteration: 3404 Loss: nan\n",
      "Iteration: 3405 Loss: nan\n",
      "Iteration: 3406 Loss: nan\n",
      "Iteration: 3407 Loss: nan\n",
      "Iteration: 3408 Loss: nan\n",
      "Iteration: 3409 Loss: nan\n",
      "Iteration: 3410 Loss: nan\n",
      "Iteration: 3411 Loss: nan\n",
      "Iteration: 3412 Loss: nan\n",
      "Iteration: 3413 Loss: nan\n",
      "Iteration: 3414 Loss: nan\n",
      "Iteration: 3415 Loss: nan\n",
      "Iteration: 3416 Loss: nan\n",
      "Iteration: 3417 Loss: nan\n",
      "Iteration: 3418 Loss: nan\n",
      "Iteration: 3419 Loss: nan\n",
      "Iteration: 3420 Loss: nan\n",
      "Iteration: 3421 Loss: nan\n",
      "Iteration: 3422 Loss: nan\n",
      "Iteration: 3423 Loss: nan\n",
      "Iteration: 3424 Loss: nan\n",
      "Iteration: 3425 Loss: nan\n",
      "Iteration: 3426 Loss: nan\n",
      "Iteration: 3427 Loss: nan\n",
      "Iteration: 3428 Loss: nan\n",
      "Iteration: 3429 Loss: nan\n",
      "Iteration: 3430 Loss: nan\n",
      "Iteration: 3431 Loss: nan\n",
      "Iteration: 3432 Loss: nan\n",
      "Iteration: 3433 Loss: nan\n",
      "Iteration: 3434 Loss: nan\n",
      "Iteration: 3435 Loss: nan\n",
      "Iteration: 3436 Loss: nan\n",
      "Iteration: 3437 Loss: nan\n",
      "Iteration: 3438 Loss: nan\n",
      "Iteration: 3439 Loss: nan\n",
      "Iteration: 3440 Loss: nan\n",
      "Iteration: 3441 Loss: nan\n",
      "Iteration: 3442 Loss: nan\n",
      "Iteration: 3443 Loss: nan\n",
      "Iteration: 3444 Loss: nan\n",
      "Iteration: 3445 Loss: nan\n",
      "Iteration: 3446 Loss: nan\n",
      "Iteration: 3447 Loss: nan\n",
      "Iteration: 3448 Loss: nan\n",
      "Iteration: 3449 Loss: nan\n",
      "Iteration: 3450 Loss: nan\n",
      "Iteration: 3451 Loss: nan\n",
      "Iteration: 3452 Loss: nan\n",
      "Iteration: 3453 Loss: nan\n",
      "Iteration: 3454 Loss: nan\n",
      "Iteration: 3455 Loss: nan\n",
      "Iteration: 3456 Loss: nan\n",
      "Iteration: 3457 Loss: nan\n",
      "Iteration: 3458 Loss: nan\n",
      "Iteration: 3459 Loss: nan\n",
      "Iteration: 3460 Loss: nan\n",
      "Iteration: 3461 Loss: nan\n",
      "Iteration: 3462 Loss: nan\n",
      "Iteration: 3463 Loss: nan\n",
      "Iteration: 3464 Loss: nan\n",
      "Iteration: 3465 Loss: nan\n",
      "Iteration: 3466 Loss: nan\n",
      "Iteration: 3467 Loss: nan\n",
      "Iteration: 3468 Loss: nan\n",
      "Iteration: 3469 Loss: nan\n",
      "Iteration: 3470 Loss: nan\n",
      "Iteration: 3471 Loss: nan\n",
      "Iteration: 3472 Loss: nan\n",
      "Iteration: 3473 Loss: nan\n",
      "Iteration: 3474 Loss: nan\n",
      "Iteration: 3475 Loss: nan\n",
      "Iteration: 3476 Loss: nan\n",
      "Iteration: 3477 Loss: nan\n",
      "Iteration: 3478 Loss: nan\n",
      "Iteration: 3479 Loss: nan\n",
      "Iteration: 3480 Loss: nan\n",
      "Iteration: 3481 Loss: nan\n",
      "Iteration: 3482 Loss: nan\n",
      "Iteration: 3483 Loss: nan\n",
      "Iteration: 3484 Loss: nan\n",
      "Iteration: 3485 Loss: nan\n",
      "Iteration: 3486 Loss: nan\n",
      "Iteration: 3487 Loss: nan\n",
      "Iteration: 3488 Loss: nan\n",
      "Iteration: 3489 Loss: nan\n",
      "Iteration: 3490 Loss: nan\n",
      "Iteration: 3491 Loss: nan\n",
      "Iteration: 3492 Loss: nan\n",
      "Iteration: 3493 Loss: nan\n",
      "Iteration: 3494 Loss: nan\n",
      "Iteration: 3495 Loss: nan\n",
      "Iteration: 3496 Loss: nan\n",
      "Iteration: 3497 Loss: nan\n",
      "Iteration: 3498 Loss: nan\n",
      "Iteration: 3499 Loss: nan\n",
      "Iteration: 3500 Loss: nan\n",
      "Iteration: 3501 Loss: nan\n",
      "Iteration: 3502 Loss: nan\n",
      "Iteration: 3503 Loss: nan\n",
      "Iteration: 3504 Loss: nan\n",
      "Iteration: 3505 Loss: nan\n",
      "Iteration: 3506 Loss: nan\n",
      "Iteration: 3507 Loss: nan\n",
      "Iteration: 3508 Loss: nan\n",
      "Iteration: 3509 Loss: nan\n",
      "Iteration: 3510 Loss: nan\n",
      "Iteration: 3511 Loss: nan\n",
      "Iteration: 3512 Loss: nan\n",
      "Iteration: 3513 Loss: nan\n",
      "Iteration: 3514 Loss: nan\n",
      "Iteration: 3515 Loss: nan\n",
      "Iteration: 3516 Loss: nan\n",
      "Iteration: 3517 Loss: nan\n",
      "Iteration: 3518 Loss: nan\n",
      "Iteration: 3519 Loss: nan\n",
      "Iteration: 3520 Loss: nan\n",
      "Iteration: 3521 Loss: nan\n",
      "Iteration: 3522 Loss: nan\n",
      "Iteration: 3523 Loss: nan\n",
      "Iteration: 3524 Loss: nan\n",
      "Iteration: 3525 Loss: nan\n",
      "Iteration: 3526 Loss: nan\n",
      "Iteration: 3527 Loss: nan\n",
      "Iteration: 3528 Loss: nan\n",
      "Iteration: 3529 Loss: nan\n",
      "Iteration: 3530 Loss: nan\n",
      "Iteration: 3531 Loss: nan\n",
      "Iteration: 3532 Loss: nan\n",
      "Iteration: 3533 Loss: nan\n",
      "Iteration: 3534 Loss: nan\n",
      "Iteration: 3535 Loss: nan\n",
      "Iteration: 3536 Loss: nan\n",
      "Iteration: 3537 Loss: nan\n",
      "Iteration: 3538 Loss: nan\n",
      "Iteration: 3539 Loss: nan\n",
      "Iteration: 3540 Loss: nan\n",
      "Iteration: 3541 Loss: nan\n",
      "Iteration: 3542 Loss: nan\n",
      "Iteration: 3543 Loss: nan\n",
      "Iteration: 3544 Loss: nan\n",
      "Iteration: 3545 Loss: nan\n",
      "Iteration: 3546 Loss: nan\n",
      "Iteration: 3547 Loss: nan\n",
      "Iteration: 3548 Loss: nan\n",
      "Iteration: 3549 Loss: nan\n",
      "Iteration: 3550 Loss: nan\n",
      "Iteration: 3551 Loss: nan\n",
      "Iteration: 3552 Loss: nan\n",
      "Iteration: 3553 Loss: nan\n",
      "Iteration: 3554 Loss: nan\n",
      "Iteration: 3555 Loss: nan\n",
      "Iteration: 3556 Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3557 Loss: nan\n",
      "Iteration: 3558 Loss: nan\n",
      "Iteration: 3559 Loss: nan\n",
      "Iteration: 3560 Loss: nan\n",
      "Iteration: 3561 Loss: nan\n",
      "Iteration: 3562 Loss: nan\n",
      "Iteration: 3563 Loss: nan\n",
      "Iteration: 3564 Loss: nan\n",
      "Iteration: 3565 Loss: nan\n",
      "Iteration: 3566 Loss: nan\n",
      "Iteration: 3567 Loss: nan\n",
      "Iteration: 3568 Loss: nan\n",
      "Iteration: 3569 Loss: nan\n",
      "Iteration: 3570 Loss: nan\n",
      "Iteration: 3571 Loss: nan\n",
      "Iteration: 3572 Loss: nan\n",
      "Iteration: 3573 Loss: nan\n",
      "Iteration: 3574 Loss: nan\n",
      "Iteration: 3575 Loss: nan\n",
      "Iteration: 3576 Loss: nan\n",
      "Iteration: 3577 Loss: nan\n",
      "Iteration: 3578 Loss: nan\n",
      "Iteration: 3579 Loss: nan\n",
      "Iteration: 3580 Loss: nan\n",
      "Iteration: 3581 Loss: nan\n",
      "Iteration: 3582 Loss: nan\n",
      "Iteration: 3583 Loss: nan\n",
      "Iteration: 3584 Loss: nan\n",
      "Iteration: 3585 Loss: nan\n",
      "Iteration: 3586 Loss: nan\n",
      "Iteration: 3587 Loss: nan\n",
      "Iteration: 3588 Loss: nan\n",
      "Iteration: 3589 Loss: nan\n",
      "Iteration: 3590 Loss: nan\n",
      "Iteration: 3591 Loss: nan\n",
      "Iteration: 3592 Loss: nan\n",
      "Iteration: 3593 Loss: nan\n",
      "Iteration: 3594 Loss: nan\n",
      "Iteration: 3595 Loss: nan\n",
      "Iteration: 3596 Loss: nan\n",
      "Iteration: 3597 Loss: nan\n",
      "Iteration: 3598 Loss: nan\n",
      "Iteration: 3599 Loss: nan\n",
      "Iteration: 3600 Loss: nan\n",
      "Iteration: 3601 Loss: nan\n",
      "Iteration: 3602 Loss: nan\n",
      "Iteration: 3603 Loss: nan\n",
      "Iteration: 3604 Loss: nan\n",
      "Iteration: 3605 Loss: nan\n",
      "Iteration: 3606 Loss: nan\n",
      "Iteration: 3607 Loss: nan\n",
      "Iteration: 3608 Loss: nan\n",
      "Iteration: 3609 Loss: nan\n",
      "Iteration: 3610 Loss: nan\n",
      "Iteration: 3611 Loss: nan\n",
      "Iteration: 3612 Loss: nan\n",
      "Iteration: 3613 Loss: nan\n",
      "Iteration: 3614 Loss: nan\n",
      "Iteration: 3615 Loss: nan\n",
      "Iteration: 3616 Loss: nan\n",
      "Iteration: 3617 Loss: nan\n",
      "Iteration: 3618 Loss: nan\n",
      "Iteration: 3619 Loss: nan\n",
      "Iteration: 3620 Loss: nan\n",
      "Iteration: 3621 Loss: nan\n",
      "Iteration: 3622 Loss: nan\n",
      "Iteration: 3623 Loss: nan\n",
      "Iteration: 3624 Loss: nan\n",
      "Iteration: 3625 Loss: nan\n",
      "Iteration: 3626 Loss: nan\n",
      "Iteration: 3627 Loss: nan\n",
      "Iteration: 3628 Loss: nan\n",
      "Iteration: 3629 Loss: nan\n",
      "Iteration: 3630 Loss: nan\n",
      "Iteration: 3631 Loss: nan\n",
      "Iteration: 3632 Loss: nan\n",
      "Iteration: 3633 Loss: nan\n",
      "Iteration: 3634 Loss: nan\n",
      "Iteration: 3635 Loss: nan\n",
      "Iteration: 3636 Loss: nan\n",
      "Iteration: 3637 Loss: nan\n",
      "Iteration: 3638 Loss: nan\n",
      "Iteration: 3639 Loss: nan\n",
      "Iteration: 3640 Loss: nan\n",
      "Iteration: 3641 Loss: nan\n",
      "Iteration: 3642 Loss: nan\n",
      "Iteration: 3643 Loss: nan\n",
      "Iteration: 3644 Loss: nan\n",
      "Iteration: 3645 Loss: nan\n",
      "Iteration: 3646 Loss: nan\n",
      "Iteration: 3647 Loss: nan\n",
      "Iteration: 3648 Loss: nan\n",
      "Iteration: 3649 Loss: nan\n",
      "Iteration: 3650 Loss: nan\n",
      "Iteration: 3651 Loss: nan\n",
      "Iteration: 3652 Loss: nan\n",
      "Iteration: 3653 Loss: nan\n",
      "Iteration: 3654 Loss: nan\n",
      "Iteration: 3655 Loss: nan\n",
      "Iteration: 3656 Loss: nan\n",
      "Iteration: 3657 Loss: nan\n",
      "Iteration: 3658 Loss: nan\n",
      "Iteration: 3659 Loss: nan\n",
      "Iteration: 3660 Loss: nan\n",
      "Iteration: 3661 Loss: nan\n",
      "Iteration: 3662 Loss: nan\n",
      "Iteration: 3663 Loss: nan\n",
      "Iteration: 3664 Loss: nan\n",
      "Iteration: 3665 Loss: nan\n",
      "Iteration: 3666 Loss: nan\n",
      "Iteration: 3667 Loss: nan\n",
      "Iteration: 3668 Loss: nan\n",
      "Iteration: 3669 Loss: nan\n",
      "Iteration: 3670 Loss: nan\n",
      "Iteration: 3671 Loss: nan\n",
      "Iteration: 3672 Loss: nan\n",
      "Iteration: 3673 Loss: nan\n",
      "Iteration: 3674 Loss: nan\n",
      "Iteration: 3675 Loss: nan\n",
      "Iteration: 3676 Loss: nan\n",
      "Iteration: 3677 Loss: nan\n",
      "Iteration: 3678 Loss: nan\n",
      "Iteration: 3679 Loss: nan\n",
      "Iteration: 3680 Loss: nan\n",
      "Iteration: 3681 Loss: nan\n",
      "Iteration: 3682 Loss: nan\n",
      "Iteration: 3683 Loss: nan\n",
      "Iteration: 3684 Loss: nan\n",
      "Iteration: 3685 Loss: nan\n",
      "Iteration: 3686 Loss: nan\n",
      "Iteration: 3687 Loss: nan\n",
      "Iteration: 3688 Loss: nan\n",
      "Iteration: 3689 Loss: nan\n",
      "Iteration: 3690 Loss: nan\n",
      "Iteration: 3691 Loss: nan\n",
      "Iteration: 3692 Loss: nan\n",
      "Iteration: 3693 Loss: nan\n",
      "Iteration: 3694 Loss: nan\n",
      "Iteration: 3695 Loss: nan\n",
      "Iteration: 3696 Loss: nan\n",
      "Iteration: 3697 Loss: nan\n",
      "Iteration: 3698 Loss: nan\n",
      "Iteration: 3699 Loss: nan\n",
      "Iteration: 3700 Loss: nan\n",
      "Iteration: 3701 Loss: nan\n",
      "Iteration: 3702 Loss: nan\n",
      "Iteration: 3703 Loss: nan\n",
      "Iteration: 3704 Loss: nan\n",
      "Iteration: 3705 Loss: nan\n",
      "Iteration: 3706 Loss: nan\n",
      "Iteration: 3707 Loss: nan\n",
      "Iteration: 3708 Loss: nan\n",
      "Iteration: 3709 Loss: nan\n",
      "Iteration: 3710 Loss: nan\n",
      "Iteration: 3711 Loss: nan\n",
      "Iteration: 3712 Loss: nan\n",
      "Iteration: 3713 Loss: nan\n",
      "Iteration: 3714 Loss: nan\n",
      "Iteration: 3715 Loss: nan\n",
      "Iteration: 3716 Loss: nan\n",
      "Iteration: 3717 Loss: nan\n",
      "Iteration: 3718 Loss: nan\n",
      "Iteration: 3719 Loss: nan\n",
      "Iteration: 3720 Loss: nan\n",
      "Iteration: 3721 Loss: nan\n",
      "Iteration: 3722 Loss: nan\n",
      "Iteration: 3723 Loss: nan\n",
      "Iteration: 3724 Loss: nan\n",
      "Iteration: 3725 Loss: nan\n",
      "Iteration: 3726 Loss: nan\n",
      "Iteration: 3727 Loss: nan\n",
      "Iteration: 3728 Loss: nan\n",
      "Iteration: 3729 Loss: nan\n",
      "Iteration: 3730 Loss: nan\n",
      "Iteration: 3731 Loss: nan\n",
      "Iteration: 3732 Loss: nan\n",
      "Iteration: 3733 Loss: nan\n",
      "Iteration: 3734 Loss: nan\n",
      "Iteration: 3735 Loss: nan\n",
      "Iteration: 3736 Loss: nan\n",
      "Iteration: 3737 Loss: nan\n",
      "Iteration: 3738 Loss: nan\n",
      "Iteration: 3739 Loss: nan\n",
      "Iteration: 3740 Loss: nan\n",
      "Iteration: 3741 Loss: nan\n",
      "Iteration: 3742 Loss: nan\n",
      "Iteration: 3743 Loss: nan\n",
      "Iteration: 3744 Loss: nan\n",
      "Iteration: 3745 Loss: nan\n",
      "Iteration: 3746 Loss: nan\n",
      "Iteration: 3747 Loss: nan\n",
      "Iteration: 3748 Loss: nan\n",
      "Iteration: 3749 Loss: nan\n",
      "Iteration: 3750 Loss: nan\n",
      "Iteration: 3751 Loss: nan\n",
      "Iteration: 3752 Loss: nan\n",
      "Iteration: 3753 Loss: nan\n",
      "Iteration: 3754 Loss: nan\n",
      "Iteration: 3755 Loss: nan\n",
      "Iteration: 3756 Loss: nan\n",
      "Iteration: 3757 Loss: nan\n",
      "Iteration: 3758 Loss: nan\n",
      "Iteration: 3759 Loss: nan\n",
      "Iteration: 3760 Loss: nan\n",
      "Iteration: 3761 Loss: nan\n",
      "Iteration: 3762 Loss: nan\n",
      "Iteration: 3763 Loss: nan\n",
      "Iteration: 3764 Loss: nan\n",
      "Iteration: 3765 Loss: nan\n",
      "Iteration: 3766 Loss: nan\n",
      "Iteration: 3767 Loss: nan\n",
      "Iteration: 3768 Loss: nan\n",
      "Iteration: 3769 Loss: nan\n",
      "Iteration: 3770 Loss: nan\n",
      "Iteration: 3771 Loss: nan\n",
      "Iteration: 3772 Loss: nan\n",
      "Iteration: 3773 Loss: nan\n",
      "Iteration: 3774 Loss: nan\n",
      "Iteration: 3775 Loss: nan\n",
      "Iteration: 3776 Loss: nan\n",
      "Iteration: 3777 Loss: nan\n",
      "Iteration: 3778 Loss: nan\n",
      "Iteration: 3779 Loss: nan\n",
      "Iteration: 3780 Loss: nan\n",
      "Iteration: 3781 Loss: nan\n",
      "Iteration: 3782 Loss: nan\n",
      "Iteration: 3783 Loss: nan\n",
      "Iteration: 3784 Loss: nan\n",
      "Iteration: 3785 Loss: nan\n",
      "Iteration: 3786 Loss: nan\n",
      "Iteration: 3787 Loss: nan\n",
      "Iteration: 3788 Loss: nan\n",
      "Iteration: 3789 Loss: nan\n",
      "Iteration: 3790 Loss: nan\n",
      "Iteration: 3791 Loss: nan\n",
      "Iteration: 3792 Loss: nan\n",
      "Iteration: 3793 Loss: nan\n",
      "Iteration: 3794 Loss: nan\n",
      "Iteration: 3795 Loss: nan\n",
      "Iteration: 3796 Loss: nan\n",
      "Iteration: 3797 Loss: nan\n",
      "Iteration: 3798 Loss: nan\n",
      "Iteration: 3799 Loss: nan\n",
      "Iteration: 3800 Loss: nan\n",
      "Iteration: 3801 Loss: nan\n",
      "Iteration: 3802 Loss: nan\n",
      "Iteration: 3803 Loss: nan\n",
      "Iteration: 3804 Loss: nan\n",
      "Iteration: 3805 Loss: nan\n",
      "Iteration: 3806 Loss: nan\n",
      "Iteration: 3807 Loss: nan\n",
      "Iteration: 3808 Loss: nan\n",
      "Iteration: 3809 Loss: nan\n",
      "Iteration: 3810 Loss: nan\n",
      "Iteration: 3811 Loss: nan\n",
      "Iteration: 3812 Loss: nan\n",
      "Iteration: 3813 Loss: nan\n",
      "Iteration: 3814 Loss: nan\n",
      "Iteration: 3815 Loss: nan\n",
      "Iteration: 3816 Loss: nan\n",
      "Iteration: 3817 Loss: nan\n",
      "Iteration: 3818 Loss: nan\n",
      "Iteration: 3819 Loss: nan\n",
      "Iteration: 3820 Loss: nan\n",
      "Iteration: 3821 Loss: nan\n",
      "Iteration: 3822 Loss: nan\n",
      "Iteration: 3823 Loss: nan\n",
      "Iteration: 3824 Loss: nan\n",
      "Iteration: 3825 Loss: nan\n",
      "Iteration: 3826 Loss: nan\n",
      "Iteration: 3827 Loss: nan\n",
      "Iteration: 3828 Loss: nan\n",
      "Iteration: 3829 Loss: nan\n",
      "Iteration: 3830 Loss: nan\n",
      "Iteration: 3831 Loss: nan\n",
      "Iteration: 3832 Loss: nan\n",
      "Iteration: 3833 Loss: nan\n",
      "Iteration: 3834 Loss: nan\n",
      "Iteration: 3835 Loss: nan\n",
      "Iteration: 3836 Loss: nan\n",
      "Iteration: 3837 Loss: nan\n",
      "Iteration: 3838 Loss: nan\n",
      "Iteration: 3839 Loss: nan\n",
      "Iteration: 3840 Loss: nan\n",
      "Iteration: 3841 Loss: nan\n",
      "Iteration: 3842 Loss: nan\n",
      "Iteration: 3843 Loss: nan\n",
      "Iteration: 3844 Loss: nan\n",
      "Iteration: 3845 Loss: nan\n",
      "Iteration: 3846 Loss: nan\n",
      "Iteration: 3847 Loss: nan\n",
      "Iteration: 3848 Loss: nan\n",
      "Iteration: 3849 Loss: nan\n",
      "Iteration: 3850 Loss: nan\n",
      "Iteration: 3851 Loss: nan\n",
      "Iteration: 3852 Loss: nan\n",
      "Iteration: 3853 Loss: nan\n",
      "Iteration: 3854 Loss: nan\n",
      "Iteration: 3855 Loss: nan\n",
      "Iteration: 3856 Loss: nan\n",
      "Iteration: 3857 Loss: nan\n",
      "Iteration: 3858 Loss: nan\n",
      "Iteration: 3859 Loss: nan\n",
      "Iteration: 3860 Loss: nan\n",
      "Iteration: 3861 Loss: nan\n",
      "Iteration: 3862 Loss: nan\n",
      "Iteration: 3863 Loss: nan\n",
      "Iteration: 3864 Loss: nan\n",
      "Iteration: 3865 Loss: nan\n",
      "Iteration: 3866 Loss: nan\n",
      "Iteration: 3867 Loss: nan\n",
      "Iteration: 3868 Loss: nan\n",
      "Iteration: 3869 Loss: nan\n",
      "Iteration: 3870 Loss: nan\n",
      "Iteration: 3871 Loss: nan\n",
      "Iteration: 3872 Loss: nan\n",
      "Iteration: 3873 Loss: nan\n",
      "Iteration: 3874 Loss: nan\n",
      "Iteration: 3875 Loss: nan\n",
      "Iteration: 3876 Loss: nan\n",
      "Iteration: 3877 Loss: nan\n",
      "Iteration: 3878 Loss: nan\n",
      "Iteration: 3879 Loss: nan\n",
      "Iteration: 3880 Loss: nan\n",
      "Iteration: 3881 Loss: nan\n",
      "Iteration: 3882 Loss: nan\n",
      "Iteration: 3883 Loss: nan\n",
      "Iteration: 3884 Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3885 Loss: nan\n",
      "Iteration: 3886 Loss: nan\n",
      "Iteration: 3887 Loss: nan\n",
      "Iteration: 3888 Loss: nan\n",
      "Iteration: 3889 Loss: nan\n",
      "Iteration: 3890 Loss: nan\n",
      "Iteration: 3891 Loss: nan\n",
      "Iteration: 3892 Loss: nan\n",
      "Iteration: 3893 Loss: nan\n",
      "Iteration: 3894 Loss: nan\n",
      "Iteration: 3895 Loss: nan\n",
      "Iteration: 3896 Loss: nan\n",
      "Iteration: 3897 Loss: nan\n",
      "Iteration: 3898 Loss: nan\n",
      "Iteration: 3899 Loss: nan\n",
      "Iteration: 3900 Loss: nan\n",
      "Iteration: 3901 Loss: nan\n",
      "Iteration: 3902 Loss: nan\n",
      "Iteration: 3903 Loss: nan\n",
      "Iteration: 3904 Loss: nan\n",
      "Iteration: 3905 Loss: nan\n",
      "Iteration: 3906 Loss: nan\n",
      "Iteration: 3907 Loss: nan\n",
      "Iteration: 3908 Loss: nan\n",
      "Iteration: 3909 Loss: nan\n",
      "Iteration: 3910 Loss: nan\n",
      "Iteration: 3911 Loss: nan\n",
      "Iteration: 3912 Loss: nan\n",
      "Iteration: 3913 Loss: nan\n",
      "Iteration: 3914 Loss: nan\n",
      "Iteration: 3915 Loss: nan\n",
      "Iteration: 3916 Loss: nan\n",
      "Iteration: 3917 Loss: nan\n",
      "Iteration: 3918 Loss: nan\n",
      "Iteration: 3919 Loss: nan\n",
      "Iteration: 3920 Loss: nan\n",
      "Iteration: 3921 Loss: nan\n",
      "Iteration: 3922 Loss: nan\n",
      "Iteration: 3923 Loss: nan\n",
      "Iteration: 3924 Loss: nan\n",
      "Iteration: 3925 Loss: nan\n",
      "Iteration: 3926 Loss: nan\n",
      "Iteration: 3927 Loss: nan\n",
      "Iteration: 3928 Loss: nan\n",
      "Iteration: 3929 Loss: nan\n",
      "Iteration: 3930 Loss: nan\n",
      "Iteration: 3931 Loss: nan\n",
      "Iteration: 3932 Loss: nan\n",
      "Iteration: 3933 Loss: nan\n",
      "Iteration: 3934 Loss: nan\n",
      "Iteration: 3935 Loss: nan\n",
      "Iteration: 3936 Loss: nan\n",
      "Iteration: 3937 Loss: nan\n",
      "Iteration: 3938 Loss: nan\n",
      "Iteration: 3939 Loss: nan\n",
      "Iteration: 3940 Loss: nan\n",
      "Iteration: 3941 Loss: nan\n",
      "Iteration: 3942 Loss: nan\n",
      "Iteration: 3943 Loss: nan\n",
      "Iteration: 3944 Loss: nan\n",
      "Iteration: 3945 Loss: nan\n",
      "Iteration: 3946 Loss: nan\n",
      "Iteration: 3947 Loss: nan\n",
      "Iteration: 3948 Loss: nan\n",
      "Iteration: 3949 Loss: nan\n",
      "Iteration: 3950 Loss: nan\n",
      "Iteration: 3951 Loss: nan\n",
      "Iteration: 3952 Loss: nan\n",
      "Iteration: 3953 Loss: nan\n",
      "Iteration: 3954 Loss: nan\n",
      "Iteration: 3955 Loss: nan\n",
      "Iteration: 3956 Loss: nan\n",
      "Iteration: 3957 Loss: nan\n",
      "Iteration: 3958 Loss: nan\n",
      "Iteration: 3959 Loss: nan\n",
      "Iteration: 3960 Loss: nan\n",
      "Iteration: 3961 Loss: nan\n",
      "Iteration: 3962 Loss: nan\n",
      "Iteration: 3963 Loss: nan\n",
      "Iteration: 3964 Loss: nan\n",
      "Iteration: 3965 Loss: nan\n",
      "Iteration: 3966 Loss: nan\n",
      "Iteration: 3967 Loss: nan\n",
      "Iteration: 3968 Loss: nan\n",
      "Iteration: 3969 Loss: nan\n",
      "Iteration: 3970 Loss: nan\n",
      "Iteration: 3971 Loss: nan\n",
      "Iteration: 3972 Loss: nan\n",
      "Iteration: 3973 Loss: nan\n",
      "Iteration: 3974 Loss: nan\n",
      "Iteration: 3975 Loss: nan\n",
      "Iteration: 3976 Loss: nan\n",
      "Iteration: 3977 Loss: nan\n",
      "Iteration: 3978 Loss: nan\n",
      "Iteration: 3979 Loss: nan\n",
      "Iteration: 3980 Loss: nan\n",
      "Iteration: 3981 Loss: nan\n",
      "Iteration: 3982 Loss: nan\n",
      "Iteration: 3983 Loss: nan\n",
      "Iteration: 3984 Loss: nan\n",
      "Iteration: 3985 Loss: nan\n",
      "Iteration: 3986 Loss: nan\n",
      "Iteration: 3987 Loss: nan\n",
      "Iteration: 3988 Loss: nan\n",
      "Iteration: 3989 Loss: nan\n",
      "Iteration: 3990 Loss: nan\n",
      "Iteration: 3991 Loss: nan\n",
      "Iteration: 3992 Loss: nan\n",
      "Iteration: 3993 Loss: nan\n",
      "Iteration: 3994 Loss: nan\n",
      "Iteration: 3995 Loss: nan\n",
      "Iteration: 3996 Loss: nan\n",
      "Iteration: 3997 Loss: nan\n",
      "Iteration: 3998 Loss: nan\n",
      "Iteration: 3999 Loss: nan\n",
      "Iteration: 4000 Loss: nan\n",
      "Iteration: 4001 Loss: nan\n",
      "Iteration: 4002 Loss: nan\n",
      "Iteration: 4003 Loss: nan\n",
      "Iteration: 4004 Loss: nan\n",
      "Iteration: 4005 Loss: nan\n",
      "Iteration: 4006 Loss: nan\n",
      "Iteration: 4007 Loss: nan\n",
      "Iteration: 4008 Loss: nan\n",
      "Iteration: 4009 Loss: nan\n",
      "Iteration: 4010 Loss: nan\n",
      "Iteration: 4011 Loss: nan\n",
      "Iteration: 4012 Loss: nan\n",
      "Iteration: 4013 Loss: nan\n",
      "Iteration: 4014 Loss: nan\n",
      "Iteration: 4015 Loss: nan\n",
      "Iteration: 4016 Loss: nan\n",
      "Iteration: 4017 Loss: nan\n",
      "Iteration: 4018 Loss: nan\n",
      "Iteration: 4019 Loss: nan\n",
      "Iteration: 4020 Loss: nan\n",
      "Iteration: 4021 Loss: nan\n",
      "Iteration: 4022 Loss: nan\n",
      "Iteration: 4023 Loss: nan\n",
      "Iteration: 4024 Loss: nan\n",
      "Iteration: 4025 Loss: nan\n",
      "Iteration: 4026 Loss: nan\n",
      "Iteration: 4027 Loss: nan\n",
      "Iteration: 4028 Loss: nan\n",
      "Iteration: 4029 Loss: nan\n",
      "Iteration: 4030 Loss: nan\n",
      "Iteration: 4031 Loss: nan\n",
      "Iteration: 4032 Loss: nan\n",
      "Iteration: 4033 Loss: nan\n",
      "Iteration: 4034 Loss: nan\n",
      "Iteration: 4035 Loss: nan\n",
      "Iteration: 4036 Loss: nan\n",
      "Iteration: 4037 Loss: nan\n",
      "Iteration: 4038 Loss: nan\n",
      "Iteration: 4039 Loss: nan\n",
      "Iteration: 4040 Loss: nan\n",
      "Iteration: 4041 Loss: nan\n",
      "Iteration: 4042 Loss: nan\n",
      "Iteration: 4043 Loss: nan\n",
      "Iteration: 4044 Loss: nan\n",
      "Iteration: 4045 Loss: nan\n",
      "Iteration: 4046 Loss: nan\n",
      "Iteration: 4047 Loss: nan\n",
      "Iteration: 4048 Loss: nan\n",
      "Iteration: 4049 Loss: nan\n",
      "Iteration: 4050 Loss: nan\n",
      "Iteration: 4051 Loss: nan\n",
      "Iteration: 4052 Loss: nan\n",
      "Iteration: 4053 Loss: nan\n",
      "Iteration: 4054 Loss: nan\n",
      "Iteration: 4055 Loss: nan\n",
      "Iteration: 4056 Loss: nan\n",
      "Iteration: 4057 Loss: nan\n",
      "Iteration: 4058 Loss: nan\n",
      "Iteration: 4059 Loss: nan\n",
      "Iteration: 4060 Loss: nan\n",
      "Iteration: 4061 Loss: nan\n",
      "Iteration: 4062 Loss: nan\n",
      "Iteration: 4063 Loss: nan\n",
      "Iteration: 4064 Loss: nan\n",
      "Iteration: 4065 Loss: nan\n",
      "Iteration: 4066 Loss: nan\n",
      "Iteration: 4067 Loss: nan\n",
      "Iteration: 4068 Loss: nan\n",
      "Iteration: 4069 Loss: nan\n",
      "Iteration: 4070 Loss: nan\n",
      "Iteration: 4071 Loss: nan\n",
      "Iteration: 4072 Loss: nan\n",
      "Iteration: 4073 Loss: nan\n",
      "Iteration: 4074 Loss: nan\n",
      "Iteration: 4075 Loss: nan\n",
      "Iteration: 4076 Loss: nan\n",
      "Iteration: 4077 Loss: nan\n",
      "Iteration: 4078 Loss: nan\n",
      "Iteration: 4079 Loss: nan\n",
      "Iteration: 4080 Loss: nan\n",
      "Iteration: 4081 Loss: nan\n",
      "Iteration: 4082 Loss: nan\n",
      "Iteration: 4083 Loss: nan\n",
      "Iteration: 4084 Loss: nan\n",
      "Iteration: 4085 Loss: nan\n",
      "Iteration: 4086 Loss: nan\n",
      "Iteration: 4087 Loss: nan\n",
      "Iteration: 4088 Loss: nan\n",
      "Iteration: 4089 Loss: nan\n",
      "Iteration: 4090 Loss: nan\n",
      "Iteration: 4091 Loss: nan\n",
      "Iteration: 4092 Loss: nan\n",
      "Iteration: 4093 Loss: nan\n",
      "Iteration: 4094 Loss: nan\n",
      "Iteration: 4095 Loss: nan\n",
      "Iteration: 4096 Loss: nan\n",
      "Iteration: 4097 Loss: nan\n",
      "Iteration: 4098 Loss: nan\n",
      "Iteration: 4099 Loss: nan\n",
      "Iteration: 4100 Loss: nan\n",
      "Iteration: 4101 Loss: nan\n",
      "Iteration: 4102 Loss: nan\n",
      "Iteration: 4103 Loss: nan\n",
      "Iteration: 4104 Loss: nan\n",
      "Iteration: 4105 Loss: nan\n",
      "Iteration: 4106 Loss: nan\n",
      "Iteration: 4107 Loss: nan\n",
      "Iteration: 4108 Loss: nan\n",
      "Iteration: 4109 Loss: nan\n",
      "Iteration: 4110 Loss: nan\n",
      "Iteration: 4111 Loss: nan\n",
      "Iteration: 4112 Loss: nan\n",
      "Iteration: 4113 Loss: nan\n",
      "Iteration: 4114 Loss: nan\n",
      "Iteration: 4115 Loss: nan\n",
      "Iteration: 4116 Loss: nan\n",
      "Iteration: 4117 Loss: nan\n",
      "Iteration: 4118 Loss: nan\n",
      "Iteration: 4119 Loss: nan\n",
      "Iteration: 4120 Loss: nan\n",
      "Iteration: 4121 Loss: nan\n",
      "Iteration: 4122 Loss: nan\n",
      "Iteration: 4123 Loss: nan\n",
      "Iteration: 4124 Loss: nan\n",
      "Iteration: 4125 Loss: nan\n",
      "Iteration: 4126 Loss: nan\n",
      "Iteration: 4127 Loss: nan\n",
      "Iteration: 4128 Loss: nan\n",
      "Iteration: 4129 Loss: nan\n",
      "Iteration: 4130 Loss: nan\n",
      "Iteration: 4131 Loss: nan\n",
      "Iteration: 4132 Loss: nan\n",
      "Iteration: 4133 Loss: nan\n",
      "Iteration: 4134 Loss: nan\n",
      "Iteration: 4135 Loss: nan\n",
      "Iteration: 4136 Loss: nan\n",
      "Iteration: 4137 Loss: nan\n",
      "Iteration: 4138 Loss: nan\n",
      "Iteration: 4139 Loss: nan\n",
      "Iteration: 4140 Loss: nan\n",
      "Iteration: 4141 Loss: nan\n",
      "Iteration: 4142 Loss: nan\n",
      "Iteration: 4143 Loss: nan\n",
      "Iteration: 4144 Loss: nan\n",
      "Iteration: 4145 Loss: nan\n",
      "Iteration: 4146 Loss: nan\n",
      "Iteration: 4147 Loss: nan\n",
      "Iteration: 4148 Loss: nan\n",
      "Iteration: 4149 Loss: nan\n",
      "Iteration: 4150 Loss: nan\n",
      "Iteration: 4151 Loss: nan\n",
      "Iteration: 4152 Loss: nan\n",
      "Iteration: 4153 Loss: nan\n",
      "Iteration: 4154 Loss: nan\n",
      "Iteration: 4155 Loss: nan\n",
      "Iteration: 4156 Loss: nan\n",
      "Iteration: 4157 Loss: nan\n",
      "Iteration: 4158 Loss: nan\n",
      "Iteration: 4159 Loss: nan\n",
      "Iteration: 4160 Loss: nan\n",
      "Iteration: 4161 Loss: nan\n",
      "Iteration: 4162 Loss: nan\n",
      "Iteration: 4163 Loss: nan\n",
      "Iteration: 4164 Loss: nan\n",
      "Iteration: 4165 Loss: nan\n",
      "Iteration: 4166 Loss: nan\n",
      "Iteration: 4167 Loss: nan\n",
      "Iteration: 4168 Loss: nan\n",
      "Iteration: 4169 Loss: nan\n",
      "Iteration: 4170 Loss: nan\n",
      "Iteration: 4171 Loss: nan\n",
      "Iteration: 4172 Loss: nan\n",
      "Iteration: 4173 Loss: nan\n",
      "Iteration: 4174 Loss: nan\n",
      "Iteration: 4175 Loss: nan\n",
      "Iteration: 4176 Loss: nan\n",
      "Iteration: 4177 Loss: nan\n",
      "Iteration: 4178 Loss: nan\n",
      "Iteration: 4179 Loss: nan\n",
      "Iteration: 4180 Loss: nan\n",
      "Iteration: 4181 Loss: nan\n",
      "Iteration: 4182 Loss: nan\n",
      "Iteration: 4183 Loss: nan\n",
      "Iteration: 4184 Loss: nan\n",
      "Iteration: 4185 Loss: nan\n",
      "Iteration: 4186 Loss: nan\n",
      "Iteration: 4187 Loss: nan\n",
      "Iteration: 4188 Loss: nan\n",
      "Iteration: 4189 Loss: nan\n",
      "Iteration: 4190 Loss: nan\n",
      "Iteration: 4191 Loss: nan\n",
      "Iteration: 4192 Loss: nan\n",
      "Iteration: 4193 Loss: nan\n",
      "Iteration: 4194 Loss: nan\n",
      "Iteration: 4195 Loss: nan\n",
      "Iteration: 4196 Loss: nan\n",
      "Iteration: 4197 Loss: nan\n",
      "Iteration: 4198 Loss: nan\n",
      "Iteration: 4199 Loss: nan\n",
      "Iteration: 4200 Loss: nan\n",
      "Iteration: 4201 Loss: nan\n",
      "Iteration: 4202 Loss: nan\n",
      "Iteration: 4203 Loss: nan\n",
      "Iteration: 4204 Loss: nan\n",
      "Iteration: 4205 Loss: nan\n",
      "Iteration: 4206 Loss: nan\n",
      "Iteration: 4207 Loss: nan\n",
      "Iteration: 4208 Loss: nan\n",
      "Iteration: 4209 Loss: nan\n",
      "Iteration: 4210 Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4211 Loss: nan\n",
      "Iteration: 4212 Loss: nan\n",
      "Iteration: 4213 Loss: nan\n",
      "Iteration: 4214 Loss: nan\n",
      "Iteration: 4215 Loss: nan\n",
      "Iteration: 4216 Loss: nan\n",
      "Iteration: 4217 Loss: nan\n",
      "Iteration: 4218 Loss: nan\n",
      "Iteration: 4219 Loss: nan\n",
      "Iteration: 4220 Loss: nan\n",
      "Iteration: 4221 Loss: nan\n",
      "Iteration: 4222 Loss: nan\n",
      "Iteration: 4223 Loss: nan\n",
      "Iteration: 4224 Loss: nan\n",
      "Iteration: 4225 Loss: nan\n",
      "Iteration: 4226 Loss: nan\n",
      "Iteration: 4227 Loss: nan\n",
      "Iteration: 4228 Loss: nan\n",
      "Iteration: 4229 Loss: nan\n",
      "Iteration: 4230 Loss: nan\n",
      "Iteration: 4231 Loss: nan\n",
      "Iteration: 4232 Loss: nan\n",
      "Iteration: 4233 Loss: nan\n",
      "Iteration: 4234 Loss: nan\n",
      "Iteration: 4235 Loss: nan\n",
      "Iteration: 4236 Loss: nan\n",
      "Iteration: 4237 Loss: nan\n",
      "Iteration: 4238 Loss: nan\n",
      "Iteration: 4239 Loss: nan\n",
      "Iteration: 4240 Loss: nan\n",
      "Iteration: 4241 Loss: nan\n",
      "Iteration: 4242 Loss: nan\n",
      "Iteration: 4243 Loss: nan\n",
      "Iteration: 4244 Loss: nan\n",
      "Iteration: 4245 Loss: nan\n",
      "Iteration: 4246 Loss: nan\n",
      "Iteration: 4247 Loss: nan\n",
      "Iteration: 4248 Loss: nan\n",
      "Iteration: 4249 Loss: nan\n",
      "Iteration: 4250 Loss: nan\n",
      "Iteration: 4251 Loss: nan\n",
      "Iteration: 4252 Loss: nan\n",
      "Iteration: 4253 Loss: nan\n",
      "Iteration: 4254 Loss: nan\n",
      "Iteration: 4255 Loss: nan\n",
      "Iteration: 4256 Loss: nan\n",
      "Iteration: 4257 Loss: nan\n",
      "Iteration: 4258 Loss: nan\n",
      "Iteration: 4259 Loss: nan\n",
      "Iteration: 4260 Loss: nan\n",
      "Iteration: 4261 Loss: nan\n",
      "Iteration: 4262 Loss: nan\n",
      "Iteration: 4263 Loss: nan\n",
      "Iteration: 4264 Loss: nan\n",
      "Iteration: 4265 Loss: nan\n",
      "Iteration: 4266 Loss: nan\n",
      "Iteration: 4267 Loss: nan\n",
      "Iteration: 4268 Loss: nan\n",
      "Iteration: 4269 Loss: nan\n",
      "Iteration: 4270 Loss: nan\n",
      "Iteration: 4271 Loss: nan\n",
      "Iteration: 4272 Loss: nan\n",
      "Iteration: 4273 Loss: nan\n",
      "Iteration: 4274 Loss: nan\n",
      "Iteration: 4275 Loss: nan\n",
      "Iteration: 4276 Loss: nan\n",
      "Iteration: 4277 Loss: nan\n",
      "Iteration: 4278 Loss: nan\n",
      "Iteration: 4279 Loss: nan\n",
      "Iteration: 4280 Loss: nan\n",
      "Iteration: 4281 Loss: nan\n",
      "Iteration: 4282 Loss: nan\n",
      "Iteration: 4283 Loss: nan\n",
      "Iteration: 4284 Loss: nan\n",
      "Iteration: 4285 Loss: nan\n",
      "Iteration: 4286 Loss: nan\n",
      "Iteration: 4287 Loss: nan\n",
      "Iteration: 4288 Loss: nan\n",
      "Iteration: 4289 Loss: nan\n",
      "Iteration: 4290 Loss: nan\n",
      "Iteration: 4291 Loss: nan\n",
      "Iteration: 4292 Loss: nan\n",
      "Iteration: 4293 Loss: nan\n",
      "Iteration: 4294 Loss: nan\n",
      "Iteration: 4295 Loss: nan\n",
      "Iteration: 4296 Loss: nan\n",
      "Iteration: 4297 Loss: nan\n",
      "Iteration: 4298 Loss: nan\n",
      "Iteration: 4299 Loss: nan\n",
      "Iteration: 4300 Loss: nan\n",
      "Iteration: 4301 Loss: nan\n",
      "Iteration: 4302 Loss: nan\n",
      "Iteration: 4303 Loss: nan\n",
      "Iteration: 4304 Loss: nan\n",
      "Iteration: 4305 Loss: nan\n",
      "Iteration: 4306 Loss: nan\n",
      "Iteration: 4307 Loss: nan\n",
      "Iteration: 4308 Loss: nan\n",
      "Iteration: 4309 Loss: nan\n",
      "Iteration: 4310 Loss: nan\n",
      "Iteration: 4311 Loss: nan\n",
      "Iteration: 4312 Loss: nan\n",
      "Iteration: 4313 Loss: nan\n",
      "Iteration: 4314 Loss: nan\n",
      "Iteration: 4315 Loss: nan\n",
      "Iteration: 4316 Loss: nan\n",
      "Iteration: 4317 Loss: nan\n",
      "Iteration: 4318 Loss: nan\n",
      "Iteration: 4319 Loss: nan\n",
      "Iteration: 4320 Loss: nan\n",
      "Iteration: 4321 Loss: nan\n",
      "Iteration: 4322 Loss: nan\n",
      "Iteration: 4323 Loss: nan\n",
      "Iteration: 4324 Loss: nan\n",
      "Iteration: 4325 Loss: nan\n",
      "Iteration: 4326 Loss: nan\n",
      "Iteration: 4327 Loss: nan\n",
      "Iteration: 4328 Loss: nan\n",
      "Iteration: 4329 Loss: nan\n",
      "Iteration: 4330 Loss: nan\n",
      "Iteration: 4331 Loss: nan\n",
      "Iteration: 4332 Loss: nan\n",
      "Iteration: 4333 Loss: nan\n",
      "Iteration: 4334 Loss: nan\n",
      "Iteration: 4335 Loss: nan\n",
      "Iteration: 4336 Loss: nan\n",
      "Iteration: 4337 Loss: nan\n",
      "Iteration: 4338 Loss: nan\n",
      "Iteration: 4339 Loss: nan\n",
      "Iteration: 4340 Loss: nan\n",
      "Iteration: 4341 Loss: nan\n",
      "Iteration: 4342 Loss: nan\n",
      "Iteration: 4343 Loss: nan\n",
      "Iteration: 4344 Loss: nan\n",
      "Iteration: 4345 Loss: nan\n",
      "Iteration: 4346 Loss: nan\n",
      "Iteration: 4347 Loss: nan\n",
      "Iteration: 4348 Loss: nan\n",
      "Iteration: 4349 Loss: nan\n",
      "Iteration: 4350 Loss: nan\n",
      "Iteration: 4351 Loss: nan\n",
      "Iteration: 4352 Loss: nan\n",
      "Iteration: 4353 Loss: nan\n",
      "Iteration: 4354 Loss: nan\n",
      "Iteration: 4355 Loss: nan\n",
      "Iteration: 4356 Loss: nan\n",
      "Iteration: 4357 Loss: nan\n",
      "Iteration: 4358 Loss: nan\n",
      "Iteration: 4359 Loss: nan\n",
      "Iteration: 4360 Loss: nan\n",
      "Iteration: 4361 Loss: nan\n",
      "Iteration: 4362 Loss: nan\n",
      "Iteration: 4363 Loss: nan\n",
      "Iteration: 4364 Loss: nan\n",
      "Iteration: 4365 Loss: nan\n",
      "Iteration: 4366 Loss: nan\n",
      "Iteration: 4367 Loss: nan\n",
      "Iteration: 4368 Loss: nan\n",
      "Iteration: 4369 Loss: nan\n",
      "Iteration: 4370 Loss: nan\n",
      "Iteration: 4371 Loss: nan\n",
      "Iteration: 4372 Loss: nan\n",
      "Iteration: 4373 Loss: nan\n",
      "Iteration: 4374 Loss: nan\n",
      "Iteration: 4375 Loss: nan\n",
      "Iteration: 4376 Loss: nan\n",
      "Iteration: 4377 Loss: nan\n",
      "Iteration: 4378 Loss: nan\n",
      "Iteration: 4379 Loss: nan\n",
      "Iteration: 4380 Loss: nan\n",
      "Iteration: 4381 Loss: nan\n",
      "Iteration: 4382 Loss: nan\n",
      "Iteration: 4383 Loss: nan\n",
      "Iteration: 4384 Loss: nan\n",
      "Iteration: 4385 Loss: nan\n",
      "Iteration: 4386 Loss: nan\n",
      "Iteration: 4387 Loss: nan\n",
      "Iteration: 4388 Loss: nan\n",
      "Iteration: 4389 Loss: nan\n",
      "Iteration: 4390 Loss: nan\n",
      "Iteration: 4391 Loss: nan\n",
      "Iteration: 4392 Loss: nan\n",
      "Iteration: 4393 Loss: nan\n",
      "Iteration: 4394 Loss: nan\n",
      "Iteration: 4395 Loss: nan\n",
      "Iteration: 4396 Loss: nan\n",
      "Iteration: 4397 Loss: nan\n",
      "Iteration: 4398 Loss: nan\n",
      "Iteration: 4399 Loss: nan\n",
      "Iteration: 4400 Loss: nan\n",
      "Iteration: 4401 Loss: nan\n",
      "Iteration: 4402 Loss: nan\n",
      "Iteration: 4403 Loss: nan\n",
      "Iteration: 4404 Loss: nan\n",
      "Iteration: 4405 Loss: nan\n",
      "Iteration: 4406 Loss: nan\n",
      "Iteration: 4407 Loss: nan\n",
      "Iteration: 4408 Loss: nan\n",
      "Iteration: 4409 Loss: nan\n",
      "Iteration: 4410 Loss: nan\n",
      "Iteration: 4411 Loss: nan\n",
      "Iteration: 4412 Loss: nan\n",
      "Iteration: 4413 Loss: nan\n",
      "Iteration: 4414 Loss: nan\n",
      "Iteration: 4415 Loss: nan\n",
      "Iteration: 4416 Loss: nan\n",
      "Iteration: 4417 Loss: nan\n",
      "Iteration: 4418 Loss: nan\n",
      "Iteration: 4419 Loss: nan\n",
      "Iteration: 4420 Loss: nan\n",
      "Iteration: 4421 Loss: nan\n",
      "Iteration: 4422 Loss: nan\n",
      "Iteration: 4423 Loss: nan\n",
      "Iteration: 4424 Loss: nan\n",
      "Iteration: 4425 Loss: nan\n",
      "Iteration: 4426 Loss: nan\n",
      "Iteration: 4427 Loss: nan\n",
      "Iteration: 4428 Loss: nan\n",
      "Iteration: 4429 Loss: nan\n",
      "Iteration: 4430 Loss: nan\n",
      "Iteration: 4431 Loss: nan\n",
      "Iteration: 4432 Loss: nan\n",
      "Iteration: 4433 Loss: nan\n",
      "Iteration: 4434 Loss: nan\n",
      "Iteration: 4435 Loss: nan\n",
      "Iteration: 4436 Loss: nan\n",
      "Iteration: 4437 Loss: nan\n",
      "Iteration: 4438 Loss: nan\n",
      "Iteration: 4439 Loss: nan\n",
      "Iteration: 4440 Loss: nan\n",
      "Iteration: 4441 Loss: nan\n",
      "Iteration: 4442 Loss: nan\n",
      "Iteration: 4443 Loss: nan\n",
      "Iteration: 4444 Loss: nan\n",
      "Iteration: 4445 Loss: nan\n",
      "Iteration: 4446 Loss: nan\n",
      "Iteration: 4447 Loss: nan\n",
      "Iteration: 4448 Loss: nan\n",
      "Iteration: 4449 Loss: nan\n",
      "Iteration: 4450 Loss: nan\n",
      "Iteration: 4451 Loss: nan\n",
      "Iteration: 4452 Loss: nan\n",
      "Iteration: 4453 Loss: nan\n",
      "Iteration: 4454 Loss: nan\n",
      "Iteration: 4455 Loss: nan\n",
      "Iteration: 4456 Loss: nan\n",
      "Iteration: 4457 Loss: nan\n",
      "Iteration: 4458 Loss: nan\n",
      "Iteration: 4459 Loss: nan\n",
      "Iteration: 4460 Loss: nan\n",
      "Iteration: 4461 Loss: nan\n",
      "Iteration: 4462 Loss: nan\n",
      "Iteration: 4463 Loss: nan\n",
      "Iteration: 4464 Loss: nan\n",
      "Iteration: 4465 Loss: nan\n",
      "Iteration: 4466 Loss: nan\n",
      "Iteration: 4467 Loss: nan\n",
      "Iteration: 4468 Loss: nan\n",
      "Iteration: 4469 Loss: nan\n",
      "Iteration: 4470 Loss: nan\n",
      "Iteration: 4471 Loss: nan\n",
      "Iteration: 4472 Loss: nan\n",
      "Iteration: 4473 Loss: nan\n",
      "Iteration: 4474 Loss: nan\n",
      "Iteration: 4475 Loss: nan\n",
      "Iteration: 4476 Loss: nan\n",
      "Iteration: 4477 Loss: nan\n",
      "Iteration: 4478 Loss: nan\n",
      "Iteration: 4479 Loss: nan\n",
      "Iteration: 4480 Loss: nan\n",
      "Iteration: 4481 Loss: nan\n",
      "Iteration: 4482 Loss: nan\n",
      "Iteration: 4483 Loss: nan\n",
      "Iteration: 4484 Loss: nan\n",
      "Iteration: 4485 Loss: nan\n",
      "Iteration: 4486 Loss: nan\n",
      "Iteration: 4487 Loss: nan\n",
      "Iteration: 4488 Loss: nan\n",
      "Iteration: 4489 Loss: nan\n",
      "Iteration: 4490 Loss: nan\n",
      "Iteration: 4491 Loss: nan\n",
      "Iteration: 4492 Loss: nan\n",
      "Iteration: 4493 Loss: nan\n",
      "Iteration: 4494 Loss: nan\n",
      "Iteration: 4495 Loss: nan\n",
      "Iteration: 4496 Loss: nan\n",
      "Iteration: 4497 Loss: nan\n",
      "Iteration: 4498 Loss: nan\n",
      "Iteration: 4499 Loss: nan\n",
      "Iteration: 4500 Loss: nan\n",
      "Iteration: 4501 Loss: nan\n",
      "Iteration: 4502 Loss: nan\n",
      "Iteration: 4503 Loss: nan\n",
      "Iteration: 4504 Loss: nan\n",
      "Iteration: 4505 Loss: nan\n",
      "Iteration: 4506 Loss: nan\n",
      "Iteration: 4507 Loss: nan\n",
      "Iteration: 4508 Loss: nan\n",
      "Iteration: 4509 Loss: nan\n",
      "Iteration: 4510 Loss: nan\n",
      "Iteration: 4511 Loss: nan\n",
      "Iteration: 4512 Loss: nan\n",
      "Iteration: 4513 Loss: nan\n",
      "Iteration: 4514 Loss: nan\n",
      "Iteration: 4515 Loss: nan\n",
      "Iteration: 4516 Loss: nan\n",
      "Iteration: 4517 Loss: nan\n",
      "Iteration: 4518 Loss: nan\n",
      "Iteration: 4519 Loss: nan\n",
      "Iteration: 4520 Loss: nan\n",
      "Iteration: 4521 Loss: nan\n",
      "Iteration: 4522 Loss: nan\n",
      "Iteration: 4523 Loss: nan\n",
      "Iteration: 4524 Loss: nan\n",
      "Iteration: 4525 Loss: nan\n",
      "Iteration: 4526 Loss: nan\n",
      "Iteration: 4527 Loss: nan\n",
      "Iteration: 4528 Loss: nan\n",
      "Iteration: 4529 Loss: nan\n",
      "Iteration: 4530 Loss: nan\n",
      "Iteration: 4531 Loss: nan\n",
      "Iteration: 4532 Loss: nan\n",
      "Iteration: 4533 Loss: nan\n",
      "Iteration: 4534 Loss: nan\n",
      "Iteration: 4535 Loss: nan\n",
      "Iteration: 4536 Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4537 Loss: nan\n",
      "Iteration: 4538 Loss: nan\n",
      "Iteration: 4539 Loss: nan\n",
      "Iteration: 4540 Loss: nan\n",
      "Iteration: 4541 Loss: nan\n",
      "Iteration: 4542 Loss: nan\n",
      "Iteration: 4543 Loss: nan\n",
      "Iteration: 4544 Loss: nan\n",
      "Iteration: 4545 Loss: nan\n",
      "Iteration: 4546 Loss: nan\n",
      "Iteration: 4547 Loss: nan\n",
      "Iteration: 4548 Loss: nan\n",
      "Iteration: 4549 Loss: nan\n",
      "Iteration: 4550 Loss: nan\n",
      "Iteration: 4551 Loss: nan\n",
      "Iteration: 4552 Loss: nan\n",
      "Iteration: 4553 Loss: nan\n",
      "Iteration: 4554 Loss: nan\n",
      "Iteration: 4555 Loss: nan\n",
      "Iteration: 4556 Loss: nan\n",
      "Iteration: 4557 Loss: nan\n",
      "Iteration: 4558 Loss: nan\n",
      "Iteration: 4559 Loss: nan\n",
      "Iteration: 4560 Loss: nan\n",
      "Iteration: 4561 Loss: nan\n",
      "Iteration: 4562 Loss: nan\n",
      "Iteration: 4563 Loss: nan\n",
      "Iteration: 4564 Loss: nan\n",
      "Iteration: 4565 Loss: nan\n",
      "Iteration: 4566 Loss: nan\n",
      "Iteration: 4567 Loss: nan\n",
      "Iteration: 4568 Loss: nan\n",
      "Iteration: 4569 Loss: nan\n",
      "Iteration: 4570 Loss: nan\n",
      "Iteration: 4571 Loss: nan\n",
      "Iteration: 4572 Loss: nan\n",
      "Iteration: 4573 Loss: nan\n",
      "Iteration: 4574 Loss: nan\n",
      "Iteration: 4575 Loss: nan\n",
      "Iteration: 4576 Loss: nan\n",
      "Iteration: 4577 Loss: nan\n",
      "Iteration: 4578 Loss: nan\n",
      "Iteration: 4579 Loss: nan\n",
      "Iteration: 4580 Loss: nan\n",
      "Iteration: 4581 Loss: nan\n",
      "Iteration: 4582 Loss: nan\n",
      "Iteration: 4583 Loss: nan\n",
      "Iteration: 4584 Loss: nan\n",
      "Iteration: 4585 Loss: nan\n",
      "Iteration: 4586 Loss: nan\n",
      "Iteration: 4587 Loss: nan\n",
      "Iteration: 4588 Loss: nan\n",
      "Iteration: 4589 Loss: nan\n",
      "Iteration: 4590 Loss: nan\n",
      "Iteration: 4591 Loss: nan\n",
      "Iteration: 4592 Loss: nan\n",
      "Iteration: 4593 Loss: nan\n",
      "Iteration: 4594 Loss: nan\n",
      "Iteration: 4595 Loss: nan\n",
      "Iteration: 4596 Loss: nan\n",
      "Iteration: 4597 Loss: nan\n",
      "Iteration: 4598 Loss: nan\n",
      "Iteration: 4599 Loss: nan\n",
      "Iteration: 4600 Loss: nan\n",
      "Iteration: 4601 Loss: nan\n",
      "Iteration: 4602 Loss: nan\n",
      "Iteration: 4603 Loss: nan\n",
      "Iteration: 4604 Loss: nan\n",
      "Iteration: 4605 Loss: nan\n",
      "Iteration: 4606 Loss: nan\n",
      "Iteration: 4607 Loss: nan\n",
      "Iteration: 4608 Loss: nan\n",
      "Iteration: 4609 Loss: nan\n",
      "Iteration: 4610 Loss: nan\n",
      "Iteration: 4611 Loss: nan\n",
      "Iteration: 4612 Loss: nan\n",
      "Iteration: 4613 Loss: nan\n",
      "Iteration: 4614 Loss: nan\n",
      "Iteration: 4615 Loss: nan\n",
      "Iteration: 4616 Loss: nan\n",
      "Iteration: 4617 Loss: nan\n",
      "Iteration: 4618 Loss: nan\n",
      "Iteration: 4619 Loss: nan\n",
      "Iteration: 4620 Loss: nan\n",
      "Iteration: 4621 Loss: nan\n",
      "Iteration: 4622 Loss: nan\n",
      "Iteration: 4623 Loss: nan\n",
      "Iteration: 4624 Loss: nan\n",
      "Iteration: 4625 Loss: nan\n",
      "Iteration: 4626 Loss: nan\n",
      "Iteration: 4627 Loss: nan\n",
      "Iteration: 4628 Loss: nan\n",
      "Iteration: 4629 Loss: nan\n",
      "Iteration: 4630 Loss: nan\n",
      "Iteration: 4631 Loss: nan\n",
      "Iteration: 4632 Loss: nan\n",
      "Iteration: 4633 Loss: nan\n",
      "Iteration: 4634 Loss: nan\n",
      "Iteration: 4635 Loss: nan\n",
      "Iteration: 4636 Loss: nan\n",
      "Iteration: 4637 Loss: nan\n",
      "Iteration: 4638 Loss: nan\n",
      "Iteration: 4639 Loss: nan\n",
      "Iteration: 4640 Loss: nan\n",
      "Iteration: 4641 Loss: nan\n",
      "Iteration: 4642 Loss: nan\n",
      "Iteration: 4643 Loss: nan\n",
      "Iteration: 4644 Loss: nan\n",
      "Iteration: 4645 Loss: nan\n",
      "Iteration: 4646 Loss: nan\n",
      "Iteration: 4647 Loss: nan\n",
      "Iteration: 4648 Loss: nan\n",
      "Iteration: 4649 Loss: nan\n",
      "Iteration: 4650 Loss: nan\n",
      "Iteration: 4651 Loss: nan\n",
      "Iteration: 4652 Loss: nan\n",
      "Iteration: 4653 Loss: nan\n",
      "Iteration: 4654 Loss: nan\n",
      "Iteration: 4655 Loss: nan\n",
      "Iteration: 4656 Loss: nan\n",
      "Iteration: 4657 Loss: nan\n",
      "Iteration: 4658 Loss: nan\n",
      "Iteration: 4659 Loss: nan\n",
      "Iteration: 4660 Loss: nan\n",
      "Iteration: 4661 Loss: nan\n",
      "Iteration: 4662 Loss: nan\n",
      "Iteration: 4663 Loss: nan\n",
      "Iteration: 4664 Loss: nan\n",
      "Iteration: 4665 Loss: nan\n",
      "Iteration: 4666 Loss: nan\n",
      "Iteration: 4667 Loss: nan\n",
      "Iteration: 4668 Loss: nan\n",
      "Iteration: 4669 Loss: nan\n",
      "Iteration: 4670 Loss: nan\n",
      "Iteration: 4671 Loss: nan\n",
      "Iteration: 4672 Loss: nan\n",
      "Iteration: 4673 Loss: nan\n",
      "Iteration: 4674 Loss: nan\n",
      "Iteration: 4675 Loss: nan\n",
      "Iteration: 4676 Loss: nan\n",
      "Iteration: 4677 Loss: nan\n",
      "Iteration: 4678 Loss: nan\n",
      "Iteration: 4679 Loss: nan\n",
      "Iteration: 4680 Loss: nan\n",
      "Iteration: 4681 Loss: nan\n",
      "Iteration: 4682 Loss: nan\n",
      "Iteration: 4683 Loss: nan\n",
      "Iteration: 4684 Loss: nan\n",
      "Iteration: 4685 Loss: nan\n",
      "Iteration: 4686 Loss: nan\n",
      "Iteration: 4687 Loss: nan\n",
      "Iteration: 4688 Loss: nan\n",
      "Iteration: 4689 Loss: nan\n",
      "Iteration: 4690 Loss: nan\n",
      "Iteration: 4691 Loss: nan\n",
      "Iteration: 4692 Loss: nan\n",
      "Iteration: 4693 Loss: nan\n",
      "Iteration: 4694 Loss: nan\n",
      "Iteration: 4695 Loss: nan\n",
      "Iteration: 4696 Loss: nan\n",
      "Iteration: 4697 Loss: nan\n",
      "Iteration: 4698 Loss: nan\n",
      "Iteration: 4699 Loss: nan\n",
      "Iteration: 4700 Loss: nan\n",
      "Iteration: 4701 Loss: nan\n",
      "Iteration: 4702 Loss: nan\n",
      "Iteration: 4703 Loss: nan\n",
      "Iteration: 4704 Loss: nan\n",
      "Iteration: 4705 Loss: nan\n",
      "Iteration: 4706 Loss: nan\n",
      "Iteration: 4707 Loss: nan\n",
      "Iteration: 4708 Loss: nan\n",
      "Iteration: 4709 Loss: nan\n",
      "Iteration: 4710 Loss: nan\n",
      "Iteration: 4711 Loss: nan\n",
      "Iteration: 4712 Loss: nan\n",
      "Iteration: 4713 Loss: nan\n",
      "Iteration: 4714 Loss: nan\n",
      "Iteration: 4715 Loss: nan\n",
      "Iteration: 4716 Loss: nan\n",
      "Iteration: 4717 Loss: nan\n",
      "Iteration: 4718 Loss: nan\n",
      "Iteration: 4719 Loss: nan\n",
      "Iteration: 4720 Loss: nan\n",
      "Iteration: 4721 Loss: nan\n",
      "Iteration: 4722 Loss: nan\n",
      "Iteration: 4723 Loss: nan\n",
      "Iteration: 4724 Loss: nan\n",
      "Iteration: 4725 Loss: nan\n",
      "Iteration: 4726 Loss: nan\n",
      "Iteration: 4727 Loss: nan\n",
      "Iteration: 4728 Loss: nan\n",
      "Iteration: 4729 Loss: nan\n",
      "Iteration: 4730 Loss: nan\n",
      "Iteration: 4731 Loss: nan\n",
      "Iteration: 4732 Loss: nan\n",
      "Iteration: 4733 Loss: nan\n",
      "Iteration: 4734 Loss: nan\n",
      "Iteration: 4735 Loss: nan\n",
      "Iteration: 4736 Loss: nan\n",
      "Iteration: 4737 Loss: nan\n",
      "Iteration: 4738 Loss: nan\n",
      "Iteration: 4739 Loss: nan\n",
      "Iteration: 4740 Loss: nan\n",
      "Iteration: 4741 Loss: nan\n",
      "Iteration: 4742 Loss: nan\n",
      "Iteration: 4743 Loss: nan\n",
      "Iteration: 4744 Loss: nan\n",
      "Iteration: 4745 Loss: nan\n",
      "Iteration: 4746 Loss: nan\n",
      "Iteration: 4747 Loss: nan\n",
      "Iteration: 4748 Loss: nan\n",
      "Iteration: 4749 Loss: nan\n",
      "Iteration: 4750 Loss: nan\n",
      "Iteration: 4751 Loss: nan\n",
      "Iteration: 4752 Loss: nan\n",
      "Iteration: 4753 Loss: nan\n",
      "Iteration: 4754 Loss: nan\n",
      "Iteration: 4755 Loss: nan\n",
      "Iteration: 4756 Loss: nan\n",
      "Iteration: 4757 Loss: nan\n",
      "Iteration: 4758 Loss: nan\n",
      "Iteration: 4759 Loss: nan\n",
      "Iteration: 4760 Loss: nan\n",
      "Iteration: 4761 Loss: nan\n",
      "Iteration: 4762 Loss: nan\n",
      "Iteration: 4763 Loss: nan\n",
      "Iteration: 4764 Loss: nan\n",
      "Iteration: 4765 Loss: nan\n",
      "Iteration: 4766 Loss: nan\n",
      "Iteration: 4767 Loss: nan\n",
      "Iteration: 4768 Loss: nan\n",
      "Iteration: 4769 Loss: nan\n",
      "Iteration: 4770 Loss: nan\n",
      "Iteration: 4771 Loss: nan\n",
      "Iteration: 4772 Loss: nan\n",
      "Iteration: 4773 Loss: nan\n",
      "Iteration: 4774 Loss: nan\n",
      "Iteration: 4775 Loss: nan\n",
      "Iteration: 4776 Loss: nan\n",
      "Iteration: 4777 Loss: nan\n",
      "Iteration: 4778 Loss: nan\n",
      "Iteration: 4779 Loss: nan\n",
      "Iteration: 4780 Loss: nan\n",
      "Iteration: 4781 Loss: nan\n",
      "Iteration: 4782 Loss: nan\n",
      "Iteration: 4783 Loss: nan\n",
      "Iteration: 4784 Loss: nan\n",
      "Iteration: 4785 Loss: nan\n",
      "Iteration: 4786 Loss: nan\n",
      "Iteration: 4787 Loss: nan\n",
      "Iteration: 4788 Loss: nan\n",
      "Iteration: 4789 Loss: nan\n",
      "Iteration: 4790 Loss: nan\n",
      "Iteration: 4791 Loss: nan\n",
      "Iteration: 4792 Loss: nan\n",
      "Iteration: 4793 Loss: nan\n",
      "Iteration: 4794 Loss: nan\n",
      "Iteration: 4795 Loss: nan\n",
      "Iteration: 4796 Loss: nan\n",
      "Iteration: 4797 Loss: nan\n",
      "Iteration: 4798 Loss: nan\n",
      "Iteration: 4799 Loss: nan\n",
      "Iteration: 4800 Loss: nan\n",
      "Iteration: 4801 Loss: nan\n",
      "Iteration: 4802 Loss: nan\n",
      "Iteration: 4803 Loss: nan\n",
      "Iteration: 4804 Loss: nan\n",
      "Iteration: 4805 Loss: nan\n",
      "Iteration: 4806 Loss: nan\n",
      "Iteration: 4807 Loss: nan\n",
      "Iteration: 4808 Loss: nan\n",
      "Iteration: 4809 Loss: nan\n",
      "Iteration: 4810 Loss: nan\n",
      "Iteration: 4811 Loss: nan\n",
      "Iteration: 4812 Loss: nan\n",
      "Iteration: 4813 Loss: nan\n",
      "Iteration: 4814 Loss: nan\n",
      "Iteration: 4815 Loss: nan\n",
      "Iteration: 4816 Loss: nan\n",
      "Iteration: 4817 Loss: nan\n",
      "Iteration: 4818 Loss: nan\n",
      "Iteration: 4819 Loss: nan\n",
      "Iteration: 4820 Loss: nan\n",
      "Iteration: 4821 Loss: nan\n",
      "Iteration: 4822 Loss: nan\n",
      "Iteration: 4823 Loss: nan\n",
      "Iteration: 4824 Loss: nan\n",
      "Iteration: 4825 Loss: nan\n",
      "Iteration: 4826 Loss: nan\n",
      "Iteration: 4827 Loss: nan\n",
      "Iteration: 4828 Loss: nan\n",
      "Iteration: 4829 Loss: nan\n",
      "Iteration: 4830 Loss: nan\n",
      "Iteration: 4831 Loss: nan\n",
      "Iteration: 4832 Loss: nan\n",
      "Iteration: 4833 Loss: nan\n",
      "Iteration: 4834 Loss: nan\n",
      "Iteration: 4835 Loss: nan\n",
      "Iteration: 4836 Loss: nan\n",
      "Iteration: 4837 Loss: nan\n",
      "Iteration: 4838 Loss: nan\n",
      "Iteration: 4839 Loss: nan\n",
      "Iteration: 4840 Loss: nan\n",
      "Iteration: 4841 Loss: nan\n",
      "Iteration: 4842 Loss: nan\n",
      "Iteration: 4843 Loss: nan\n",
      "Iteration: 4844 Loss: nan\n",
      "Iteration: 4845 Loss: nan\n",
      "Iteration: 4846 Loss: nan\n",
      "Iteration: 4847 Loss: nan\n",
      "Iteration: 4848 Loss: nan\n",
      "Iteration: 4849 Loss: nan\n",
      "Iteration: 4850 Loss: nan\n",
      "Iteration: 4851 Loss: nan\n",
      "Iteration: 4852 Loss: nan\n",
      "Iteration: 4853 Loss: nan\n",
      "Iteration: 4854 Loss: nan\n",
      "Iteration: 4855 Loss: nan\n",
      "Iteration: 4856 Loss: nan\n",
      "Iteration: 4857 Loss: nan\n",
      "Iteration: 4858 Loss: nan\n",
      "Iteration: 4859 Loss: nan\n",
      "Iteration: 4860 Loss: nan\n",
      "Iteration: 4861 Loss: nan\n",
      "Iteration: 4862 Loss: nan\n",
      "Iteration: 4863 Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4864 Loss: nan\n",
      "Iteration: 4865 Loss: nan\n",
      "Iteration: 4866 Loss: nan\n",
      "Iteration: 4867 Loss: nan\n",
      "Iteration: 4868 Loss: nan\n",
      "Iteration: 4869 Loss: nan\n",
      "Iteration: 4870 Loss: nan\n",
      "Iteration: 4871 Loss: nan\n",
      "Iteration: 4872 Loss: nan\n",
      "Iteration: 4873 Loss: nan\n",
      "Iteration: 4874 Loss: nan\n",
      "Iteration: 4875 Loss: nan\n",
      "Iteration: 4876 Loss: nan\n",
      "Iteration: 4877 Loss: nan\n",
      "Iteration: 4878 Loss: nan\n",
      "Iteration: 4879 Loss: nan\n",
      "Iteration: 4880 Loss: nan\n",
      "Iteration: 4881 Loss: nan\n",
      "Iteration: 4882 Loss: nan\n",
      "Iteration: 4883 Loss: nan\n",
      "Iteration: 4884 Loss: nan\n",
      "Iteration: 4885 Loss: nan\n",
      "Iteration: 4886 Loss: nan\n",
      "Iteration: 4887 Loss: nan\n",
      "Iteration: 4888 Loss: nan\n",
      "Iteration: 4889 Loss: nan\n",
      "Iteration: 4890 Loss: nan\n",
      "Iteration: 4891 Loss: nan\n",
      "Iteration: 4892 Loss: nan\n",
      "Iteration: 4893 Loss: nan\n",
      "Iteration: 4894 Loss: nan\n",
      "Iteration: 4895 Loss: nan\n",
      "Iteration: 4896 Loss: nan\n",
      "Iteration: 4897 Loss: nan\n",
      "Iteration: 4898 Loss: nan\n",
      "Iteration: 4899 Loss: nan\n",
      "Iteration: 4900 Loss: nan\n",
      "Iteration: 4901 Loss: nan\n",
      "Iteration: 4902 Loss: nan\n",
      "Iteration: 4903 Loss: nan\n",
      "Iteration: 4904 Loss: nan\n",
      "Iteration: 4905 Loss: nan\n",
      "Iteration: 4906 Loss: nan\n",
      "Iteration: 4907 Loss: nan\n",
      "Iteration: 4908 Loss: nan\n",
      "Iteration: 4909 Loss: nan\n",
      "Iteration: 4910 Loss: nan\n",
      "Iteration: 4911 Loss: nan\n",
      "Iteration: 4912 Loss: nan\n",
      "Iteration: 4913 Loss: nan\n",
      "Iteration: 4914 Loss: nan\n",
      "Iteration: 4915 Loss: nan\n",
      "Iteration: 4916 Loss: nan\n",
      "Iteration: 4917 Loss: nan\n",
      "Iteration: 4918 Loss: nan\n",
      "Iteration: 4919 Loss: nan\n",
      "Iteration: 4920 Loss: nan\n",
      "Iteration: 4921 Loss: nan\n",
      "Iteration: 4922 Loss: nan\n",
      "Iteration: 4923 Loss: nan\n",
      "Iteration: 4924 Loss: nan\n",
      "Iteration: 4925 Loss: nan\n",
      "Iteration: 4926 Loss: nan\n",
      "Iteration: 4927 Loss: nan\n",
      "Iteration: 4928 Loss: nan\n",
      "Iteration: 4929 Loss: nan\n",
      "Iteration: 4930 Loss: nan\n",
      "Iteration: 4931 Loss: nan\n",
      "Iteration: 4932 Loss: nan\n",
      "Iteration: 4933 Loss: nan\n",
      "Iteration: 4934 Loss: nan\n",
      "Iteration: 4935 Loss: nan\n",
      "Iteration: 4936 Loss: nan\n",
      "Iteration: 4937 Loss: nan\n",
      "Iteration: 4938 Loss: nan\n",
      "Iteration: 4939 Loss: nan\n",
      "Iteration: 4940 Loss: nan\n",
      "Iteration: 4941 Loss: nan\n",
      "Iteration: 4942 Loss: nan\n",
      "Iteration: 4943 Loss: nan\n",
      "Iteration: 4944 Loss: nan\n",
      "Iteration: 4945 Loss: nan\n",
      "Iteration: 4946 Loss: nan\n",
      "Iteration: 4947 Loss: nan\n",
      "Iteration: 4948 Loss: nan\n",
      "Iteration: 4949 Loss: nan\n",
      "Iteration: 4950 Loss: nan\n",
      "Iteration: 4951 Loss: nan\n",
      "Iteration: 4952 Loss: nan\n",
      "Iteration: 4953 Loss: nan\n",
      "Iteration: 4954 Loss: nan\n",
      "Iteration: 4955 Loss: nan\n",
      "Iteration: 4956 Loss: nan\n",
      "Iteration: 4957 Loss: nan\n",
      "Iteration: 4958 Loss: nan\n",
      "Iteration: 4959 Loss: nan\n",
      "Iteration: 4960 Loss: nan\n",
      "Iteration: 4961 Loss: nan\n",
      "Iteration: 4962 Loss: nan\n",
      "Iteration: 4963 Loss: nan\n",
      "Iteration: 4964 Loss: nan\n",
      "Iteration: 4965 Loss: nan\n",
      "Iteration: 4966 Loss: nan\n",
      "Iteration: 4967 Loss: nan\n",
      "Iteration: 4968 Loss: nan\n",
      "Iteration: 4969 Loss: nan\n",
      "Iteration: 4970 Loss: nan\n",
      "Iteration: 4971 Loss: nan\n",
      "Iteration: 4972 Loss: nan\n",
      "Iteration: 4973 Loss: nan\n",
      "Iteration: 4974 Loss: nan\n",
      "Iteration: 4975 Loss: nan\n",
      "Iteration: 4976 Loss: nan\n",
      "Iteration: 4977 Loss: nan\n",
      "Iteration: 4978 Loss: nan\n",
      "Iteration: 4979 Loss: nan\n",
      "Iteration: 4980 Loss: nan\n",
      "Iteration: 4981 Loss: nan\n",
      "Iteration: 4982 Loss: nan\n",
      "Iteration: 4983 Loss: nan\n",
      "Iteration: 4984 Loss: nan\n",
      "Iteration: 4985 Loss: nan\n",
      "Iteration: 4986 Loss: nan\n",
      "Iteration: 4987 Loss: nan\n",
      "Iteration: 4988 Loss: nan\n",
      "Iteration: 4989 Loss: nan\n",
      "Iteration: 4990 Loss: nan\n",
      "Iteration: 4991 Loss: nan\n",
      "Iteration: 4992 Loss: nan\n",
      "Iteration: 4993 Loss: nan\n",
      "Iteration: 4994 Loss: nan\n",
      "Iteration: 4995 Loss: nan\n",
      "Iteration: 4996 Loss: nan\n",
      "Iteration: 4997 Loss: nan\n",
      "Iteration: 4998 Loss: nan\n",
      "Iteration: 4999 Loss: nan\n",
      "Iteration: 5000 Loss: nan\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f569fdd2c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, losses = train_5000(var_model,train_loader,optimizer,loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11269, 53975) (11269,)\n",
      "(0, 53975) (0,)\n",
      "Iteration: 1 Loss: 0.3622467517852783\n",
      "Iteration: 2 Loss: -1.376916766166687\n",
      "Iteration: 3 Loss: -3.7766125202178955\n",
      "Iteration: 4 Loss: -13.707437515258789\n",
      "Iteration: 5 Loss: -27.997180938720703\n",
      "Iteration: 6 Loss: -41.2104606628418\n",
      "Iteration: 7 Loss: -130.37808227539062\n",
      "Iteration: 8 Loss: -385.4134521484375\n",
      "Iteration: 9 Loss: -529.7797241210938\n",
      "Iteration: 10 Loss: -1071.483154296875\n",
      "Iteration: 11 Loss: -1781.5927734375\n",
      "Iteration: 12 Loss: -6194.84423828125\n",
      "Iteration: 13 Loss: -12807.44140625\n",
      "Iteration: 14 Loss: -23499.001953125\n",
      "Iteration: 15 Loss: -42617.26953125\n",
      "Iteration: 16 Loss: -100649.3828125\n",
      "Iteration: 17 Loss: -240658.921875\n",
      "Iteration: 18 Loss: -317381.875\n",
      "Iteration: 19 Loss: -1145876.25\n",
      "Iteration: 20 Loss: -1861345.125\n",
      "Iteration: 21 Loss: -3135172.75\n",
      "Iteration: 22 Loss: -12349751.0\n",
      "Iteration: 23 Loss: -18653986.0\n",
      "Iteration: 24 Loss: -78752816.0\n",
      "Iteration: 25 Loss: -109641760.0\n",
      "Iteration: 26 Loss: -280338368.0\n",
      "Iteration: 27 Loss: -705534592.0\n",
      "Iteration: 28 Loss: -1394209280.0\n",
      "Iteration: 29 Loss: -3616339712.0\n",
      "Iteration: 30 Loss: -11694696448.0\n",
      "Iteration: 31 Loss: -13088016384.0\n",
      "Iteration: 32 Loss: -65478664192.0\n",
      "Iteration: 33 Loss: -99002900480.0\n",
      "Iteration: 34 Loss: -345230082048.0\n",
      "Iteration: 35 Loss: -944072622080.0\n",
      "Iteration: 36 Loss: -2325728722944.0\n",
      "Iteration: 37 Loss: -7077324914688.0\n",
      "Iteration: 38 Loss: -44984745066496.0\n",
      "Iteration: 39 Loss: -80430049001472.0\n",
      "Iteration: 40 Loss: -392470219194368.0\n",
      "Iteration: 41 Loss: -275434440753152.0\n",
      "Iteration: 42 Loss: -210987718279168.0\n",
      "Iteration: 43 Loss: -2283833558827008.0\n",
      "Iteration: 44 Loss: -5579941517197312.0\n",
      "Iteration: 45 Loss: -1.2237465632899072e+16\n",
      "Iteration: 46 Loss: -3.279765661496115e+16\n",
      "Iteration: 47 Loss: -1.3431143559646413e+17\n",
      "Iteration: 48 Loss: -3.0292988454240256e+17\n",
      "Iteration: 49 Loss: -1.7943048794773914e+17\n",
      "Iteration: 50 Loss: -1.6259330561435238e+18\n",
      "Iteration: 51 Loss: -3.408520130966061e+18\n",
      "Iteration: 52 Loss: -8.058376795063321e+18\n",
      "Iteration: 53 Loss: -1.5278458437319524e+19\n",
      "Iteration: 54 Loss: -6.261738951198271e+19\n",
      "Iteration: 55 Loss: -9.50963912504393e+19\n",
      "Iteration: 56 Loss: -1.684335705324939e+20\n",
      "Iteration: 57 Loss: -2.6762598564402666e+20\n",
      "Iteration: 58 Loss: -1.3420836664804995e+21\n",
      "Iteration: 59 Loss: -1.694209988245437e+21\n",
      "Iteration: 60 Loss: -5.291530840826775e+21\n",
      "Iteration: 61 Loss: -1.9984626519064588e+22\n",
      "Iteration: 62 Loss: -2.8016379871514335e+22\n",
      "Iteration: 63 Loss: -1.221602389716165e+23\n",
      "Iteration: 64 Loss: -9.69539971811903e+22\n",
      "Iteration: 65 Loss: -4.720680335169158e+23\n",
      "Iteration: 66 Loss: -1.277396386621349e+24\n",
      "Iteration: 67 Loss: -1.9698393129059644e+24\n",
      "Iteration: 68 Loss: -1.1871545110902353e+25\n",
      "Iteration: 69 Loss: -2.409492035983655e+25\n",
      "Iteration: 70 Loss: -5.925881872447455e+25\n",
      "Iteration: 71 Loss: -4.541228218269252e+25\n",
      "Iteration: 72 Loss: -3.163166139150908e+26\n",
      "Iteration: 73 Loss: -2.5076568457405022e+26\n",
      "Iteration: 74 Loss: -2.7899806113333003e+27\n",
      "Iteration: 75 Loss: -3.808078257706314e+27\n",
      "Iteration: 76 Loss: -3.3554004707997746e+27\n",
      "Iteration: 77 Loss: -9.675386921566282e+27\n",
      "Iteration: 78 Loss: -1.4288803097013831e+28\n",
      "Iteration: 79 Loss: -4.649153746339124e+28\n",
      "Iteration: 80 Loss: -1.3056430019456782e+29\n",
      "Iteration: 81 Loss: -2.2899999891239794e+29\n",
      "Iteration: 82 Loss: -9.116750635299204e+29\n",
      "Iteration: 83 Loss: -1.0914730849571522e+30\n",
      "Iteration: 84 Loss: -2.204615258536532e+30\n",
      "Iteration: 85 Loss: -4.6939660952838034e+30\n",
      "Iteration: 86 Loss: -1.6435918339573561e+31\n",
      "Iteration: 87 Loss: -1.248026832753917e+31\n",
      "Iteration: 88 Loss: -4.626647121464854e+31\n",
      "Iteration: 89 Loss: -1.8669168698643813e+32\n",
      "Iteration: 90 Loss: -2.8826654346267563e+32\n",
      "Iteration: 91 Loss: -1.285010852369148e+33\n",
      "Iteration: 92 Loss: -3.016365628177787e+33\n",
      "Iteration: 93 Loss: -8.561868753356431e+33\n",
      "Iteration: 94 Loss: -7.588392320543665e+33\n",
      "Iteration: 95 Loss: -6.203170469582373e+34\n",
      "Iteration: 96 Loss: -1.009363621305227e+35\n",
      "Iteration: 97 Loss: -2.4409776961383517e+35\n",
      "Iteration: 98 Loss: -2.5512430951767665e+35\n",
      "Iteration: 99 Loss: -1.2317367118450009e+36\n",
      "Iteration: 100 Loss: -inf\n",
      "Iteration: 101 Loss: -inf\n",
      "Iteration: 102 Loss: -inf\n",
      "Iteration: 103 Loss: -inf\n",
      "Iteration: 104 Loss: -inf\n",
      "Iteration: 105 Loss: -inf\n",
      "Iteration: 106 Loss: -inf\n",
      "Iteration: 107 Loss: -inf\n",
      "Iteration: 108 Loss: -inf\n",
      "Iteration: 109 Loss: -inf\n",
      "Iteration: 110 Loss: -inf\n",
      "Iteration: 111 Loss: -inf\n",
      "Iteration: 112 Loss: -inf\n",
      "Iteration: 113 Loss: -inf\n",
      "Iteration: 114 Loss: -inf\n",
      "Iteration: 115 Loss: -inf\n",
      "Iteration: 116 Loss: -inf\n",
      "Iteration: 117 Loss: -inf\n",
      "Iteration: 118 Loss: -inf\n",
      "Iteration: 119 Loss: -inf\n",
      "Iteration: 120 Loss: -inf\n",
      "Iteration: 121 Loss: -inf\n",
      "Iteration: 122 Loss: -inf\n",
      "Iteration: 123 Loss: -inf\n",
      "Iteration: 124 Loss: -inf\n",
      "Iteration: 125 Loss: -inf\n",
      "Iteration: 126 Loss: -inf\n",
      "Iteration: 127 Loss: -inf\n",
      "Iteration: 128 Loss: -inf\n",
      "Iteration: 129 Loss: -inf\n",
      "Iteration: 130 Loss: -inf\n",
      "Iteration: 131 Loss: -inf\n",
      "Iteration: 132 Loss: -inf\n",
      "Iteration: 133 Loss: -inf\n",
      "Iteration: 134 Loss: -inf\n",
      "Iteration: 135 Loss: -inf\n",
      "Iteration: 136 Loss: -inf\n",
      "Iteration: 137 Loss: -inf\n",
      "Iteration: 138 Loss: -inf\n",
      "Iteration: 139 Loss: -inf\n",
      "Iteration: 140 Loss: -inf\n",
      "Iteration: 141 Loss: -inf\n",
      "Iteration: 142 Loss: -inf\n",
      "Iteration: 143 Loss: -inf\n",
      "Iteration: 144 Loss: -inf\n",
      "Iteration: 145 Loss: -inf\n",
      "Iteration: 146 Loss: -inf\n",
      "Iteration: 147 Loss: -inf\n",
      "Iteration: 148 Loss: -inf\n",
      "Iteration: 149 Loss: -inf\n",
      "Iteration: 150 Loss: -inf\n",
      "Iteration: 151 Loss: -inf\n",
      "Iteration: 152 Loss: -inf\n",
      "Iteration: 153 Loss: -inf\n",
      "Iteration: 154 Loss: -inf\n",
      "Iteration: 155 Loss: -inf\n",
      "Iteration: 156 Loss: -inf\n",
      "Iteration: 157 Loss: -inf\n",
      "Iteration: 158 Loss: -inf\n",
      "Iteration: 159 Loss: -inf\n",
      "Iteration: 160 Loss: -inf\n",
      "Iteration: 161 Loss: -inf\n",
      "Iteration: 162 Loss: -inf\n",
      "Iteration: 163 Loss: -inf\n",
      "Iteration: 164 Loss: -inf\n",
      "Iteration: 165 Loss: -inf\n",
      "Iteration: 166 Loss: -inf\n",
      "Iteration: 167 Loss: -inf\n",
      "Iteration: 168 Loss: -inf\n",
      "Iteration: 169 Loss: -inf\n",
      "Iteration: 170 Loss: -inf\n",
      "Iteration: 171 Loss: -inf\n",
      "Iteration: 172 Loss: -inf\n",
      "Iteration: 173 Loss: -inf\n",
      "Iteration: 174 Loss: -inf\n",
      "Iteration: 175 Loss: -inf\n",
      "Iteration: 176 Loss: -inf\n",
      "Iteration: 177 Loss: -inf\n",
      "Iteration: 178 Loss: -inf\n",
      "Iteration: 179 Loss: -inf\n",
      "Iteration: 180 Loss: -inf\n",
      "Iteration: 181 Loss: -inf\n",
      "Iteration: 182 Loss: -inf\n",
      "Iteration: 183 Loss: -inf\n",
      "Iteration: 184 Loss: -inf\n",
      "Iteration: 185 Loss: -inf\n",
      "Iteration: 186 Loss: -inf\n",
      "Iteration: 187 Loss: -inf\n",
      "Iteration: 188 Loss: -inf\n",
      "Iteration: 189 Loss: -inf\n",
      "Iteration: 190 Loss: -inf\n",
      "Iteration: 191 Loss: -inf\n",
      "Iteration: 192 Loss: -inf\n",
      "Iteration: 193 Loss: -inf\n",
      "Iteration: 194 Loss: -inf\n",
      "Iteration: 195 Loss: -inf\n",
      "Iteration: 196 Loss: -inf\n",
      "Iteration: 197 Loss: -inf\n",
      "Iteration: 198 Loss: -inf\n",
      "Iteration: 199 Loss: -inf\n",
      "Iteration: 200 Loss: nan\n",
      "Iteration: 201 Loss: nan\n",
      "Iteration: 202 Loss: nan\n",
      "Iteration: 203 Loss: nan\n",
      "Iteration: 204 Loss: nan\n",
      "Iteration: 205 Loss: nan\n",
      "Iteration: 206 Loss: nan\n",
      "Iteration: 207 Loss: nan\n",
      "Iteration: 208 Loss: nan\n",
      "Iteration: 209 Loss: nan\n",
      "Iteration: 210 Loss: nan\n",
      "Iteration: 211 Loss: nan\n",
      "Iteration: 212 Loss: nan\n",
      "Iteration: 213 Loss: nan\n",
      "Iteration: 214 Loss: nan\n",
      "Iteration: 215 Loss: nan\n",
      "Iteration: 216 Loss: nan\n",
      "Iteration: 217 Loss: nan\n",
      "Iteration: 218 Loss: nan\n",
      "Iteration: 219 Loss: nan\n",
      "Iteration: 220 Loss: nan\n",
      "Iteration: 221 Loss: nan\n",
      "Iteration: 222 Loss: nan\n",
      "Iteration: 223 Loss: nan\n",
      "Iteration: 224 Loss: nan\n",
      "Iteration: 225 Loss: nan\n",
      "Iteration: 226 Loss: nan\n",
      "Iteration: 227 Loss: nan\n",
      "Iteration: 228 Loss: nan\n",
      "Iteration: 229 Loss: nan\n",
      "Iteration: 230 Loss: nan\n",
      "Iteration: 231 Loss: nan\n",
      "Iteration: 232 Loss: nan\n",
      "Iteration: 233 Loss: nan\n",
      "Iteration: 234 Loss: nan\n",
      "Iteration: 235 Loss: nan\n",
      "Iteration: 236 Loss: nan\n",
      "Iteration: 237 Loss: nan\n",
      "Iteration: 238 Loss: nan\n",
      "Iteration: 239 Loss: nan\n",
      "Iteration: 240 Loss: nan\n",
      "Iteration: 241 Loss: nan\n",
      "Iteration: 242 Loss: nan\n",
      "Iteration: 243 Loss: nan\n",
      "Iteration: 244 Loss: nan\n",
      "Iteration: 245 Loss: nan\n",
      "Iteration: 246 Loss: nan\n",
      "Iteration: 247 Loss: nan\n",
      "Iteration: 248 Loss: nan\n",
      "Iteration: 249 Loss: nan\n",
      "Iteration: 250 Loss: nan\n",
      "Iteration: 251 Loss: nan\n",
      "Iteration: 252 Loss: nan\n",
      "Iteration: 253 Loss: nan\n",
      "Iteration: 254 Loss: nan\n",
      "Iteration: 255 Loss: nan\n",
      "Iteration: 256 Loss: nan\n",
      "Iteration: 257 Loss: nan\n",
      "Iteration: 258 Loss: nan\n",
      "Iteration: 259 Loss: nan\n",
      "Iteration: 260 Loss: nan\n",
      "Iteration: 261 Loss: nan\n",
      "Iteration: 262 Loss: nan\n",
      "Iteration: 263 Loss: nan\n",
      "Iteration: 264 Loss: nan\n",
      "Iteration: 265 Loss: nan\n",
      "Iteration: 266 Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 267 Loss: nan\n",
      "Iteration: 268 Loss: nan\n",
      "Iteration: 269 Loss: nan\n",
      "Iteration: 270 Loss: nan\n",
      "Iteration: 271 Loss: nan\n",
      "Iteration: 272 Loss: nan\n",
      "Iteration: 273 Loss: nan\n",
      "Iteration: 274 Loss: nan\n",
      "Iteration: 275 Loss: nan\n",
      "Iteration: 276 Loss: nan\n",
      "Iteration: 277 Loss: nan\n",
      "Iteration: 278 Loss: nan\n",
      "Iteration: 279 Loss: nan\n",
      "Iteration: 280 Loss: nan\n",
      "Iteration: 281 Loss: nan\n",
      "Iteration: 282 Loss: nan\n",
      "Iteration: 283 Loss: nan\n",
      "Iteration: 284 Loss: nan\n",
      "Iteration: 285 Loss: nan\n",
      "Iteration: 286 Loss: nan\n",
      "Iteration: 287 Loss: nan\n",
      "Iteration: 288 Loss: nan\n",
      "Iteration: 289 Loss: nan\n",
      "Iteration: 290 Loss: nan\n",
      "Iteration: 291 Loss: nan\n",
      "Iteration: 292 Loss: nan\n",
      "Iteration: 293 Loss: nan\n",
      "Iteration: 294 Loss: nan\n",
      "Iteration: 295 Loss: nan\n",
      "Iteration: 296 Loss: nan\n",
      "Iteration: 297 Loss: nan\n",
      "Iteration: 298 Loss: nan\n",
      "Iteration: 299 Loss: nan\n",
      "Iteration: 300 Loss: nan\n",
      "Iteration: 301 Loss: nan\n",
      "Iteration: 302 Loss: nan\n",
      "Iteration: 303 Loss: nan\n",
      "Iteration: 304 Loss: nan\n",
      "Iteration: 305 Loss: nan\n",
      "Iteration: 306 Loss: nan\n",
      "Iteration: 307 Loss: nan\n",
      "Iteration: 308 Loss: nan\n",
      "Iteration: 309 Loss: nan\n",
      "Iteration: 310 Loss: nan\n",
      "Iteration: 311 Loss: nan\n",
      "Iteration: 312 Loss: nan\n",
      "Iteration: 313 Loss: nan\n",
      "Iteration: 314 Loss: nan\n",
      "Iteration: 315 Loss: nan\n",
      "Iteration: 316 Loss: nan\n",
      "Iteration: 317 Loss: nan\n",
      "Iteration: 318 Loss: nan\n",
      "Iteration: 319 Loss: nan\n",
      "Iteration: 320 Loss: nan\n",
      "Iteration: 321 Loss: nan\n",
      "Iteration: 322 Loss: nan\n",
      "Iteration: 323 Loss: nan\n",
      "Iteration: 324 Loss: nan\n",
      "Iteration: 325 Loss: nan\n",
      "Iteration: 326 Loss: nan\n",
      "Iteration: 327 Loss: nan\n",
      "Iteration: 328 Loss: nan\n",
      "Iteration: 329 Loss: nan\n",
      "Iteration: 330 Loss: nan\n",
      "Iteration: 331 Loss: nan\n",
      "Iteration: 332 Loss: nan\n",
      "Iteration: 333 Loss: nan\n",
      "Iteration: 334 Loss: nan\n",
      "Iteration: 335 Loss: nan\n",
      "Iteration: 336 Loss: nan\n",
      "Iteration: 337 Loss: nan\n",
      "Iteration: 338 Loss: nan\n",
      "Iteration: 339 Loss: nan\n",
      "Iteration: 340 Loss: nan\n",
      "Iteration: 341 Loss: nan\n",
      "Iteration: 342 Loss: nan\n",
      "Iteration: 343 Loss: nan\n",
      "Iteration: 344 Loss: nan\n",
      "Iteration: 345 Loss: nan\n",
      "Iteration: 346 Loss: nan\n",
      "Iteration: 347 Loss: nan\n",
      "Iteration: 348 Loss: nan\n",
      "Iteration: 349 Loss: nan\n",
      "Iteration: 350 Loss: nan\n",
      "Iteration: 351 Loss: nan\n",
      "Iteration: 352 Loss: nan\n",
      "Iteration: 353 Loss: nan\n",
      "Iteration: 354 Loss: nan\n",
      "Iteration: 355 Loss: nan\n",
      "Iteration: 356 Loss: nan\n",
      "Iteration: 357 Loss: nan\n",
      "Iteration: 358 Loss: nan\n",
      "Iteration: 359 Loss: nan\n",
      "Iteration: 360 Loss: nan\n",
      "Iteration: 361 Loss: nan\n",
      "Iteration: 362 Loss: nan\n",
      "Iteration: 363 Loss: nan\n",
      "Iteration: 364 Loss: nan\n",
      "Iteration: 365 Loss: nan\n",
      "Iteration: 366 Loss: nan\n",
      "Iteration: 367 Loss: nan\n",
      "Iteration: 368 Loss: nan\n",
      "Iteration: 369 Loss: nan\n",
      "Iteration: 370 Loss: nan\n",
      "Iteration: 371 Loss: nan\n",
      "Iteration: 372 Loss: nan\n",
      "Iteration: 373 Loss: nan\n",
      "Iteration: 374 Loss: nan\n",
      "Iteration: 375 Loss: nan\n",
      "Iteration: 376 Loss: nan\n",
      "Iteration: 377 Loss: nan\n",
      "Iteration: 378 Loss: nan\n",
      "Iteration: 379 Loss: nan\n",
      "Iteration: 380 Loss: nan\n",
      "Iteration: 381 Loss: nan\n",
      "Iteration: 382 Loss: nan\n",
      "Iteration: 383 Loss: nan\n",
      "Iteration: 384 Loss: nan\n",
      "Iteration: 385 Loss: nan\n",
      "Iteration: 386 Loss: nan\n",
      "Iteration: 387 Loss: nan\n",
      "Iteration: 388 Loss: nan\n",
      "Iteration: 389 Loss: nan\n",
      "Iteration: 390 Loss: nan\n",
      "Iteration: 391 Loss: nan\n",
      "Iteration: 392 Loss: nan\n",
      "Iteration: 393 Loss: nan\n",
      "Iteration: 394 Loss: nan\n",
      "Iteration: 395 Loss: nan\n",
      "Iteration: 396 Loss: nan\n",
      "Iteration: 397 Loss: nan\n",
      "Iteration: 398 Loss: nan\n",
      "Iteration: 399 Loss: nan\n",
      "Iteration: 400 Loss: nan\n",
      "Iteration: 401 Loss: nan\n",
      "Iteration: 402 Loss: nan\n",
      "Iteration: 403 Loss: nan\n",
      "Iteration: 404 Loss: nan\n",
      "Iteration: 405 Loss: nan\n",
      "Iteration: 406 Loss: nan\n",
      "Iteration: 407 Loss: nan\n",
      "Iteration: 408 Loss: nan\n",
      "Iteration: 409 Loss: nan\n",
      "Iteration: 410 Loss: nan\n",
      "Iteration: 411 Loss: nan\n",
      "Iteration: 412 Loss: nan\n",
      "Iteration: 413 Loss: nan\n",
      "Iteration: 414 Loss: nan\n",
      "Iteration: 415 Loss: nan\n",
      "Iteration: 416 Loss: nan\n",
      "Iteration: 417 Loss: nan\n",
      "Iteration: 418 Loss: nan\n",
      "Iteration: 419 Loss: nan\n",
      "Iteration: 420 Loss: nan\n",
      "Iteration: 421 Loss: nan\n",
      "Iteration: 422 Loss: nan\n",
      "Iteration: 423 Loss: nan\n",
      "Iteration: 424 Loss: nan\n",
      "Iteration: 425 Loss: nan\n",
      "Iteration: 426 Loss: nan\n",
      "Iteration: 427 Loss: nan\n",
      "Iteration: 428 Loss: nan\n",
      "Iteration: 429 Loss: nan\n",
      "Iteration: 430 Loss: nan\n",
      "Iteration: 431 Loss: nan\n",
      "Iteration: 432 Loss: nan\n",
      "Iteration: 433 Loss: nan\n",
      "Iteration: 434 Loss: nan\n",
      "Iteration: 435 Loss: nan\n",
      "Iteration: 436 Loss: nan\n",
      "Iteration: 437 Loss: nan\n",
      "Iteration: 438 Loss: nan\n",
      "Iteration: 439 Loss: nan\n",
      "Iteration: 440 Loss: nan\n",
      "Iteration: 441 Loss: nan\n",
      "Iteration: 442 Loss: nan\n",
      "Iteration: 443 Loss: nan\n",
      "Iteration: 444 Loss: nan\n",
      "Iteration: 445 Loss: nan\n",
      "Iteration: 446 Loss: nan\n",
      "Iteration: 447 Loss: nan\n",
      "Iteration: 448 Loss: nan\n",
      "Iteration: 449 Loss: nan\n",
      "Iteration: 450 Loss: nan\n",
      "Iteration: 451 Loss: nan\n",
      "Iteration: 452 Loss: nan\n",
      "Iteration: 453 Loss: nan\n",
      "Iteration: 454 Loss: nan\n",
      "Iteration: 455 Loss: nan\n",
      "Iteration: 456 Loss: nan\n",
      "Iteration: 457 Loss: nan\n",
      "Iteration: 458 Loss: nan\n",
      "Iteration: 459 Loss: nan\n",
      "Iteration: 460 Loss: nan\n",
      "Iteration: 461 Loss: nan\n",
      "Iteration: 462 Loss: nan\n",
      "Iteration: 463 Loss: nan\n",
      "Iteration: 464 Loss: nan\n",
      "Iteration: 465 Loss: nan\n",
      "Iteration: 466 Loss: nan\n",
      "Iteration: 467 Loss: nan\n",
      "Iteration: 468 Loss: nan\n",
      "Iteration: 469 Loss: nan\n",
      "Iteration: 470 Loss: nan\n",
      "Iteration: 471 Loss: nan\n",
      "Iteration: 472 Loss: nan\n",
      "Iteration: 473 Loss: nan\n",
      "Iteration: 474 Loss: nan\n",
      "Iteration: 475 Loss: nan\n",
      "Iteration: 476 Loss: nan\n",
      "Iteration: 477 Loss: nan\n",
      "Iteration: 478 Loss: nan\n",
      "Iteration: 479 Loss: nan\n",
      "Iteration: 480 Loss: nan\n",
      "Iteration: 481 Loss: nan\n",
      "Iteration: 482 Loss: nan\n",
      "Iteration: 483 Loss: nan\n",
      "Iteration: 484 Loss: nan\n",
      "Iteration: 485 Loss: nan\n",
      "Iteration: 486 Loss: nan\n",
      "Iteration: 487 Loss: nan\n",
      "Iteration: 488 Loss: nan\n",
      "Iteration: 489 Loss: nan\n",
      "Iteration: 490 Loss: nan\n",
      "Iteration: 491 Loss: nan\n",
      "Iteration: 492 Loss: nan\n",
      "Iteration: 493 Loss: nan\n",
      "Iteration: 494 Loss: nan\n",
      "Iteration: 495 Loss: nan\n",
      "Iteration: 496 Loss: nan\n",
      "Iteration: 497 Loss: nan\n",
      "Iteration: 498 Loss: nan\n",
      "Iteration: 499 Loss: nan\n",
      "Iteration: 500 Loss: nan\n",
      "Iteration: 501 Loss: nan\n",
      "Iteration: 502 Loss: nan\n",
      "Iteration: 503 Loss: nan\n",
      "Iteration: 504 Loss: nan\n",
      "Iteration: 505 Loss: nan\n",
      "Iteration: 506 Loss: nan\n",
      "Iteration: 507 Loss: nan\n",
      "Iteration: 508 Loss: nan\n",
      "Iteration: 509 Loss: nan\n",
      "Iteration: 510 Loss: nan\n",
      "Iteration: 511 Loss: nan\n",
      "Iteration: 512 Loss: nan\n",
      "Iteration: 513 Loss: nan\n",
      "Iteration: 514 Loss: nan\n",
      "Iteration: 515 Loss: nan\n",
      "Iteration: 516 Loss: nan\n",
      "Iteration: 517 Loss: nan\n",
      "Iteration: 518 Loss: nan\n",
      "Iteration: 519 Loss: nan\n",
      "Iteration: 520 Loss: nan\n",
      "Iteration: 521 Loss: nan\n",
      "Iteration: 522 Loss: nan\n",
      "Iteration: 523 Loss: nan\n",
      "Iteration: 524 Loss: nan\n",
      "Iteration: 525 Loss: nan\n",
      "Iteration: 526 Loss: nan\n",
      "Iteration: 527 Loss: nan\n",
      "Iteration: 528 Loss: nan\n",
      "Iteration: 529 Loss: nan\n",
      "Iteration: 530 Loss: nan\n",
      "Iteration: 531 Loss: nan\n",
      "Iteration: 532 Loss: nan\n",
      "Iteration: 533 Loss: nan\n",
      "Iteration: 534 Loss: nan\n",
      "Iteration: 535 Loss: nan\n",
      "Iteration: 536 Loss: nan\n",
      "Iteration: 537 Loss: nan\n",
      "Iteration: 538 Loss: nan\n",
      "Iteration: 539 Loss: nan\n",
      "Iteration: 540 Loss: nan\n",
      "Iteration: 541 Loss: nan\n",
      "Iteration: 542 Loss: nan\n",
      "Iteration: 543 Loss: nan\n",
      "Iteration: 544 Loss: nan\n",
      "Iteration: 545 Loss: nan\n",
      "Iteration: 546 Loss: nan\n",
      "Iteration: 547 Loss: nan\n",
      "Iteration: 548 Loss: nan\n",
      "Iteration: 549 Loss: nan\n",
      "Iteration: 550 Loss: nan\n",
      "Iteration: 551 Loss: nan\n",
      "Iteration: 552 Loss: nan\n",
      "Iteration: 553 Loss: nan\n",
      "Iteration: 554 Loss: nan\n",
      "Iteration: 555 Loss: nan\n",
      "Iteration: 556 Loss: nan\n",
      "Iteration: 557 Loss: nan\n",
      "Iteration: 558 Loss: nan\n",
      "Iteration: 559 Loss: nan\n",
      "Iteration: 560 Loss: nan\n",
      "Iteration: 561 Loss: nan\n",
      "Iteration: 562 Loss: nan\n",
      "Iteration: 563 Loss: nan\n",
      "Iteration: 564 Loss: nan\n",
      "Iteration: 565 Loss: nan\n",
      "Iteration: 566 Loss: nan\n",
      "Iteration: 567 Loss: nan\n",
      "Iteration: 568 Loss: nan\n",
      "Iteration: 569 Loss: nan\n",
      "Iteration: 570 Loss: nan\n",
      "Iteration: 571 Loss: nan\n",
      "Iteration: 572 Loss: nan\n",
      "Iteration: 573 Loss: nan\n",
      "Iteration: 574 Loss: nan\n",
      "Iteration: 575 Loss: nan\n",
      "Iteration: 576 Loss: nan\n",
      "Iteration: 577 Loss: nan\n",
      "Iteration: 578 Loss: nan\n",
      "Iteration: 579 Loss: nan\n",
      "Iteration: 580 Loss: nan\n",
      "Iteration: 581 Loss: nan\n",
      "Iteration: 582 Loss: nan\n",
      "Iteration: 583 Loss: nan\n",
      "Iteration: 584 Loss: nan\n",
      "Iteration: 585 Loss: nan\n",
      "Iteration: 586 Loss: nan\n",
      "Iteration: 587 Loss: nan\n",
      "Iteration: 588 Loss: nan\n",
      "Iteration: 589 Loss: nan\n",
      "Iteration: 590 Loss: nan\n",
      "Iteration: 591 Loss: nan\n",
      "Iteration: 592 Loss: nan\n",
      "Iteration: 593 Loss: nan\n",
      "Iteration: 594 Loss: nan\n",
      "Iteration: 595 Loss: nan\n",
      "Iteration: 596 Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 597 Loss: nan\n",
      "Iteration: 598 Loss: nan\n",
      "Iteration: 599 Loss: nan\n",
      "Iteration: 600 Loss: nan\n",
      "Iteration: 601 Loss: nan\n",
      "Iteration: 602 Loss: nan\n",
      "Iteration: 603 Loss: nan\n",
      "Iteration: 604 Loss: nan\n",
      "Iteration: 605 Loss: nan\n",
      "Iteration: 606 Loss: nan\n",
      "Iteration: 607 Loss: nan\n",
      "Iteration: 608 Loss: nan\n",
      "Iteration: 609 Loss: nan\n",
      "Iteration: 610 Loss: nan\n",
      "Iteration: 611 Loss: nan\n",
      "Iteration: 612 Loss: nan\n",
      "Iteration: 613 Loss: nan\n",
      "Iteration: 614 Loss: nan\n",
      "Iteration: 615 Loss: nan\n",
      "Iteration: 616 Loss: nan\n",
      "Iteration: 617 Loss: nan\n",
      "Iteration: 618 Loss: nan\n",
      "Iteration: 619 Loss: nan\n",
      "Iteration: 620 Loss: nan\n",
      "Iteration: 621 Loss: nan\n",
      "Iteration: 622 Loss: nan\n",
      "Iteration: 623 Loss: nan\n",
      "Iteration: 624 Loss: nan\n",
      "Iteration: 625 Loss: nan\n",
      "Iteration: 626 Loss: nan\n",
      "Iteration: 627 Loss: nan\n",
      "Iteration: 628 Loss: nan\n",
      "Iteration: 629 Loss: nan\n",
      "Iteration: 630 Loss: nan\n",
      "Iteration: 631 Loss: nan\n",
      "Iteration: 632 Loss: nan\n",
      "Iteration: 633 Loss: nan\n",
      "Iteration: 634 Loss: nan\n",
      "Iteration: 635 Loss: nan\n",
      "Iteration: 636 Loss: nan\n",
      "Iteration: 637 Loss: nan\n",
      "Iteration: 638 Loss: nan\n",
      "Iteration: 639 Loss: nan\n",
      "Iteration: 640 Loss: nan\n",
      "Iteration: 641 Loss: nan\n",
      "Iteration: 642 Loss: nan\n",
      "Iteration: 643 Loss: nan\n",
      "Iteration: 644 Loss: nan\n",
      "Iteration: 645 Loss: nan\n",
      "Iteration: 646 Loss: nan\n",
      "Iteration: 647 Loss: nan\n",
      "Iteration: 648 Loss: nan\n",
      "Iteration: 649 Loss: nan\n",
      "Iteration: 650 Loss: nan\n",
      "Iteration: 651 Loss: nan\n",
      "Iteration: 652 Loss: nan\n",
      "Iteration: 653 Loss: nan\n",
      "Iteration: 654 Loss: nan\n",
      "Iteration: 655 Loss: nan\n",
      "Iteration: 656 Loss: nan\n",
      "Iteration: 657 Loss: nan\n",
      "Iteration: 658 Loss: nan\n",
      "Iteration: 659 Loss: nan\n",
      "Iteration: 660 Loss: nan\n",
      "Iteration: 661 Loss: nan\n",
      "Iteration: 662 Loss: nan\n",
      "Iteration: 663 Loss: nan\n",
      "Iteration: 664 Loss: nan\n",
      "Iteration: 665 Loss: nan\n",
      "Iteration: 666 Loss: nan\n",
      "Iteration: 667 Loss: nan\n",
      "Iteration: 668 Loss: nan\n",
      "Iteration: 669 Loss: nan\n",
      "Iteration: 670 Loss: nan\n",
      "Iteration: 671 Loss: nan\n",
      "Iteration: 672 Loss: nan\n",
      "Iteration: 673 Loss: nan\n",
      "Iteration: 674 Loss: nan\n",
      "Iteration: 675 Loss: nan\n",
      "Iteration: 676 Loss: nan\n",
      "Iteration: 677 Loss: nan\n",
      "Iteration: 678 Loss: nan\n",
      "Iteration: 679 Loss: nan\n",
      "Iteration: 680 Loss: nan\n",
      "Iteration: 681 Loss: nan\n",
      "Iteration: 682 Loss: nan\n",
      "Iteration: 683 Loss: nan\n",
      "Iteration: 684 Loss: nan\n",
      "Iteration: 685 Loss: nan\n",
      "Iteration: 686 Loss: nan\n",
      "Iteration: 687 Loss: nan\n",
      "Iteration: 688 Loss: nan\n",
      "Iteration: 689 Loss: nan\n",
      "Iteration: 690 Loss: nan\n",
      "Iteration: 691 Loss: nan\n",
      "Iteration: 692 Loss: nan\n",
      "Iteration: 693 Loss: nan\n",
      "Iteration: 694 Loss: nan\n",
      "Iteration: 695 Loss: nan\n",
      "Iteration: 696 Loss: nan\n",
      "Iteration: 697 Loss: nan\n",
      "Iteration: 698 Loss: nan\n",
      "Iteration: 699 Loss: nan\n",
      "Iteration: 700 Loss: nan\n",
      "Iteration: 701 Loss: nan\n",
      "Iteration: 702 Loss: nan\n",
      "Iteration: 703 Loss: nan\n",
      "Iteration: 704 Loss: nan\n",
      "Iteration: 705 Loss: nan\n",
      "Iteration: 706 Loss: nan\n",
      "Iteration: 707 Loss: nan\n",
      "Iteration: 708 Loss: nan\n",
      "Iteration: 709 Loss: nan\n",
      "Iteration: 710 Loss: nan\n",
      "Iteration: 711 Loss: nan\n",
      "Iteration: 712 Loss: nan\n",
      "Iteration: 713 Loss: nan\n",
      "Iteration: 714 Loss: nan\n",
      "Iteration: 715 Loss: nan\n",
      "Iteration: 716 Loss: nan\n",
      "Iteration: 717 Loss: nan\n",
      "Iteration: 718 Loss: nan\n",
      "Iteration: 719 Loss: nan\n",
      "Iteration: 720 Loss: nan\n",
      "Iteration: 721 Loss: nan\n",
      "Iteration: 722 Loss: nan\n",
      "Iteration: 723 Loss: nan\n",
      "Iteration: 724 Loss: nan\n",
      "Iteration: 725 Loss: nan\n",
      "Iteration: 726 Loss: nan\n",
      "Iteration: 727 Loss: nan\n",
      "Iteration: 728 Loss: nan\n",
      "Iteration: 729 Loss: nan\n",
      "Iteration: 730 Loss: nan\n",
      "Iteration: 731 Loss: nan\n",
      "Iteration: 732 Loss: nan\n",
      "Iteration: 733 Loss: nan\n",
      "Iteration: 734 Loss: nan\n",
      "Iteration: 735 Loss: nan\n",
      "Iteration: 736 Loss: nan\n",
      "Iteration: 737 Loss: nan\n",
      "Iteration: 738 Loss: nan\n",
      "Iteration: 739 Loss: nan\n",
      "Iteration: 740 Loss: nan\n",
      "Iteration: 741 Loss: nan\n",
      "Iteration: 742 Loss: nan\n",
      "Iteration: 743 Loss: nan\n",
      "Iteration: 744 Loss: nan\n",
      "Iteration: 745 Loss: nan\n",
      "Iteration: 746 Loss: nan\n",
      "Iteration: 747 Loss: nan\n",
      "Iteration: 748 Loss: nan\n",
      "Iteration: 749 Loss: nan\n",
      "Iteration: 750 Loss: nan\n",
      "Iteration: 751 Loss: nan\n",
      "Iteration: 752 Loss: nan\n",
      "Iteration: 753 Loss: nan\n",
      "Iteration: 754 Loss: nan\n",
      "Iteration: 755 Loss: nan\n",
      "Iteration: 756 Loss: nan\n",
      "Iteration: 757 Loss: nan\n",
      "Iteration: 758 Loss: nan\n",
      "Iteration: 759 Loss: nan\n",
      "Iteration: 760 Loss: nan\n",
      "Iteration: 761 Loss: nan\n",
      "Iteration: 762 Loss: nan\n",
      "Iteration: 763 Loss: nan\n",
      "Iteration: 764 Loss: nan\n",
      "Iteration: 765 Loss: nan\n",
      "Iteration: 766 Loss: nan\n",
      "Iteration: 767 Loss: nan\n",
      "Iteration: 768 Loss: nan\n",
      "Iteration: 769 Loss: nan\n",
      "Iteration: 770 Loss: nan\n",
      "Iteration: 771 Loss: nan\n",
      "Iteration: 772 Loss: nan\n",
      "Iteration: 773 Loss: nan\n",
      "Iteration: 774 Loss: nan\n",
      "Iteration: 775 Loss: nan\n",
      "Iteration: 776 Loss: nan\n",
      "Iteration: 777 Loss: nan\n",
      "Iteration: 778 Loss: nan\n",
      "Iteration: 779 Loss: nan\n",
      "Iteration: 780 Loss: nan\n",
      "Iteration: 781 Loss: nan\n",
      "Iteration: 782 Loss: nan\n",
      "Iteration: 783 Loss: nan\n",
      "Iteration: 784 Loss: nan\n",
      "Iteration: 785 Loss: nan\n",
      "Iteration: 786 Loss: nan\n",
      "Iteration: 787 Loss: nan\n",
      "Iteration: 788 Loss: nan\n",
      "Iteration: 789 Loss: nan\n",
      "Iteration: 790 Loss: nan\n",
      "Iteration: 791 Loss: nan\n",
      "Iteration: 792 Loss: nan\n",
      "Iteration: 793 Loss: nan\n",
      "Iteration: 794 Loss: nan\n",
      "Iteration: 795 Loss: nan\n",
      "Iteration: 796 Loss: nan\n",
      "Iteration: 797 Loss: nan\n",
      "Iteration: 798 Loss: nan\n",
      "Iteration: 799 Loss: nan\n",
      "Iteration: 800 Loss: nan\n",
      "Iteration: 801 Loss: nan\n",
      "Iteration: 802 Loss: nan\n",
      "Iteration: 803 Loss: nan\n",
      "Iteration: 804 Loss: nan\n",
      "Iteration: 805 Loss: nan\n",
      "Iteration: 806 Loss: nan\n",
      "Iteration: 807 Loss: nan\n",
      "Iteration: 808 Loss: nan\n",
      "Iteration: 809 Loss: nan\n",
      "Iteration: 810 Loss: nan\n",
      "Iteration: 811 Loss: nan\n",
      "Iteration: 812 Loss: nan\n",
      "Iteration: 813 Loss: nan\n",
      "Iteration: 814 Loss: nan\n",
      "Iteration: 815 Loss: nan\n",
      "Iteration: 816 Loss: nan\n",
      "Iteration: 817 Loss: nan\n",
      "Iteration: 818 Loss: nan\n",
      "Iteration: 819 Loss: nan\n",
      "Iteration: 820 Loss: nan\n",
      "Iteration: 821 Loss: nan\n",
      "Iteration: 822 Loss: nan\n",
      "Iteration: 823 Loss: nan\n",
      "Iteration: 824 Loss: nan\n",
      "Iteration: 825 Loss: nan\n",
      "Iteration: 826 Loss: nan\n",
      "Iteration: 827 Loss: nan\n",
      "Iteration: 828 Loss: nan\n",
      "Iteration: 829 Loss: nan\n",
      "Iteration: 830 Loss: nan\n",
      "Iteration: 831 Loss: nan\n",
      "Iteration: 832 Loss: nan\n",
      "Iteration: 833 Loss: nan\n",
      "Iteration: 834 Loss: nan\n",
      "Iteration: 835 Loss: nan\n",
      "Iteration: 836 Loss: nan\n",
      "Iteration: 837 Loss: nan\n",
      "Iteration: 838 Loss: nan\n",
      "Iteration: 839 Loss: nan\n",
      "Iteration: 840 Loss: nan\n",
      "Iteration: 841 Loss: nan\n",
      "Iteration: 842 Loss: nan\n",
      "Iteration: 843 Loss: nan\n",
      "Iteration: 844 Loss: nan\n",
      "Iteration: 845 Loss: nan\n",
      "Iteration: 846 Loss: nan\n",
      "Iteration: 847 Loss: nan\n",
      "Iteration: 848 Loss: nan\n",
      "Iteration: 849 Loss: nan\n",
      "Iteration: 850 Loss: nan\n",
      "Iteration: 851 Loss: nan\n",
      "Iteration: 852 Loss: nan\n",
      "Iteration: 853 Loss: nan\n",
      "Iteration: 854 Loss: nan\n",
      "Iteration: 855 Loss: nan\n",
      "Iteration: 856 Loss: nan\n",
      "Iteration: 857 Loss: nan\n",
      "Iteration: 858 Loss: nan\n",
      "Iteration: 859 Loss: nan\n",
      "Iteration: 860 Loss: nan\n",
      "Iteration: 861 Loss: nan\n",
      "Iteration: 862 Loss: nan\n",
      "Iteration: 863 Loss: nan\n",
      "Iteration: 864 Loss: nan\n",
      "Iteration: 865 Loss: nan\n",
      "Iteration: 866 Loss: nan\n",
      "Iteration: 867 Loss: nan\n",
      "Iteration: 868 Loss: nan\n",
      "Iteration: 869 Loss: nan\n",
      "Iteration: 870 Loss: nan\n",
      "Iteration: 871 Loss: nan\n",
      "Iteration: 872 Loss: nan\n",
      "Iteration: 873 Loss: nan\n",
      "Iteration: 874 Loss: nan\n",
      "Iteration: 875 Loss: nan\n",
      "Iteration: 876 Loss: nan\n",
      "Iteration: 877 Loss: nan\n",
      "Iteration: 878 Loss: nan\n",
      "Iteration: 879 Loss: nan\n",
      "Iteration: 880 Loss: nan\n",
      "Iteration: 881 Loss: nan\n",
      "Iteration: 882 Loss: nan\n",
      "Iteration: 883 Loss: nan\n",
      "Iteration: 884 Loss: nan\n",
      "Iteration: 885 Loss: nan\n",
      "Iteration: 886 Loss: nan\n",
      "Iteration: 887 Loss: nan\n",
      "Iteration: 888 Loss: nan\n",
      "Iteration: 889 Loss: nan\n",
      "Iteration: 890 Loss: nan\n",
      "Iteration: 891 Loss: nan\n",
      "Iteration: 892 Loss: nan\n",
      "Iteration: 893 Loss: nan\n",
      "Iteration: 894 Loss: nan\n",
      "Iteration: 895 Loss: nan\n",
      "Iteration: 896 Loss: nan\n",
      "Iteration: 897 Loss: nan\n",
      "Iteration: 898 Loss: nan\n",
      "Iteration: 899 Loss: nan\n",
      "Iteration: 900 Loss: nan\n",
      "Iteration: 901 Loss: nan\n",
      "Iteration: 902 Loss: nan\n",
      "Iteration: 903 Loss: nan\n",
      "Iteration: 904 Loss: nan\n",
      "Iteration: 905 Loss: nan\n",
      "Iteration: 906 Loss: nan\n",
      "Iteration: 907 Loss: nan\n",
      "Iteration: 908 Loss: nan\n",
      "Iteration: 909 Loss: nan\n",
      "Iteration: 910 Loss: nan\n",
      "Iteration: 911 Loss: nan\n",
      "Iteration: 912 Loss: nan\n",
      "Iteration: 913 Loss: nan\n",
      "Iteration: 914 Loss: nan\n",
      "Iteration: 915 Loss: nan\n",
      "Iteration: 916 Loss: nan\n",
      "Iteration: 917 Loss: nan\n",
      "Iteration: 918 Loss: nan\n",
      "Iteration: 919 Loss: nan\n",
      "Iteration: 920 Loss: nan\n",
      "Iteration: 921 Loss: nan\n",
      "Iteration: 922 Loss: nan\n",
      "Iteration: 923 Loss: nan\n",
      "Iteration: 924 Loss: nan\n",
      "Iteration: 925 Loss: nan\n",
      "Iteration: 926 Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 927 Loss: nan\n",
      "Iteration: 928 Loss: nan\n",
      "Iteration: 929 Loss: nan\n",
      "Iteration: 930 Loss: nan\n",
      "Iteration: 931 Loss: nan\n",
      "Iteration: 932 Loss: nan\n",
      "Iteration: 933 Loss: nan\n",
      "Iteration: 934 Loss: nan\n",
      "Iteration: 935 Loss: nan\n",
      "Iteration: 936 Loss: nan\n",
      "Iteration: 937 Loss: nan\n",
      "Iteration: 938 Loss: nan\n",
      "Iteration: 939 Loss: nan\n",
      "Iteration: 940 Loss: nan\n",
      "Iteration: 941 Loss: nan\n",
      "Iteration: 942 Loss: nan\n",
      "Iteration: 943 Loss: nan\n",
      "Iteration: 944 Loss: nan\n",
      "Iteration: 945 Loss: nan\n",
      "Iteration: 946 Loss: nan\n",
      "Iteration: 947 Loss: nan\n",
      "Iteration: 948 Loss: nan\n",
      "Iteration: 949 Loss: nan\n",
      "Iteration: 950 Loss: nan\n",
      "Iteration: 951 Loss: nan\n",
      "Iteration: 952 Loss: nan\n",
      "Iteration: 953 Loss: nan\n",
      "Iteration: 954 Loss: nan\n",
      "Iteration: 955 Loss: nan\n",
      "Iteration: 956 Loss: nan\n",
      "Iteration: 957 Loss: nan\n",
      "Iteration: 958 Loss: nan\n",
      "Iteration: 959 Loss: nan\n",
      "Iteration: 960 Loss: nan\n",
      "Iteration: 961 Loss: nan\n",
      "Iteration: 962 Loss: nan\n",
      "Iteration: 963 Loss: nan\n",
      "Iteration: 964 Loss: nan\n",
      "Iteration: 965 Loss: nan\n",
      "Iteration: 966 Loss: nan\n",
      "Iteration: 967 Loss: nan\n",
      "Iteration: 968 Loss: nan\n",
      "Iteration: 969 Loss: nan\n",
      "Iteration: 970 Loss: nan\n",
      "Iteration: 971 Loss: nan\n",
      "Iteration: 972 Loss: nan\n",
      "Iteration: 973 Loss: nan\n",
      "Iteration: 974 Loss: nan\n",
      "Iteration: 975 Loss: nan\n",
      "Iteration: 976 Loss: nan\n",
      "Iteration: 977 Loss: nan\n",
      "Iteration: 978 Loss: nan\n",
      "Iteration: 979 Loss: nan\n",
      "Iteration: 980 Loss: nan\n",
      "Iteration: 981 Loss: nan\n",
      "Iteration: 982 Loss: nan\n",
      "Iteration: 983 Loss: nan\n",
      "Iteration: 984 Loss: nan\n",
      "Iteration: 985 Loss: nan\n",
      "Iteration: 986 Loss: nan\n",
      "Iteration: 987 Loss: nan\n",
      "Iteration: 988 Loss: nan\n",
      "Iteration: 989 Loss: nan\n",
      "Iteration: 990 Loss: nan\n",
      "Iteration: 991 Loss: nan\n",
      "Iteration: 992 Loss: nan\n",
      "Iteration: 993 Loss: nan\n",
      "Iteration: 994 Loss: nan\n",
      "Iteration: 995 Loss: nan\n",
      "Iteration: 996 Loss: nan\n",
      "Iteration: 997 Loss: nan\n",
      "Iteration: 998 Loss: nan\n",
      "Iteration: 999 Loss: nan\n",
      "Iteration: 1000 Loss: nan\n",
      "Iteration: 1001 Loss: nan\n",
      "Iteration: 1002 Loss: nan\n",
      "Iteration: 1003 Loss: nan\n",
      "Iteration: 1004 Loss: nan\n",
      "Iteration: 1005 Loss: nan\n",
      "Iteration: 1006 Loss: nan\n",
      "Iteration: 1007 Loss: nan\n",
      "Iteration: 1008 Loss: nan\n",
      "Iteration: 1009 Loss: nan\n",
      "Iteration: 1010 Loss: nan\n",
      "Iteration: 1011 Loss: nan\n",
      "Iteration: 1012 Loss: nan\n",
      "Iteration: 1013 Loss: nan\n",
      "Iteration: 1014 Loss: nan\n",
      "Iteration: 1015 Loss: nan\n",
      "Iteration: 1016 Loss: nan\n",
      "Iteration: 1017 Loss: nan\n",
      "Iteration: 1018 Loss: nan\n",
      "Iteration: 1019 Loss: nan\n",
      "Iteration: 1020 Loss: nan\n",
      "Iteration: 1021 Loss: nan\n",
      "Iteration: 1022 Loss: nan\n",
      "Iteration: 1023 Loss: nan\n",
      "Iteration: 1024 Loss: nan\n",
      "Iteration: 1025 Loss: nan\n",
      "Iteration: 1026 Loss: nan\n",
      "Iteration: 1027 Loss: nan\n",
      "Iteration: 1028 Loss: nan\n",
      "Iteration: 1029 Loss: nan\n",
      "Iteration: 1030 Loss: nan\n",
      "Iteration: 1031 Loss: nan\n",
      "Iteration: 1032 Loss: nan\n",
      "Iteration: 1033 Loss: nan\n",
      "Iteration: 1034 Loss: nan\n",
      "Iteration: 1035 Loss: nan\n",
      "Iteration: 1036 Loss: nan\n",
      "Iteration: 1037 Loss: nan\n",
      "Iteration: 1038 Loss: nan\n",
      "Iteration: 1039 Loss: nan\n",
      "Iteration: 1040 Loss: nan\n",
      "Iteration: 1041 Loss: nan\n",
      "Iteration: 1042 Loss: nan\n",
      "Iteration: 1043 Loss: nan\n",
      "Iteration: 1044 Loss: nan\n",
      "Iteration: 1045 Loss: nan\n",
      "Iteration: 1046 Loss: nan\n",
      "Iteration: 1047 Loss: nan\n",
      "Iteration: 1048 Loss: nan\n",
      "Iteration: 1049 Loss: nan\n",
      "Iteration: 1050 Loss: nan\n",
      "Iteration: 1051 Loss: nan\n",
      "Iteration: 1052 Loss: nan\n",
      "Iteration: 1053 Loss: nan\n",
      "Iteration: 1054 Loss: nan\n",
      "Iteration: 1055 Loss: nan\n",
      "Iteration: 1056 Loss: nan\n",
      "Iteration: 1057 Loss: nan\n",
      "Iteration: 1058 Loss: nan\n",
      "Iteration: 1059 Loss: nan\n",
      "Iteration: 1060 Loss: nan\n",
      "Iteration: 1061 Loss: nan\n",
      "Iteration: 1062 Loss: nan\n",
      "Iteration: 1063 Loss: nan\n",
      "Iteration: 1064 Loss: nan\n",
      "Iteration: 1065 Loss: nan\n",
      "Iteration: 1066 Loss: nan\n",
      "Iteration: 1067 Loss: nan\n",
      "Iteration: 1068 Loss: nan\n",
      "Iteration: 1069 Loss: nan\n",
      "Iteration: 1070 Loss: nan\n",
      "Iteration: 1071 Loss: nan\n",
      "Iteration: 1072 Loss: nan\n",
      "Iteration: 1073 Loss: nan\n",
      "Iteration: 1074 Loss: nan\n",
      "Iteration: 1075 Loss: nan\n",
      "Iteration: 1076 Loss: nan\n",
      "Iteration: 1077 Loss: nan\n",
      "Iteration: 1078 Loss: nan\n",
      "Iteration: 1079 Loss: nan\n",
      "Iteration: 1080 Loss: nan\n",
      "Iteration: 1081 Loss: nan\n",
      "Iteration: 1082 Loss: nan\n",
      "Iteration: 1083 Loss: nan\n",
      "Iteration: 1084 Loss: nan\n",
      "Iteration: 1085 Loss: nan\n",
      "Iteration: 1086 Loss: nan\n",
      "Iteration: 1087 Loss: nan\n",
      "Iteration: 1088 Loss: nan\n",
      "Iteration: 1089 Loss: nan\n",
      "Iteration: 1090 Loss: nan\n",
      "Iteration: 1091 Loss: nan\n",
      "Iteration: 1092 Loss: nan\n",
      "Iteration: 1093 Loss: nan\n",
      "Iteration: 1094 Loss: nan\n",
      "Iteration: 1095 Loss: nan\n",
      "Iteration: 1096 Loss: nan\n",
      "Iteration: 1097 Loss: nan\n",
      "Iteration: 1098 Loss: nan\n",
      "Iteration: 1099 Loss: nan\n",
      "Iteration: 1100 Loss: nan\n",
      "Iteration: 1101 Loss: nan\n",
      "Iteration: 1102 Loss: nan\n",
      "Iteration: 1103 Loss: nan\n",
      "Iteration: 1104 Loss: nan\n",
      "Iteration: 1105 Loss: nan\n",
      "Iteration: 1106 Loss: nan\n",
      "Iteration: 1107 Loss: nan\n",
      "Iteration: 1108 Loss: nan\n",
      "Iteration: 1109 Loss: nan\n",
      "Iteration: 1110 Loss: nan\n",
      "Iteration: 1111 Loss: nan\n",
      "Iteration: 1112 Loss: nan\n",
      "Iteration: 1113 Loss: nan\n",
      "Iteration: 1114 Loss: nan\n",
      "Iteration: 1115 Loss: nan\n",
      "Iteration: 1116 Loss: nan\n",
      "Iteration: 1117 Loss: nan\n",
      "Iteration: 1118 Loss: nan\n",
      "Iteration: 1119 Loss: nan\n",
      "Iteration: 1120 Loss: nan\n",
      "Iteration: 1121 Loss: nan\n",
      "Iteration: 1122 Loss: nan\n",
      "Iteration: 1123 Loss: nan\n",
      "Iteration: 1124 Loss: nan\n",
      "Iteration: 1125 Loss: nan\n",
      "Iteration: 1126 Loss: nan\n",
      "Iteration: 1127 Loss: nan\n",
      "Iteration: 1128 Loss: nan\n",
      "Iteration: 1129 Loss: nan\n",
      "Iteration: 1130 Loss: nan\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-7feb9c21d07b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mloss_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_5000\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "train_loader,_= make_loaders(data,labels,1,batch_size=100)\n",
    "var_model = MLP_20(Vocab_size,20)\n",
    "optimizer = torch.optim.SGD(var_model.parameters(),lr=0.2)\n",
    "loss_func = nn.nn.CrossEntropyLoss()\n",
    "model, losses = train_5000(var_model,train_loader,optimizer,loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1 Loss: 3.5393199920654297\n",
      "Iteration: 2 Loss: 3.1645619869232178\n",
      "Iteration: 3 Loss: 3.7164053916931152\n",
      "Iteration: 4 Loss: 2.6979188919067383\n",
      "Iteration: 5 Loss: 3.9454195499420166\n",
      "Iteration: 6 Loss: 6.8036112785339355\n",
      "Iteration: 7 Loss: 6.061787128448486\n",
      "Iteration: 8 Loss: 5.183966636657715\n",
      "Iteration: 9 Loss: 7.287364482879639\n",
      "Iteration: 10 Loss: 5.195305824279785\n",
      "Iteration: 11 Loss: 12.733965873718262\n",
      "Iteration: 12 Loss: 15.7228422164917\n",
      "Iteration: 13 Loss: 24.353858947753906\n",
      "Iteration: 14 Loss: 89.57996368408203\n",
      "Iteration: 15 Loss: 74.3738021850586\n",
      "Iteration: 16 Loss: 128.15689086914062\n",
      "Iteration: 17 Loss: 208.1860809326172\n",
      "Iteration: 18 Loss: 181.68707275390625\n",
      "Iteration: 19 Loss: 182.86265563964844\n",
      "Iteration: 20 Loss: 380.988525390625\n",
      "Iteration: 21 Loss: 386.5858154296875\n",
      "Iteration: 22 Loss: 775.0795288085938\n",
      "Iteration: 23 Loss: 532.1685791015625\n",
      "Iteration: 24 Loss: 897.8995361328125\n",
      "Iteration: 25 Loss: 981.4090576171875\n",
      "Iteration: 26 Loss: 2118.31884765625\n",
      "Iteration: 27 Loss: 4083.72119140625\n",
      "Iteration: 28 Loss: 9774.828125\n",
      "Iteration: 29 Loss: 6578.8818359375\n",
      "Iteration: 30 Loss: 10179.654296875\n",
      "Iteration: 31 Loss: 13115.4033203125\n",
      "Iteration: 32 Loss: 22383.5703125\n",
      "Iteration: 33 Loss: 20204.078125\n",
      "Iteration: 34 Loss: 27766.15234375\n",
      "Iteration: 35 Loss: 38000.734375\n",
      "Iteration: 36 Loss: 28415.546875\n",
      "Iteration: 37 Loss: 63092.30859375\n",
      "Iteration: 38 Loss: 68543.890625\n",
      "Iteration: 39 Loss: 79000.65625\n",
      "Iteration: 40 Loss: 103579.046875\n",
      "Iteration: 41 Loss: 144973.390625\n",
      "Iteration: 42 Loss: 179851.21875\n",
      "Iteration: 43 Loss: 298814.40625\n",
      "Iteration: 44 Loss: 493978.3125\n",
      "Iteration: 45 Loss: 1004467.4375\n",
      "Iteration: 46 Loss: 761979.9375\n",
      "Iteration: 47 Loss: 902622.375\n",
      "Iteration: 48 Loss: 1459310.125\n",
      "Iteration: 49 Loss: 2194435.25\n",
      "Iteration: 50 Loss: 1598987.875\n",
      "Iteration: 51 Loss: 4322943.5\n",
      "Iteration: 52 Loss: 5528882.5\n",
      "Iteration: 53 Loss: 7235151.5\n",
      "Iteration: 54 Loss: 5160579.5\n",
      "Iteration: 55 Loss: 11441356.0\n",
      "Iteration: 56 Loss: 16534609.0\n",
      "Iteration: 57 Loss: 11985245.0\n",
      "Iteration: 58 Loss: 49724764.0\n",
      "Iteration: 59 Loss: 124538288.0\n",
      "Iteration: 60 Loss: 153999376.0\n",
      "Iteration: 61 Loss: 80162176.0\n",
      "Iteration: 62 Loss: 152084416.0\n",
      "Iteration: 63 Loss: 156668464.0\n",
      "Iteration: 64 Loss: 301972736.0\n",
      "Iteration: 65 Loss: 375037088.0\n",
      "Iteration: 66 Loss: 485337120.0\n",
      "Iteration: 67 Loss: 480572032.0\n",
      "Iteration: 68 Loss: 845704000.0\n",
      "Iteration: 69 Loss: 1796753152.0\n",
      "Iteration: 70 Loss: 1449326208.0\n",
      "Iteration: 71 Loss: 2306816000.0\n",
      "Iteration: 72 Loss: 2345272832.0\n",
      "Iteration: 73 Loss: 4177671936.0\n",
      "Iteration: 74 Loss: 6822762496.0\n",
      "Iteration: 75 Loss: 7507192832.0\n",
      "Iteration: 76 Loss: 14677045248.0\n",
      "Iteration: 77 Loss: 11100545024.0\n",
      "Iteration: 78 Loss: 17114521600.0\n",
      "Iteration: 79 Loss: 70831226880.0\n",
      "Iteration: 80 Loss: 172244549632.0\n",
      "Iteration: 81 Loss: 52909490176.0\n",
      "Iteration: 82 Loss: 180083441664.0\n",
      "Iteration: 83 Loss: 293609537536.0\n",
      "Iteration: 84 Loss: 268767346688.0\n",
      "Iteration: 85 Loss: 315851440128.0\n",
      "Iteration: 86 Loss: 503811604480.0\n",
      "Iteration: 87 Loss: 500887715840.0\n",
      "Iteration: 88 Loss: 1321708158976.0\n",
      "Iteration: 89 Loss: 2670908407808.0\n",
      "Iteration: 90 Loss: 4265686335488.0\n",
      "Iteration: 91 Loss: 11910458114048.0\n",
      "Iteration: 92 Loss: 11623864467456.0\n",
      "Iteration: 93 Loss: 11798634823680.0\n",
      "Iteration: 94 Loss: 17252405477376.0\n",
      "Iteration: 95 Loss: 27562847764480.0\n",
      "Iteration: 96 Loss: 44894932434944.0\n",
      "Iteration: 97 Loss: 50191675686912.0\n",
      "Iteration: 98 Loss: 175054964916224.0\n",
      "Iteration: 99 Loss: 171930845970432.0\n",
      "Iteration: 100 Loss: 208956802727936.0\n",
      "Iteration: 101 Loss: 272818033917952.0\n",
      "Iteration: 102 Loss: 629051110719488.0\n",
      "Iteration: 103 Loss: 719181536296960.0\n",
      "Iteration: 104 Loss: 674633464414208.0\n",
      "Iteration: 105 Loss: 842019077357568.0\n",
      "Iteration: 106 Loss: 679776889077760.0\n",
      "Iteration: 107 Loss: 1711610771013632.0\n",
      "Iteration: 108 Loss: 1408430371766272.0\n",
      "Iteration: 109 Loss: 7718371911008256.0\n",
      "Iteration: 110 Loss: 2.682011440591667e+16\n",
      "Iteration: 111 Loss: 1.3880903730200576e+16\n",
      "Iteration: 112 Loss: 2.195244543324979e+16\n",
      "Iteration: 113 Loss: 2.173502559879168e+16\n",
      "Iteration: 114 Loss: 3.850240612368384e+16\n",
      "Iteration: 115 Loss: 6.50302029079511e+16\n",
      "Iteration: 116 Loss: 9.492881887513805e+16\n",
      "Iteration: 117 Loss: 1.4704921767470694e+17\n",
      "Iteration: 118 Loss: 1.893466397212672e+17\n",
      "Iteration: 119 Loss: 5.260559781902418e+17\n",
      "Iteration: 120 Loss: 4.751730883176694e+17\n",
      "Iteration: 121 Loss: 4.088034397289185e+17\n",
      "Iteration: 122 Loss: 9.295293978645627e+17\n",
      "Iteration: 123 Loss: 1.5216036963187425e+18\n",
      "Iteration: 124 Loss: 1.6432283740484403e+18\n",
      "Iteration: 125 Loss: 2.2137491522217574e+18\n",
      "Iteration: 126 Loss: 3.167618507709874e+18\n",
      "Iteration: 127 Loss: 4.0084202721504133e+18\n",
      "Iteration: 128 Loss: 3.390794629136777e+18\n",
      "Iteration: 129 Loss: 8.32113258631109e+18\n",
      "Iteration: 130 Loss: 9.176218381185974e+18\n",
      "Iteration: 131 Loss: 1.7851766446082753e+19\n",
      "Iteration: 132 Loss: 2.181178841540172e+19\n",
      "Iteration: 133 Loss: 2.427806776329398e+19\n",
      "Iteration: 134 Loss: 4.119807177211563e+19\n",
      "Iteration: 135 Loss: 6.146468790970142e+19\n",
      "Iteration: 136 Loss: 8.280850438511238e+19\n",
      "Iteration: 137 Loss: 5.454497745099083e+19\n",
      "Iteration: 138 Loss: 1.489769294267186e+20\n",
      "Iteration: 139 Loss: 1.4348098976895468e+20\n",
      "Iteration: 140 Loss: 2.0252293459317726e+20\n",
      "Iteration: 141 Loss: 2.0550674526817065e+20\n",
      "Iteration: 142 Loss: 3.629069892948161e+20\n",
      "Iteration: 143 Loss: 7.451819198933715e+20\n",
      "Iteration: 144 Loss: 4.605535086498818e+20\n",
      "Iteration: 145 Loss: 9.699636141698507e+20\n",
      "Iteration: 146 Loss: 1.7178987815479175e+21\n",
      "Iteration: 147 Loss: 2.1279187357852143e+21\n",
      "Iteration: 148 Loss: 2.737099828231345e+21\n",
      "Iteration: 149 Loss: 4.2275765394837487e+21\n",
      "Iteration: 150 Loss: 6.300896729611467e+21\n",
      "Iteration: 151 Loss: 6.008722889036039e+21\n",
      "Iteration: 152 Loss: 7.137065998676511e+21\n",
      "Iteration: 153 Loss: 1.4937352344677925e+22\n",
      "Iteration: 154 Loss: 1.8889963579237405e+22\n",
      "Iteration: 155 Loss: 7.772760795438888e+22\n",
      "Iteration: 156 Loss: 3.6633387254539948e+22\n",
      "Iteration: 157 Loss: 1.2267836910154623e+23\n",
      "Iteration: 158 Loss: 1.5466200591360825e+23\n",
      "Iteration: 159 Loss: 3.246839887163263e+23\n",
      "Iteration: 160 Loss: 3.652094318048361e+23\n",
      "Iteration: 161 Loss: 9.75716460542458e+23\n",
      "Iteration: 162 Loss: 6.610142140114387e+23\n",
      "Iteration: 163 Loss: 1.0368373691300914e+24\n",
      "Iteration: 164 Loss: 2.2087841682331785e+24\n",
      "Iteration: 165 Loss: 3.6935875306535974e+24\n",
      "Iteration: 166 Loss: 3.6489882033997623e+24\n",
      "Iteration: 167 Loss: 6.376380531641509e+24\n",
      "Iteration: 168 Loss: 6.767667678786265e+24\n",
      "Iteration: 169 Loss: 1.1783833148674873e+25\n",
      "Iteration: 170 Loss: 1.4766340776921886e+25\n",
      "Iteration: 171 Loss: 2.925389821650081e+25\n",
      "Iteration: 172 Loss: 1.963060537981403e+25\n",
      "Iteration: 173 Loss: 4.318103780651291e+25\n",
      "Iteration: 174 Loss: 5.203118148221034e+25\n",
      "Iteration: 175 Loss: 1.1982149021356705e+26\n",
      "Iteration: 176 Loss: 1.8449518464018238e+26\n",
      "Iteration: 177 Loss: 2.9338736637170206e+26\n",
      "Iteration: 178 Loss: 2.3144779301145974e+26\n",
      "Iteration: 179 Loss: 2.774251773637923e+26\n",
      "Iteration: 180 Loss: 5.1440955769479115e+26\n",
      "Iteration: 181 Loss: 7.353286870587404e+26\n",
      "Iteration: 182 Loss: 6.333835675921238e+26\n",
      "Iteration: 183 Loss: 1.0504265983798575e+27\n",
      "Iteration: 184 Loss: 1.0084282320644564e+27\n",
      "Iteration: 185 Loss: 1.3460004757733605e+27\n",
      "Iteration: 186 Loss: 2.9072546799772644e+27\n",
      "Iteration: 187 Loss: 3.0743137069710713e+27\n",
      "Iteration: 188 Loss: 4.373035325257467e+27\n",
      "Iteration: 189 Loss: 5.473693809702585e+27\n",
      "Iteration: 190 Loss: 9.896805699440212e+27\n",
      "Iteration: 191 Loss: 1.6407567196825403e+28\n",
      "Iteration: 192 Loss: 5.693030023957569e+28\n",
      "Iteration: 193 Loss: 4.664815002542913e+28\n",
      "Iteration: 194 Loss: 6.21499083012591e+28\n",
      "Iteration: 195 Loss: 1.0751222876890006e+29\n",
      "Iteration: 196 Loss: 1.2667836870787701e+29\n",
      "Iteration: 197 Loss: 2.712468372626618e+29\n",
      "Iteration: 198 Loss: 2.831039250330443e+29\n",
      "Iteration: 199 Loss: 3.235595131079579e+29\n",
      "Iteration: 200 Loss: 5.89425421547515e+29\n",
      "Iteration: 201 Loss: 8.508498498626035e+29\n",
      "Iteration: 202 Loss: 3.040734347287131e+30\n",
      "Iteration: 203 Loss: 3.2502227557075973e+30\n",
      "Iteration: 204 Loss: 2.4391163421648248e+30\n",
      "Iteration: 205 Loss: 5.861283408270391e+30\n",
      "Iteration: 206 Loss: 1.367432073279508e+31\n",
      "Iteration: 207 Loss: 1.497957738849406e+31\n",
      "Iteration: 208 Loss: 4.415689565942101e+31\n",
      "Iteration: 209 Loss: 3.5665306406233925e+31\n",
      "Iteration: 210 Loss: 5.812296766918951e+31\n",
      "Iteration: 211 Loss: 1.6480856288355112e+32\n",
      "Iteration: 212 Loss: 2.02932725274964e+32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 213 Loss: 3.2535609746625574e+32\n",
      "Iteration: 214 Loss: 7.596324266602881e+32\n",
      "Iteration: 215 Loss: 7.636444355851071e+32\n",
      "Iteration: 216 Loss: 9.751330286990871e+32\n",
      "Iteration: 217 Loss: 1.8786550946881378e+33\n",
      "Iteration: 218 Loss: 3.997609206282039e+33\n",
      "Iteration: 219 Loss: 3.740035045613058e+33\n",
      "Iteration: 220 Loss: 9.361777946051131e+33\n",
      "Iteration: 221 Loss: 1.3358732282052389e+34\n",
      "Iteration: 222 Loss: 1.74036109907945e+34\n",
      "Iteration: 223 Loss: 2.9791301868091398e+34\n",
      "Iteration: 224 Loss: 3.253365596775857e+34\n",
      "Iteration: 225 Loss: 3.4429212038833053e+34\n",
      "Iteration: 226 Loss: 7.206075213009031e+34\n",
      "Iteration: 227 Loss: 1.3000381817472324e+35\n",
      "Iteration: 228 Loss: 1.4969897873429887e+35\n",
      "Iteration: 229 Loss: 2.0061517525153776e+35\n",
      "Iteration: 230 Loss: 3.803453711094216e+35\n",
      "Iteration: 231 Loss: 4.95851713098155e+35\n",
      "Iteration: 232 Loss: 8.716401972124062e+35\n",
      "Iteration: 233 Loss: 1.0869184057990277e+36\n",
      "Iteration: 234 Loss: 1.647861639988108e+36\n",
      "Iteration: 235 Loss: 1.1803281717188578e+36\n",
      "Iteration: 236 Loss: 2.0054912273244962e+36\n",
      "Iteration: 237 Loss: 3.0210649104639163e+36\n",
      "Iteration: 238 Loss: inf\n",
      "Iteration: 239 Loss: inf\n",
      "Iteration: 240 Loss: inf\n",
      "Iteration: 241 Loss: nan\n",
      "Iteration: 242 Loss: nan\n",
      "Iteration: 243 Loss: nan\n",
      "Iteration: 244 Loss: nan\n",
      "Iteration: 245 Loss: nan\n",
      "Iteration: 246 Loss: nan\n",
      "Iteration: 247 Loss: nan\n",
      "Iteration: 248 Loss: nan\n",
      "Iteration: 249 Loss: nan\n",
      "Iteration: 250 Loss: nan\n",
      "Iteration: 251 Loss: nan\n",
      "Iteration: 252 Loss: nan\n",
      "Iteration: 253 Loss: nan\n",
      "Iteration: 254 Loss: nan\n",
      "Iteration: 255 Loss: nan\n",
      "Iteration: 256 Loss: nan\n",
      "Iteration: 257 Loss: nan\n",
      "Iteration: 258 Loss: nan\n",
      "Iteration: 259 Loss: nan\n",
      "Iteration: 260 Loss: nan\n",
      "Iteration: 261 Loss: nan\n",
      "Iteration: 262 Loss: nan\n",
      "Iteration: 263 Loss: nan\n",
      "Iteration: 264 Loss: nan\n",
      "Iteration: 265 Loss: nan\n",
      "Iteration: 266 Loss: nan\n",
      "Iteration: 267 Loss: nan\n",
      "Iteration: 268 Loss: nan\n",
      "Iteration: 269 Loss: nan\n",
      "Iteration: 270 Loss: nan\n",
      "Iteration: 271 Loss: nan\n",
      "Iteration: 272 Loss: nan\n",
      "Iteration: 273 Loss: nan\n",
      "Iteration: 274 Loss: nan\n",
      "Iteration: 275 Loss: nan\n",
      "Iteration: 276 Loss: nan\n",
      "Iteration: 277 Loss: nan\n",
      "Iteration: 278 Loss: nan\n",
      "Iteration: 279 Loss: nan\n",
      "Iteration: 280 Loss: nan\n",
      "Iteration: 281 Loss: nan\n",
      "Iteration: 282 Loss: nan\n",
      "Iteration: 283 Loss: nan\n",
      "Iteration: 284 Loss: nan\n",
      "Iteration: 285 Loss: nan\n",
      "Iteration: 286 Loss: nan\n",
      "Iteration: 287 Loss: nan\n",
      "Iteration: 288 Loss: nan\n",
      "Iteration: 289 Loss: nan\n",
      "Iteration: 290 Loss: nan\n",
      "Iteration: 291 Loss: nan\n",
      "Iteration: 292 Loss: nan\n",
      "Iteration: 293 Loss: nan\n",
      "Iteration: 294 Loss: nan\n",
      "Iteration: 295 Loss: nan\n",
      "Iteration: 296 Loss: nan\n",
      "Iteration: 297 Loss: nan\n",
      "Iteration: 298 Loss: nan\n",
      "Iteration: 299 Loss: nan\n",
      "Iteration: 300 Loss: nan\n",
      "Iteration: 301 Loss: nan\n",
      "Iteration: 302 Loss: nan\n",
      "Iteration: 303 Loss: nan\n",
      "Iteration: 304 Loss: nan\n",
      "Iteration: 305 Loss: nan\n",
      "Iteration: 306 Loss: nan\n",
      "Iteration: 307 Loss: nan\n",
      "Iteration: 308 Loss: nan\n",
      "Iteration: 309 Loss: nan\n",
      "Iteration: 310 Loss: nan\n",
      "Iteration: 311 Loss: nan\n",
      "Iteration: 312 Loss: nan\n",
      "Iteration: 313 Loss: nan\n",
      "Iteration: 314 Loss: nan\n",
      "Iteration: 315 Loss: nan\n",
      "Iteration: 316 Loss: nan\n",
      "Iteration: 317 Loss: nan\n",
      "Iteration: 318 Loss: nan\n",
      "Iteration: 319 Loss: nan\n",
      "Iteration: 320 Loss: nan\n",
      "Iteration: 321 Loss: nan\n",
      "Iteration: 322 Loss: nan\n",
      "Iteration: 323 Loss: nan\n",
      "Iteration: 324 Loss: nan\n",
      "Iteration: 325 Loss: nan\n",
      "Iteration: 326 Loss: nan\n",
      "Iteration: 327 Loss: nan\n",
      "Iteration: 328 Loss: nan\n",
      "Iteration: 329 Loss: nan\n",
      "Iteration: 330 Loss: nan\n",
      "Iteration: 331 Loss: nan\n",
      "Iteration: 332 Loss: nan\n",
      "Iteration: 333 Loss: nan\n",
      "Iteration: 334 Loss: nan\n",
      "Iteration: 335 Loss: nan\n",
      "Iteration: 336 Loss: nan\n",
      "Iteration: 337 Loss: nan\n",
      "Iteration: 338 Loss: nan\n",
      "Iteration: 339 Loss: nan\n",
      "Iteration: 340 Loss: nan\n",
      "Iteration: 341 Loss: nan\n",
      "Iteration: 342 Loss: nan\n",
      "Iteration: 343 Loss: nan\n",
      "Iteration: 344 Loss: nan\n",
      "Iteration: 345 Loss: nan\n",
      "Iteration: 346 Loss: nan\n",
      "Iteration: 347 Loss: nan\n",
      "Iteration: 348 Loss: nan\n",
      "Iteration: 349 Loss: nan\n",
      "Iteration: 350 Loss: nan\n",
      "Iteration: 351 Loss: nan\n",
      "Iteration: 352 Loss: nan\n",
      "Iteration: 353 Loss: nan\n",
      "Iteration: 354 Loss: nan\n",
      "Iteration: 355 Loss: nan\n",
      "Iteration: 356 Loss: nan\n",
      "Iteration: 357 Loss: nan\n",
      "Iteration: 358 Loss: nan\n",
      "Iteration: 359 Loss: nan\n",
      "Iteration: 360 Loss: nan\n",
      "Iteration: 361 Loss: nan\n",
      "Iteration: 362 Loss: nan\n",
      "Iteration: 363 Loss: nan\n",
      "Iteration: 364 Loss: nan\n",
      "Iteration: 365 Loss: nan\n",
      "Iteration: 366 Loss: nan\n",
      "Iteration: 367 Loss: nan\n",
      "Iteration: 368 Loss: nan\n",
      "Iteration: 369 Loss: nan\n",
      "Iteration: 370 Loss: nan\n",
      "Iteration: 371 Loss: nan\n",
      "Iteration: 372 Loss: nan\n",
      "Iteration: 373 Loss: nan\n",
      "Iteration: 374 Loss: nan\n",
      "Iteration: 375 Loss: nan\n",
      "Iteration: 376 Loss: nan\n",
      "Iteration: 377 Loss: nan\n",
      "Iteration: 378 Loss: nan\n",
      "Iteration: 379 Loss: nan\n",
      "Iteration: 380 Loss: nan\n",
      "Iteration: 381 Loss: nan\n",
      "Iteration: 382 Loss: nan\n",
      "Iteration: 383 Loss: nan\n",
      "Iteration: 384 Loss: nan\n",
      "Iteration: 385 Loss: nan\n",
      "Iteration: 386 Loss: nan\n",
      "Iteration: 387 Loss: nan\n",
      "Iteration: 388 Loss: nan\n",
      "Iteration: 389 Loss: nan\n",
      "Iteration: 390 Loss: nan\n",
      "Iteration: 391 Loss: nan\n",
      "Iteration: 392 Loss: nan\n",
      "Iteration: 393 Loss: nan\n",
      "Iteration: 394 Loss: nan\n",
      "Iteration: 395 Loss: nan\n",
      "Iteration: 396 Loss: nan\n",
      "Iteration: 397 Loss: nan\n",
      "Iteration: 398 Loss: nan\n",
      "Iteration: 399 Loss: nan\n",
      "Iteration: 400 Loss: nan\n",
      "Iteration: 401 Loss: nan\n",
      "Iteration: 402 Loss: nan\n",
      "Iteration: 403 Loss: nan\n",
      "Iteration: 404 Loss: nan\n",
      "Iteration: 405 Loss: nan\n",
      "Iteration: 406 Loss: nan\n",
      "Iteration: 407 Loss: nan\n",
      "Iteration: 408 Loss: nan\n",
      "Iteration: 409 Loss: nan\n",
      "Iteration: 410 Loss: nan\n",
      "Iteration: 411 Loss: nan\n",
      "Iteration: 412 Loss: nan\n",
      "Iteration: 413 Loss: nan\n",
      "Iteration: 414 Loss: nan\n",
      "Iteration: 415 Loss: nan\n",
      "Iteration: 416 Loss: nan\n",
      "Iteration: 417 Loss: nan\n",
      "Iteration: 418 Loss: nan\n",
      "Iteration: 419 Loss: nan\n",
      "Iteration: 420 Loss: nan\n",
      "Iteration: 421 Loss: nan\n",
      "Iteration: 422 Loss: nan\n",
      "Iteration: 423 Loss: nan\n",
      "Iteration: 424 Loss: nan\n",
      "Iteration: 425 Loss: nan\n",
      "Iteration: 426 Loss: nan\n",
      "Iteration: 427 Loss: nan\n",
      "Iteration: 428 Loss: nan\n",
      "Iteration: 429 Loss: nan\n",
      "Iteration: 430 Loss: nan\n",
      "Iteration: 431 Loss: nan\n",
      "Iteration: 432 Loss: nan\n",
      "Iteration: 433 Loss: nan\n",
      "Iteration: 434 Loss: nan\n",
      "Iteration: 435 Loss: nan\n",
      "Iteration: 436 Loss: nan\n",
      "Iteration: 437 Loss: nan\n",
      "Iteration: 438 Loss: nan\n",
      "Iteration: 439 Loss: nan\n",
      "Iteration: 440 Loss: nan\n",
      "Iteration: 441 Loss: nan\n",
      "Iteration: 442 Loss: nan\n",
      "Iteration: 443 Loss: nan\n",
      "Iteration: 444 Loss: nan\n",
      "Iteration: 445 Loss: nan\n",
      "Iteration: 446 Loss: nan\n",
      "Iteration: 447 Loss: nan\n",
      "Iteration: 448 Loss: nan\n",
      "Iteration: 449 Loss: nan\n",
      "Iteration: 450 Loss: nan\n",
      "Iteration: 451 Loss: nan\n",
      "Iteration: 452 Loss: nan\n",
      "Iteration: 453 Loss: nan\n",
      "Iteration: 454 Loss: nan\n",
      "Iteration: 455 Loss: nan\n",
      "Iteration: 456 Loss: nan\n",
      "Iteration: 457 Loss: nan\n",
      "Iteration: 458 Loss: nan\n",
      "Iteration: 459 Loss: nan\n",
      "Iteration: 460 Loss: nan\n",
      "Iteration: 461 Loss: nan\n",
      "Iteration: 462 Loss: nan\n",
      "Iteration: 463 Loss: nan\n",
      "Iteration: 464 Loss: nan\n",
      "Iteration: 465 Loss: nan\n",
      "Iteration: 466 Loss: nan\n",
      "Iteration: 467 Loss: nan\n",
      "Iteration: 468 Loss: nan\n",
      "Iteration: 469 Loss: nan\n",
      "Iteration: 470 Loss: nan\n",
      "Iteration: 471 Loss: nan\n",
      "Iteration: 472 Loss: nan\n",
      "Iteration: 473 Loss: nan\n",
      "Iteration: 474 Loss: nan\n",
      "Iteration: 475 Loss: nan\n",
      "Iteration: 476 Loss: nan\n",
      "Iteration: 477 Loss: nan\n",
      "Iteration: 478 Loss: nan\n",
      "Iteration: 479 Loss: nan\n",
      "Iteration: 480 Loss: nan\n",
      "Iteration: 481 Loss: nan\n",
      "Iteration: 482 Loss: nan\n",
      "Iteration: 483 Loss: nan\n",
      "Iteration: 484 Loss: nan\n",
      "Iteration: 485 Loss: nan\n",
      "Iteration: 486 Loss: nan\n",
      "Iteration: 487 Loss: nan\n",
      "Iteration: 488 Loss: nan\n",
      "Iteration: 489 Loss: nan\n",
      "Iteration: 490 Loss: nan\n",
      "Iteration: 491 Loss: nan\n",
      "Iteration: 492 Loss: nan\n",
      "Iteration: 493 Loss: nan\n",
      "Iteration: 494 Loss: nan\n",
      "Iteration: 495 Loss: nan\n",
      "Iteration: 496 Loss: nan\n",
      "Iteration: 497 Loss: nan\n",
      "Iteration: 498 Loss: nan\n",
      "Iteration: 499 Loss: nan\n",
      "Iteration: 500 Loss: nan\n",
      "Iteration: 501 Loss: nan\n",
      "Iteration: 502 Loss: nan\n",
      "Iteration: 503 Loss: nan\n",
      "Iteration: 504 Loss: nan\n",
      "Iteration: 505 Loss: nan\n",
      "Iteration: 506 Loss: nan\n",
      "Iteration: 507 Loss: nan\n",
      "Iteration: 508 Loss: nan\n",
      "Iteration: 509 Loss: nan\n",
      "Iteration: 510 Loss: nan\n",
      "Iteration: 511 Loss: nan\n",
      "Iteration: 512 Loss: nan\n",
      "Iteration: 513 Loss: nan\n",
      "Iteration: 514 Loss: nan\n",
      "Iteration: 515 Loss: nan\n",
      "Iteration: 516 Loss: nan\n",
      "Iteration: 517 Loss: nan\n",
      "Iteration: 518 Loss: nan\n",
      "Iteration: 519 Loss: nan\n",
      "Iteration: 520 Loss: nan\n",
      "Iteration: 521 Loss: nan\n",
      "Iteration: 522 Loss: nan\n",
      "Iteration: 523 Loss: nan\n",
      "Iteration: 524 Loss: nan\n",
      "Iteration: 525 Loss: nan\n",
      "Iteration: 526 Loss: nan\n",
      "Iteration: 527 Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 528 Loss: nan\n",
      "Iteration: 529 Loss: nan\n",
      "Iteration: 530 Loss: nan\n",
      "Iteration: 531 Loss: nan\n",
      "Iteration: 532 Loss: nan\n",
      "Iteration: 533 Loss: nan\n",
      "Iteration: 534 Loss: nan\n",
      "Iteration: 535 Loss: nan\n",
      "Iteration: 536 Loss: nan\n",
      "Iteration: 537 Loss: nan\n",
      "Iteration: 538 Loss: nan\n",
      "Iteration: 539 Loss: nan\n",
      "Iteration: 540 Loss: nan\n",
      "Iteration: 541 Loss: nan\n",
      "Iteration: 542 Loss: nan\n",
      "Iteration: 543 Loss: nan\n",
      "Iteration: 544 Loss: nan\n",
      "Iteration: 545 Loss: nan\n",
      "Iteration: 546 Loss: nan\n",
      "Iteration: 547 Loss: nan\n",
      "Iteration: 548 Loss: nan\n",
      "Iteration: 549 Loss: nan\n",
      "Iteration: 550 Loss: nan\n",
      "Iteration: 551 Loss: nan\n",
      "Iteration: 552 Loss: nan\n",
      "Iteration: 553 Loss: nan\n",
      "Iteration: 554 Loss: nan\n",
      "Iteration: 555 Loss: nan\n",
      "Iteration: 556 Loss: nan\n",
      "Iteration: 557 Loss: nan\n",
      "Iteration: 558 Loss: nan\n",
      "Iteration: 559 Loss: nan\n",
      "Iteration: 560 Loss: nan\n",
      "Iteration: 561 Loss: nan\n",
      "Iteration: 562 Loss: nan\n",
      "Iteration: 563 Loss: nan\n",
      "Iteration: 564 Loss: nan\n",
      "Iteration: 565 Loss: nan\n",
      "Iteration: 566 Loss: nan\n",
      "Iteration: 567 Loss: nan\n",
      "Iteration: 568 Loss: nan\n",
      "Iteration: 569 Loss: nan\n",
      "Iteration: 570 Loss: nan\n",
      "Iteration: 571 Loss: nan\n",
      "Iteration: 572 Loss: nan\n",
      "Iteration: 573 Loss: nan\n",
      "Iteration: 574 Loss: nan\n",
      "Iteration: 575 Loss: nan\n",
      "Iteration: 576 Loss: nan\n",
      "Iteration: 577 Loss: nan\n",
      "Iteration: 578 Loss: nan\n",
      "Iteration: 579 Loss: nan\n",
      "Iteration: 580 Loss: nan\n",
      "Iteration: 581 Loss: nan\n",
      "Iteration: 582 Loss: nan\n",
      "Iteration: 583 Loss: nan\n",
      "Iteration: 584 Loss: nan\n",
      "Iteration: 585 Loss: nan\n",
      "Iteration: 586 Loss: nan\n",
      "Iteration: 587 Loss: nan\n",
      "Iteration: 588 Loss: nan\n",
      "Iteration: 589 Loss: nan\n",
      "Iteration: 590 Loss: nan\n",
      "Iteration: 591 Loss: nan\n",
      "Iteration: 592 Loss: nan\n",
      "Iteration: 593 Loss: nan\n",
      "Iteration: 594 Loss: nan\n",
      "Iteration: 595 Loss: nan\n",
      "Iteration: 596 Loss: nan\n",
      "Iteration: 597 Loss: nan\n",
      "Iteration: 598 Loss: nan\n",
      "Iteration: 599 Loss: nan\n",
      "Iteration: 600 Loss: nan\n",
      "Iteration: 601 Loss: nan\n",
      "Iteration: 602 Loss: nan\n",
      "Iteration: 603 Loss: nan\n",
      "Iteration: 604 Loss: nan\n",
      "Iteration: 605 Loss: nan\n",
      "Iteration: 606 Loss: nan\n",
      "Iteration: 607 Loss: nan\n",
      "Iteration: 608 Loss: nan\n",
      "Iteration: 609 Loss: nan\n",
      "Iteration: 610 Loss: nan\n",
      "Iteration: 611 Loss: nan\n",
      "Iteration: 612 Loss: nan\n",
      "Iteration: 613 Loss: nan\n",
      "Iteration: 614 Loss: nan\n",
      "Iteration: 615 Loss: nan\n",
      "Iteration: 616 Loss: nan\n",
      "Iteration: 617 Loss: nan\n",
      "Iteration: 618 Loss: nan\n",
      "Iteration: 619 Loss: nan\n",
      "Iteration: 620 Loss: nan\n",
      "Iteration: 621 Loss: nan\n",
      "Iteration: 622 Loss: nan\n",
      "Iteration: 623 Loss: nan\n",
      "Iteration: 624 Loss: nan\n",
      "Iteration: 625 Loss: nan\n",
      "Iteration: 626 Loss: nan\n",
      "Iteration: 627 Loss: nan\n",
      "Iteration: 628 Loss: nan\n",
      "Iteration: 629 Loss: nan\n",
      "Iteration: 630 Loss: nan\n",
      "Iteration: 631 Loss: nan\n",
      "Iteration: 632 Loss: nan\n",
      "Iteration: 633 Loss: nan\n",
      "Iteration: 634 Loss: nan\n",
      "Iteration: 635 Loss: nan\n",
      "Iteration: 636 Loss: nan\n",
      "Iteration: 637 Loss: nan\n",
      "Iteration: 638 Loss: nan\n",
      "Iteration: 639 Loss: nan\n",
      "Iteration: 640 Loss: nan\n",
      "Iteration: 641 Loss: nan\n",
      "Iteration: 642 Loss: nan\n",
      "Iteration: 643 Loss: nan\n",
      "Iteration: 644 Loss: nan\n",
      "Iteration: 645 Loss: nan\n",
      "Iteration: 646 Loss: nan\n",
      "Iteration: 647 Loss: nan\n",
      "Iteration: 648 Loss: nan\n",
      "Iteration: 649 Loss: nan\n",
      "Iteration: 650 Loss: nan\n",
      "Iteration: 651 Loss: nan\n",
      "Iteration: 652 Loss: nan\n",
      "Iteration: 653 Loss: nan\n",
      "Iteration: 654 Loss: nan\n",
      "Iteration: 655 Loss: nan\n",
      "Iteration: 656 Loss: nan\n",
      "Iteration: 657 Loss: nan\n",
      "Iteration: 658 Loss: nan\n",
      "Iteration: 659 Loss: nan\n",
      "Iteration: 660 Loss: nan\n",
      "Iteration: 661 Loss: nan\n",
      "Iteration: 662 Loss: nan\n",
      "Iteration: 663 Loss: nan\n",
      "Iteration: 664 Loss: nan\n",
      "Iteration: 665 Loss: nan\n",
      "Iteration: 666 Loss: nan\n",
      "Iteration: 667 Loss: nan\n",
      "Iteration: 668 Loss: nan\n",
      "Iteration: 669 Loss: nan\n",
      "Iteration: 670 Loss: nan\n",
      "Iteration: 671 Loss: nan\n",
      "Iteration: 672 Loss: nan\n",
      "Iteration: 673 Loss: nan\n",
      "Iteration: 674 Loss: nan\n",
      "Iteration: 675 Loss: nan\n",
      "Iteration: 676 Loss: nan\n",
      "Iteration: 677 Loss: nan\n",
      "Iteration: 678 Loss: nan\n",
      "Iteration: 679 Loss: nan\n",
      "Iteration: 680 Loss: nan\n",
      "Iteration: 681 Loss: nan\n",
      "Iteration: 682 Loss: nan\n",
      "Iteration: 683 Loss: nan\n",
      "Iteration: 684 Loss: nan\n",
      "Iteration: 685 Loss: nan\n",
      "Iteration: 686 Loss: nan\n",
      "Iteration: 687 Loss: nan\n",
      "Iteration: 688 Loss: nan\n",
      "Iteration: 689 Loss: nan\n",
      "Iteration: 690 Loss: nan\n",
      "Iteration: 691 Loss: nan\n",
      "Iteration: 692 Loss: nan\n",
      "Iteration: 693 Loss: nan\n",
      "Iteration: 694 Loss: nan\n",
      "Iteration: 695 Loss: nan\n",
      "Iteration: 696 Loss: nan\n",
      "Iteration: 697 Loss: nan\n",
      "Iteration: 698 Loss: nan\n",
      "Iteration: 699 Loss: nan\n",
      "Iteration: 700 Loss: nan\n",
      "Iteration: 701 Loss: nan\n",
      "Iteration: 702 Loss: nan\n",
      "Iteration: 703 Loss: nan\n",
      "Iteration: 704 Loss: nan\n",
      "Iteration: 705 Loss: nan\n",
      "Iteration: 706 Loss: nan\n",
      "Iteration: 707 Loss: nan\n",
      "Iteration: 708 Loss: nan\n",
      "Iteration: 709 Loss: nan\n",
      "Iteration: 710 Loss: nan\n",
      "Iteration: 711 Loss: nan\n",
      "Iteration: 712 Loss: nan\n",
      "Iteration: 713 Loss: nan\n",
      "Iteration: 714 Loss: nan\n",
      "Iteration: 715 Loss: nan\n",
      "Iteration: 716 Loss: nan\n",
      "Iteration: 717 Loss: nan\n",
      "Iteration: 718 Loss: nan\n",
      "Iteration: 719 Loss: nan\n",
      "Iteration: 720 Loss: nan\n",
      "Iteration: 721 Loss: nan\n",
      "Iteration: 722 Loss: nan\n",
      "Iteration: 723 Loss: nan\n",
      "Iteration: 724 Loss: nan\n",
      "Iteration: 725 Loss: nan\n",
      "Iteration: 726 Loss: nan\n",
      "Iteration: 727 Loss: nan\n",
      "Iteration: 728 Loss: nan\n",
      "Iteration: 729 Loss: nan\n",
      "Iteration: 730 Loss: nan\n",
      "Iteration: 731 Loss: nan\n",
      "Iteration: 732 Loss: nan\n",
      "Iteration: 733 Loss: nan\n",
      "Iteration: 734 Loss: nan\n",
      "Iteration: 735 Loss: nan\n",
      "Iteration: 736 Loss: nan\n",
      "Iteration: 737 Loss: nan\n",
      "Iteration: 738 Loss: nan\n",
      "Iteration: 739 Loss: nan\n",
      "Iteration: 740 Loss: nan\n",
      "Iteration: 741 Loss: nan\n",
      "Iteration: 742 Loss: nan\n",
      "Iteration: 743 Loss: nan\n",
      "Iteration: 744 Loss: nan\n",
      "Iteration: 745 Loss: nan\n",
      "Iteration: 746 Loss: nan\n",
      "Iteration: 747 Loss: nan\n",
      "Iteration: 748 Loss: nan\n",
      "Iteration: 749 Loss: nan\n",
      "Iteration: 750 Loss: nan\n",
      "Iteration: 751 Loss: nan\n",
      "Iteration: 752 Loss: nan\n",
      "Iteration: 753 Loss: nan\n",
      "Iteration: 754 Loss: nan\n",
      "Iteration: 755 Loss: nan\n",
      "Iteration: 756 Loss: nan\n",
      "Iteration: 757 Loss: nan\n",
      "Iteration: 758 Loss: nan\n",
      "Iteration: 759 Loss: nan\n",
      "Iteration: 760 Loss: nan\n",
      "Iteration: 761 Loss: nan\n",
      "Iteration: 762 Loss: nan\n",
      "Iteration: 763 Loss: nan\n",
      "Iteration: 764 Loss: nan\n",
      "Iteration: 765 Loss: nan\n",
      "Iteration: 766 Loss: nan\n",
      "Iteration: 767 Loss: nan\n",
      "Iteration: 768 Loss: nan\n",
      "Iteration: 769 Loss: nan\n",
      "Iteration: 770 Loss: nan\n",
      "Iteration: 771 Loss: nan\n",
      "Iteration: 772 Loss: nan\n",
      "Iteration: 773 Loss: nan\n",
      "Iteration: 774 Loss: nan\n",
      "Iteration: 775 Loss: nan\n",
      "Iteration: 776 Loss: nan\n",
      "Iteration: 777 Loss: nan\n",
      "Iteration: 778 Loss: nan\n",
      "Iteration: 779 Loss: nan\n",
      "Iteration: 780 Loss: nan\n",
      "Iteration: 781 Loss: nan\n",
      "Iteration: 782 Loss: nan\n",
      "Iteration: 783 Loss: nan\n",
      "Iteration: 784 Loss: nan\n",
      "Iteration: 785 Loss: nan\n",
      "Iteration: 786 Loss: nan\n",
      "Iteration: 787 Loss: nan\n",
      "Iteration: 788 Loss: nan\n",
      "Iteration: 789 Loss: nan\n",
      "Iteration: 790 Loss: nan\n",
      "Iteration: 791 Loss: nan\n",
      "Iteration: 792 Loss: nan\n",
      "Iteration: 793 Loss: nan\n",
      "Iteration: 794 Loss: nan\n",
      "Iteration: 795 Loss: nan\n",
      "Iteration: 796 Loss: nan\n",
      "Iteration: 797 Loss: nan\n",
      "Iteration: 798 Loss: nan\n",
      "Iteration: 799 Loss: nan\n",
      "Iteration: 800 Loss: nan\n",
      "Iteration: 801 Loss: nan\n",
      "Iteration: 802 Loss: nan\n",
      "Iteration: 803 Loss: nan\n",
      "Iteration: 804 Loss: nan\n",
      "Iteration: 805 Loss: nan\n",
      "Iteration: 806 Loss: nan\n",
      "Iteration: 807 Loss: nan\n",
      "Iteration: 808 Loss: nan\n",
      "Iteration: 809 Loss: nan\n",
      "Iteration: 810 Loss: nan\n",
      "Iteration: 811 Loss: nan\n",
      "Iteration: 812 Loss: nan\n",
      "Iteration: 813 Loss: nan\n",
      "Iteration: 814 Loss: nan\n",
      "Iteration: 815 Loss: nan\n",
      "Iteration: 816 Loss: nan\n",
      "Iteration: 817 Loss: nan\n",
      "Iteration: 818 Loss: nan\n",
      "Iteration: 819 Loss: nan\n",
      "Iteration: 820 Loss: nan\n",
      "Iteration: 821 Loss: nan\n",
      "Iteration: 822 Loss: nan\n",
      "Iteration: 823 Loss: nan\n",
      "Iteration: 824 Loss: nan\n",
      "Iteration: 825 Loss: nan\n",
      "Iteration: 826 Loss: nan\n",
      "Iteration: 827 Loss: nan\n",
      "Iteration: 828 Loss: nan\n",
      "Iteration: 829 Loss: nan\n",
      "Iteration: 830 Loss: nan\n",
      "Iteration: 831 Loss: nan\n",
      "Iteration: 832 Loss: nan\n",
      "Iteration: 833 Loss: nan\n",
      "Iteration: 834 Loss: nan\n",
      "Iteration: 835 Loss: nan\n",
      "Iteration: 836 Loss: nan\n",
      "Iteration: 837 Loss: nan\n",
      "Iteration: 838 Loss: nan\n",
      "Iteration: 839 Loss: nan\n",
      "Iteration: 840 Loss: nan\n",
      "Iteration: 841 Loss: nan\n",
      "Iteration: 842 Loss: nan\n",
      "Iteration: 843 Loss: nan\n",
      "Iteration: 844 Loss: nan\n",
      "Iteration: 845 Loss: nan\n",
      "Iteration: 846 Loss: nan\n",
      "Iteration: 847 Loss: nan\n",
      "Iteration: 848 Loss: nan\n",
      "Iteration: 849 Loss: nan\n",
      "Iteration: 850 Loss: nan\n",
      "Iteration: 851 Loss: nan\n",
      "Iteration: 852 Loss: nan\n",
      "Iteration: 853 Loss: nan\n",
      "Iteration: 854 Loss: nan\n",
      "Iteration: 855 Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 856 Loss: nan\n",
      "Iteration: 857 Loss: nan\n",
      "Iteration: 858 Loss: nan\n",
      "Iteration: 859 Loss: nan\n",
      "Iteration: 860 Loss: nan\n",
      "Iteration: 861 Loss: nan\n",
      "Iteration: 862 Loss: nan\n",
      "Iteration: 863 Loss: nan\n",
      "Iteration: 864 Loss: nan\n",
      "Iteration: 865 Loss: nan\n",
      "Iteration: 866 Loss: nan\n",
      "Iteration: 867 Loss: nan\n",
      "Iteration: 868 Loss: nan\n",
      "Iteration: 869 Loss: nan\n",
      "Iteration: 870 Loss: nan\n",
      "Iteration: 871 Loss: nan\n",
      "Iteration: 872 Loss: nan\n",
      "Iteration: 873 Loss: nan\n",
      "Iteration: 874 Loss: nan\n",
      "Iteration: 875 Loss: nan\n",
      "Iteration: 876 Loss: nan\n",
      "Iteration: 877 Loss: nan\n",
      "Iteration: 878 Loss: nan\n",
      "Iteration: 879 Loss: nan\n",
      "Iteration: 880 Loss: nan\n",
      "Iteration: 881 Loss: nan\n",
      "Iteration: 882 Loss: nan\n",
      "Iteration: 883 Loss: nan\n",
      "Iteration: 884 Loss: nan\n",
      "Iteration: 885 Loss: nan\n",
      "Iteration: 886 Loss: nan\n",
      "Iteration: 887 Loss: nan\n",
      "Iteration: 888 Loss: nan\n",
      "Iteration: 889 Loss: nan\n",
      "Iteration: 890 Loss: nan\n",
      "Iteration: 891 Loss: nan\n",
      "Iteration: 892 Loss: nan\n",
      "Iteration: 893 Loss: nan\n",
      "Iteration: 894 Loss: nan\n",
      "Iteration: 895 Loss: nan\n",
      "Iteration: 896 Loss: nan\n",
      "Iteration: 897 Loss: nan\n",
      "Iteration: 898 Loss: nan\n",
      "Iteration: 899 Loss: nan\n",
      "Iteration: 900 Loss: nan\n",
      "Iteration: 901 Loss: nan\n",
      "Iteration: 902 Loss: nan\n",
      "Iteration: 903 Loss: nan\n",
      "Iteration: 904 Loss: nan\n",
      "Iteration: 905 Loss: nan\n",
      "Iteration: 906 Loss: nan\n",
      "Iteration: 907 Loss: nan\n",
      "Iteration: 908 Loss: nan\n",
      "Iteration: 909 Loss: nan\n",
      "Iteration: 910 Loss: nan\n",
      "Iteration: 911 Loss: nan\n",
      "Iteration: 912 Loss: nan\n",
      "Iteration: 913 Loss: nan\n",
      "Iteration: 914 Loss: nan\n",
      "Iteration: 915 Loss: nan\n",
      "Iteration: 916 Loss: nan\n",
      "Iteration: 917 Loss: nan\n",
      "Iteration: 918 Loss: nan\n",
      "Iteration: 919 Loss: nan\n",
      "Iteration: 920 Loss: nan\n",
      "Iteration: 921 Loss: nan\n",
      "Iteration: 922 Loss: nan\n",
      "Iteration: 923 Loss: nan\n",
      "Iteration: 924 Loss: nan\n",
      "Iteration: 925 Loss: nan\n",
      "Iteration: 926 Loss: nan\n",
      "Iteration: 927 Loss: nan\n",
      "Iteration: 928 Loss: nan\n",
      "Iteration: 929 Loss: nan\n",
      "Iteration: 930 Loss: nan\n",
      "Iteration: 931 Loss: nan\n",
      "Iteration: 932 Loss: nan\n",
      "Iteration: 933 Loss: nan\n",
      "Iteration: 934 Loss: nan\n",
      "Iteration: 935 Loss: nan\n",
      "Iteration: 936 Loss: nan\n",
      "Iteration: 937 Loss: nan\n",
      "Iteration: 938 Loss: nan\n",
      "Iteration: 939 Loss: nan\n",
      "Iteration: 940 Loss: nan\n",
      "Iteration: 941 Loss: nan\n",
      "Iteration: 942 Loss: nan\n",
      "Iteration: 943 Loss: nan\n",
      "Iteration: 944 Loss: nan\n",
      "Iteration: 945 Loss: nan\n",
      "Iteration: 946 Loss: nan\n",
      "Iteration: 947 Loss: nan\n",
      "Iteration: 948 Loss: nan\n",
      "Iteration: 949 Loss: nan\n",
      "Iteration: 950 Loss: nan\n",
      "Iteration: 951 Loss: nan\n",
      "Iteration: 952 Loss: nan\n",
      "Iteration: 953 Loss: nan\n",
      "Iteration: 954 Loss: nan\n",
      "Iteration: 955 Loss: nan\n",
      "Iteration: 956 Loss: nan\n",
      "Iteration: 957 Loss: nan\n",
      "Iteration: 958 Loss: nan\n",
      "Iteration: 959 Loss: nan\n",
      "Iteration: 960 Loss: nan\n",
      "Iteration: 961 Loss: nan\n",
      "Iteration: 962 Loss: nan\n",
      "Iteration: 963 Loss: nan\n",
      "Iteration: 964 Loss: nan\n",
      "Iteration: 965 Loss: nan\n",
      "Iteration: 966 Loss: nan\n",
      "Iteration: 967 Loss: nan\n",
      "Iteration: 968 Loss: nan\n",
      "Iteration: 969 Loss: nan\n",
      "Iteration: 970 Loss: nan\n",
      "Iteration: 971 Loss: nan\n",
      "Iteration: 972 Loss: nan\n",
      "Iteration: 973 Loss: nan\n",
      "Iteration: 974 Loss: nan\n",
      "Iteration: 975 Loss: nan\n",
      "Iteration: 976 Loss: nan\n",
      "Iteration: 977 Loss: nan\n",
      "Iteration: 978 Loss: nan\n",
      "Iteration: 979 Loss: nan\n",
      "Iteration: 980 Loss: nan\n",
      "Iteration: 981 Loss: nan\n",
      "Iteration: 982 Loss: nan\n",
      "Iteration: 983 Loss: nan\n",
      "Iteration: 984 Loss: nan\n",
      "Iteration: 985 Loss: nan\n",
      "Iteration: 986 Loss: nan\n",
      "Iteration: 987 Loss: nan\n",
      "Iteration: 988 Loss: nan\n",
      "Iteration: 989 Loss: nan\n",
      "Iteration: 990 Loss: nan\n",
      "Iteration: 991 Loss: nan\n",
      "Iteration: 992 Loss: nan\n",
      "Iteration: 993 Loss: nan\n",
      "Iteration: 994 Loss: nan\n",
      "Iteration: 995 Loss: nan\n",
      "Iteration: 996 Loss: nan\n",
      "Iteration: 997 Loss: nan\n",
      "Iteration: 998 Loss: nan\n",
      "Iteration: 999 Loss: nan\n",
      "Iteration: 1000 Loss: nan\n",
      "Iteration: 1001 Loss: nan\n",
      "Iteration: 1002 Loss: nan\n",
      "Iteration: 1003 Loss: nan\n",
      "Iteration: 1004 Loss: nan\n",
      "Iteration: 1005 Loss: nan\n",
      "Iteration: 1006 Loss: nan\n",
      "Iteration: 1007 Loss: nan\n",
      "Iteration: 1008 Loss: nan\n",
      "Iteration: 1009 Loss: nan\n",
      "Iteration: 1010 Loss: nan\n",
      "Iteration: 1011 Loss: nan\n",
      "Iteration: 1012 Loss: nan\n",
      "Iteration: 1013 Loss: nan\n",
      "Iteration: 1014 Loss: nan\n",
      "Iteration: 1015 Loss: nan\n",
      "Iteration: 1016 Loss: nan\n",
      "Iteration: 1017 Loss: nan\n",
      "Iteration: 1018 Loss: nan\n",
      "Iteration: 1019 Loss: nan\n",
      "Iteration: 1020 Loss: nan\n",
      "Iteration: 1021 Loss: nan\n",
      "Iteration: 1022 Loss: nan\n",
      "Iteration: 1023 Loss: nan\n",
      "Iteration: 1024 Loss: nan\n",
      "Iteration: 1025 Loss: nan\n",
      "Iteration: 1026 Loss: nan\n",
      "Iteration: 1027 Loss: nan\n",
      "Iteration: 1028 Loss: nan\n",
      "Iteration: 1029 Loss: nan\n",
      "Iteration: 1030 Loss: nan\n",
      "Iteration: 1031 Loss: nan\n",
      "Iteration: 1032 Loss: nan\n",
      "Iteration: 1033 Loss: nan\n",
      "Iteration: 1034 Loss: nan\n",
      "Iteration: 1035 Loss: nan\n",
      "Iteration: 1036 Loss: nan\n",
      "Iteration: 1037 Loss: nan\n",
      "Iteration: 1038 Loss: nan\n",
      "Iteration: 1039 Loss: nan\n",
      "Iteration: 1040 Loss: nan\n",
      "Iteration: 1041 Loss: nan\n",
      "Iteration: 1042 Loss: nan\n",
      "Iteration: 1043 Loss: nan\n",
      "Iteration: 1044 Loss: nan\n",
      "Iteration: 1045 Loss: nan\n",
      "Iteration: 1046 Loss: nan\n",
      "Iteration: 1047 Loss: nan\n",
      "Iteration: 1048 Loss: nan\n",
      "Iteration: 1049 Loss: nan\n",
      "Iteration: 1050 Loss: nan\n",
      "Iteration: 1051 Loss: nan\n",
      "Iteration: 1052 Loss: nan\n",
      "Iteration: 1053 Loss: nan\n",
      "Iteration: 1054 Loss: nan\n",
      "Iteration: 1055 Loss: nan\n",
      "Iteration: 1056 Loss: nan\n",
      "Iteration: 1057 Loss: nan\n",
      "Iteration: 1058 Loss: nan\n",
      "Iteration: 1059 Loss: nan\n",
      "Iteration: 1060 Loss: nan\n",
      "Iteration: 1061 Loss: nan\n",
      "Iteration: 1062 Loss: nan\n",
      "Iteration: 1063 Loss: nan\n",
      "Iteration: 1064 Loss: nan\n",
      "Iteration: 1065 Loss: nan\n",
      "Iteration: 1066 Loss: nan\n",
      "Iteration: 1067 Loss: nan\n",
      "Iteration: 1068 Loss: nan\n",
      "Iteration: 1069 Loss: nan\n",
      "Iteration: 1070 Loss: nan\n",
      "Iteration: 1071 Loss: nan\n",
      "Iteration: 1072 Loss: nan\n",
      "Iteration: 1073 Loss: nan\n",
      "Iteration: 1074 Loss: nan\n",
      "Iteration: 1075 Loss: nan\n",
      "Iteration: 1076 Loss: nan\n",
      "Iteration: 1077 Loss: nan\n",
      "Iteration: 1078 Loss: nan\n",
      "Iteration: 1079 Loss: nan\n",
      "Iteration: 1080 Loss: nan\n",
      "Iteration: 1081 Loss: nan\n",
      "Iteration: 1082 Loss: nan\n",
      "Iteration: 1083 Loss: nan\n",
      "Iteration: 1084 Loss: nan\n",
      "Iteration: 1085 Loss: nan\n",
      "Iteration: 1086 Loss: nan\n",
      "Iteration: 1087 Loss: nan\n",
      "Iteration: 1088 Loss: nan\n",
      "Iteration: 1089 Loss: nan\n",
      "Iteration: 1090 Loss: nan\n",
      "Iteration: 1091 Loss: nan\n",
      "Iteration: 1092 Loss: nan\n",
      "Iteration: 1093 Loss: nan\n",
      "Iteration: 1094 Loss: nan\n",
      "Iteration: 1095 Loss: nan\n",
      "Iteration: 1096 Loss: nan\n",
      "Iteration: 1097 Loss: nan\n",
      "Iteration: 1098 Loss: nan\n",
      "Iteration: 1099 Loss: nan\n",
      "Iteration: 1100 Loss: nan\n",
      "Iteration: 1101 Loss: nan\n",
      "Iteration: 1102 Loss: nan\n",
      "Iteration: 1103 Loss: nan\n",
      "Iteration: 1104 Loss: nan\n",
      "Iteration: 1105 Loss: nan\n",
      "Iteration: 1106 Loss: nan\n",
      "Iteration: 1107 Loss: nan\n",
      "Iteration: 1108 Loss: nan\n",
      "Iteration: 1109 Loss: nan\n",
      "Iteration: 1110 Loss: nan\n",
      "Iteration: 1111 Loss: nan\n",
      "Iteration: 1112 Loss: nan\n",
      "Iteration: 1113 Loss: nan\n",
      "Iteration: 1114 Loss: nan\n",
      "Iteration: 1115 Loss: nan\n",
      "Iteration: 1116 Loss: nan\n",
      "Iteration: 1117 Loss: nan\n",
      "Iteration: 1118 Loss: nan\n",
      "Iteration: 1119 Loss: nan\n",
      "Iteration: 1120 Loss: nan\n",
      "Iteration: 1121 Loss: nan\n",
      "Iteration: 1122 Loss: nan\n",
      "Iteration: 1123 Loss: nan\n",
      "Iteration: 1124 Loss: nan\n",
      "Iteration: 1125 Loss: nan\n",
      "Iteration: 1126 Loss: nan\n",
      "Iteration: 1127 Loss: nan\n",
      "Iteration: 1128 Loss: nan\n",
      "Iteration: 1129 Loss: nan\n",
      "Iteration: 1130 Loss: nan\n",
      "Iteration: 1131 Loss: nan\n",
      "Iteration: 1132 Loss: nan\n",
      "Iteration: 1133 Loss: nan\n",
      "Iteration: 1134 Loss: nan\n",
      "Iteration: 1135 Loss: nan\n",
      "Iteration: 1136 Loss: nan\n",
      "Iteration: 1137 Loss: nan\n",
      "Iteration: 1138 Loss: nan\n",
      "Iteration: 1139 Loss: nan\n",
      "Iteration: 1140 Loss: nan\n",
      "Iteration: 1141 Loss: nan\n",
      "Iteration: 1142 Loss: nan\n",
      "Iteration: 1143 Loss: nan\n",
      "Iteration: 1144 Loss: nan\n",
      "Iteration: 1145 Loss: nan\n",
      "Iteration: 1146 Loss: nan\n",
      "Iteration: 1147 Loss: nan\n",
      "Iteration: 1148 Loss: nan\n",
      "Iteration: 1149 Loss: nan\n",
      "Iteration: 1150 Loss: nan\n",
      "Iteration: 1151 Loss: nan\n",
      "Iteration: 1152 Loss: nan\n",
      "Iteration: 1153 Loss: nan\n",
      "Iteration: 1154 Loss: nan\n",
      "Iteration: 1155 Loss: nan\n",
      "Iteration: 1156 Loss: nan\n",
      "Iteration: 1157 Loss: nan\n",
      "Iteration: 1158 Loss: nan\n",
      "Iteration: 1159 Loss: nan\n",
      "Iteration: 1160 Loss: nan\n",
      "Iteration: 1161 Loss: nan\n",
      "Iteration: 1162 Loss: nan\n",
      "Iteration: 1163 Loss: nan\n",
      "Iteration: 1164 Loss: nan\n",
      "Iteration: 1165 Loss: nan\n",
      "Iteration: 1166 Loss: nan\n",
      "Iteration: 1167 Loss: nan\n",
      "Iteration: 1168 Loss: nan\n",
      "Iteration: 1169 Loss: nan\n",
      "Iteration: 1170 Loss: nan\n",
      "Iteration: 1171 Loss: nan\n",
      "Iteration: 1172 Loss: nan\n",
      "Iteration: 1173 Loss: nan\n",
      "Iteration: 1174 Loss: nan\n",
      "Iteration: 1175 Loss: nan\n",
      "Iteration: 1176 Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1177 Loss: nan\n",
      "Iteration: 1178 Loss: nan\n",
      "Iteration: 1179 Loss: nan\n",
      "Iteration: 1180 Loss: nan\n",
      "Iteration: 1181 Loss: nan\n",
      "Iteration: 1182 Loss: nan\n",
      "Iteration: 1183 Loss: nan\n",
      "Iteration: 1184 Loss: nan\n",
      "Iteration: 1185 Loss: nan\n",
      "Iteration: 1186 Loss: nan\n",
      "Iteration: 1187 Loss: nan\n",
      "Iteration: 1188 Loss: nan\n",
      "Iteration: 1189 Loss: nan\n",
      "Iteration: 1190 Loss: nan\n",
      "Iteration: 1191 Loss: nan\n",
      "Iteration: 1192 Loss: nan\n",
      "Iteration: 1193 Loss: nan\n",
      "Iteration: 1194 Loss: nan\n",
      "Iteration: 1195 Loss: nan\n",
      "Iteration: 1196 Loss: nan\n",
      "Iteration: 1197 Loss: nan\n",
      "Iteration: 1198 Loss: nan\n",
      "Iteration: 1199 Loss: nan\n",
      "Iteration: 1200 Loss: nan\n",
      "Iteration: 1201 Loss: nan\n",
      "Iteration: 1202 Loss: nan\n",
      "Iteration: 1203 Loss: nan\n",
      "Iteration: 1204 Loss: nan\n",
      "Iteration: 1205 Loss: nan\n",
      "Iteration: 1206 Loss: nan\n",
      "Iteration: 1207 Loss: nan\n",
      "Iteration: 1208 Loss: nan\n",
      "Iteration: 1209 Loss: nan\n",
      "Iteration: 1210 Loss: nan\n",
      "Iteration: 1211 Loss: nan\n",
      "Iteration: 1212 Loss: nan\n",
      "Iteration: 1213 Loss: nan\n",
      "Iteration: 1214 Loss: nan\n",
      "Iteration: 1215 Loss: nan\n",
      "Iteration: 1216 Loss: nan\n",
      "Iteration: 1217 Loss: nan\n",
      "Iteration: 1218 Loss: nan\n",
      "Iteration: 1219 Loss: nan\n",
      "Iteration: 1220 Loss: nan\n",
      "Iteration: 1221 Loss: nan\n",
      "Iteration: 1222 Loss: nan\n",
      "Iteration: 1223 Loss: nan\n",
      "Iteration: 1224 Loss: nan\n",
      "Iteration: 1225 Loss: nan\n",
      "Iteration: 1226 Loss: nan\n",
      "Iteration: 1227 Loss: nan\n",
      "Iteration: 1228 Loss: nan\n",
      "Iteration: 1229 Loss: nan\n",
      "Iteration: 1230 Loss: nan\n",
      "Iteration: 1231 Loss: nan\n",
      "Iteration: 1232 Loss: nan\n",
      "Iteration: 1233 Loss: nan\n",
      "Iteration: 1234 Loss: nan\n",
      "Iteration: 1235 Loss: nan\n",
      "Iteration: 1236 Loss: nan\n",
      "Iteration: 1237 Loss: nan\n",
      "Iteration: 1238 Loss: nan\n",
      "Iteration: 1239 Loss: nan\n",
      "Iteration: 1240 Loss: nan\n",
      "Iteration: 1241 Loss: nan\n",
      "Iteration: 1242 Loss: nan\n",
      "Iteration: 1243 Loss: nan\n",
      "Iteration: 1244 Loss: nan\n",
      "Iteration: 1245 Loss: nan\n",
      "Iteration: 1246 Loss: nan\n",
      "Iteration: 1247 Loss: nan\n",
      "Iteration: 1248 Loss: nan\n",
      "Iteration: 1249 Loss: nan\n",
      "Iteration: 1250 Loss: nan\n",
      "Iteration: 1251 Loss: nan\n",
      "Iteration: 1252 Loss: nan\n",
      "Iteration: 1253 Loss: nan\n",
      "Iteration: 1254 Loss: nan\n",
      "Iteration: 1255 Loss: nan\n",
      "Iteration: 1256 Loss: nan\n",
      "Iteration: 1257 Loss: nan\n",
      "Iteration: 1258 Loss: nan\n",
      "Iteration: 1259 Loss: nan\n",
      "Iteration: 1260 Loss: nan\n",
      "Iteration: 1261 Loss: nan\n",
      "Iteration: 1262 Loss: nan\n",
      "Iteration: 1263 Loss: nan\n",
      "Iteration: 1264 Loss: nan\n",
      "Iteration: 1265 Loss: nan\n",
      "Iteration: 1266 Loss: nan\n",
      "Iteration: 1267 Loss: nan\n",
      "Iteration: 1268 Loss: nan\n",
      "Iteration: 1269 Loss: nan\n",
      "Iteration: 1270 Loss: nan\n",
      "Iteration: 1271 Loss: nan\n",
      "Iteration: 1272 Loss: nan\n",
      "Iteration: 1273 Loss: nan\n",
      "Iteration: 1274 Loss: nan\n",
      "Iteration: 1275 Loss: nan\n",
      "Iteration: 1276 Loss: nan\n",
      "Iteration: 1277 Loss: nan\n",
      "Iteration: 1278 Loss: nan\n",
      "Iteration: 1279 Loss: nan\n",
      "Iteration: 1280 Loss: nan\n",
      "Iteration: 1281 Loss: nan\n",
      "Iteration: 1282 Loss: nan\n",
      "Iteration: 1283 Loss: nan\n",
      "Iteration: 1284 Loss: nan\n",
      "Iteration: 1285 Loss: nan\n",
      "Iteration: 1286 Loss: nan\n",
      "Iteration: 1287 Loss: nan\n",
      "Iteration: 1288 Loss: nan\n",
      "Iteration: 1289 Loss: nan\n",
      "Iteration: 1290 Loss: nan\n",
      "Iteration: 1291 Loss: nan\n",
      "Iteration: 1292 Loss: nan\n",
      "Iteration: 1293 Loss: nan\n",
      "Iteration: 1294 Loss: nan\n",
      "Iteration: 1295 Loss: nan\n",
      "Iteration: 1296 Loss: nan\n",
      "Iteration: 1297 Loss: nan\n",
      "Iteration: 1298 Loss: nan\n",
      "Iteration: 1299 Loss: nan\n",
      "Iteration: 1300 Loss: nan\n",
      "Iteration: 1301 Loss: nan\n",
      "Iteration: 1302 Loss: nan\n",
      "Iteration: 1303 Loss: nan\n",
      "Iteration: 1304 Loss: nan\n",
      "Iteration: 1305 Loss: nan\n",
      "Iteration: 1306 Loss: nan\n",
      "Iteration: 1307 Loss: nan\n",
      "Iteration: 1308 Loss: nan\n",
      "Iteration: 1309 Loss: nan\n",
      "Iteration: 1310 Loss: nan\n",
      "Iteration: 1311 Loss: nan\n",
      "Iteration: 1312 Loss: nan\n",
      "Iteration: 1313 Loss: nan\n",
      "Iteration: 1314 Loss: nan\n",
      "Iteration: 1315 Loss: nan\n",
      "Iteration: 1316 Loss: nan\n",
      "Iteration: 1317 Loss: nan\n",
      "Iteration: 1318 Loss: nan\n",
      "Iteration: 1319 Loss: nan\n",
      "Iteration: 1320 Loss: nan\n",
      "Iteration: 1321 Loss: nan\n",
      "Iteration: 1322 Loss: nan\n",
      "Iteration: 1323 Loss: nan\n",
      "Iteration: 1324 Loss: nan\n",
      "Iteration: 1325 Loss: nan\n",
      "Iteration: 1326 Loss: nan\n",
      "Iteration: 1327 Loss: nan\n",
      "Iteration: 1328 Loss: nan\n",
      "Iteration: 1329 Loss: nan\n",
      "Iteration: 1330 Loss: nan\n",
      "Iteration: 1331 Loss: nan\n",
      "Iteration: 1332 Loss: nan\n",
      "Iteration: 1333 Loss: nan\n",
      "Iteration: 1334 Loss: nan\n",
      "Iteration: 1335 Loss: nan\n",
      "Iteration: 1336 Loss: nan\n",
      "Iteration: 1337 Loss: nan\n",
      "Iteration: 1338 Loss: nan\n",
      "Iteration: 1339 Loss: nan\n",
      "Iteration: 1340 Loss: nan\n",
      "Iteration: 1341 Loss: nan\n",
      "Iteration: 1342 Loss: nan\n",
      "Iteration: 1343 Loss: nan\n",
      "Iteration: 1344 Loss: nan\n",
      "Iteration: 1345 Loss: nan\n",
      "Iteration: 1346 Loss: nan\n",
      "Iteration: 1347 Loss: nan\n",
      "Iteration: 1348 Loss: nan\n",
      "Iteration: 1349 Loss: nan\n",
      "Iteration: 1350 Loss: nan\n",
      "Iteration: 1351 Loss: nan\n",
      "Iteration: 1352 Loss: nan\n",
      "Iteration: 1353 Loss: nan\n",
      "Iteration: 1354 Loss: nan\n",
      "Iteration: 1355 Loss: nan\n",
      "Iteration: 1356 Loss: nan\n",
      "Iteration: 1357 Loss: nan\n",
      "Iteration: 1358 Loss: nan\n",
      "Iteration: 1359 Loss: nan\n",
      "Iteration: 1360 Loss: nan\n",
      "Iteration: 1361 Loss: nan\n",
      "Iteration: 1362 Loss: nan\n",
      "Iteration: 1363 Loss: nan\n",
      "Iteration: 1364 Loss: nan\n",
      "Iteration: 1365 Loss: nan\n",
      "Iteration: 1366 Loss: nan\n",
      "Iteration: 1367 Loss: nan\n",
      "Iteration: 1368 Loss: nan\n",
      "Iteration: 1369 Loss: nan\n",
      "Iteration: 1370 Loss: nan\n",
      "Iteration: 1371 Loss: nan\n",
      "Iteration: 1372 Loss: nan\n",
      "Iteration: 1373 Loss: nan\n",
      "Iteration: 1374 Loss: nan\n",
      "Iteration: 1375 Loss: nan\n",
      "Iteration: 1376 Loss: nan\n",
      "Iteration: 1377 Loss: nan\n",
      "Iteration: 1378 Loss: nan\n",
      "Iteration: 1379 Loss: nan\n",
      "Iteration: 1380 Loss: nan\n",
      "Iteration: 1381 Loss: nan\n",
      "Iteration: 1382 Loss: nan\n",
      "Iteration: 1383 Loss: nan\n",
      "Iteration: 1384 Loss: nan\n",
      "Iteration: 1385 Loss: nan\n",
      "Iteration: 1386 Loss: nan\n",
      "Iteration: 1387 Loss: nan\n",
      "Iteration: 1388 Loss: nan\n",
      "Iteration: 1389 Loss: nan\n",
      "Iteration: 1390 Loss: nan\n",
      "Iteration: 1391 Loss: nan\n",
      "Iteration: 1392 Loss: nan\n",
      "Iteration: 1393 Loss: nan\n",
      "Iteration: 1394 Loss: nan\n",
      "Iteration: 1395 Loss: nan\n",
      "Iteration: 1396 Loss: nan\n",
      "Iteration: 1397 Loss: nan\n",
      "Iteration: 1398 Loss: nan\n",
      "Iteration: 1399 Loss: nan\n",
      "Iteration: 1400 Loss: nan\n",
      "Iteration: 1401 Loss: nan\n",
      "Iteration: 1402 Loss: nan\n",
      "Iteration: 1403 Loss: nan\n",
      "Iteration: 1404 Loss: nan\n",
      "Iteration: 1405 Loss: nan\n",
      "Iteration: 1406 Loss: nan\n",
      "Iteration: 1407 Loss: nan\n",
      "Iteration: 1408 Loss: nan\n",
      "Iteration: 1409 Loss: nan\n",
      "Iteration: 1410 Loss: nan\n",
      "Iteration: 1411 Loss: nan\n",
      "Iteration: 1412 Loss: nan\n",
      "Iteration: 1413 Loss: nan\n",
      "Iteration: 1414 Loss: nan\n",
      "Iteration: 1415 Loss: nan\n",
      "Iteration: 1416 Loss: nan\n",
      "Iteration: 1417 Loss: nan\n",
      "Iteration: 1418 Loss: nan\n",
      "Iteration: 1419 Loss: nan\n",
      "Iteration: 1420 Loss: nan\n",
      "Iteration: 1421 Loss: nan\n",
      "Iteration: 1422 Loss: nan\n",
      "Iteration: 1423 Loss: nan\n",
      "Iteration: 1424 Loss: nan\n",
      "Iteration: 1425 Loss: nan\n",
      "Iteration: 1426 Loss: nan\n",
      "Iteration: 1427 Loss: nan\n",
      "Iteration: 1428 Loss: nan\n",
      "Iteration: 1429 Loss: nan\n",
      "Iteration: 1430 Loss: nan\n",
      "Iteration: 1431 Loss: nan\n",
      "Iteration: 1432 Loss: nan\n",
      "Iteration: 1433 Loss: nan\n",
      "Iteration: 1434 Loss: nan\n",
      "Iteration: 1435 Loss: nan\n",
      "Iteration: 1436 Loss: nan\n",
      "Iteration: 1437 Loss: nan\n",
      "Iteration: 1438 Loss: nan\n",
      "Iteration: 1439 Loss: nan\n",
      "Iteration: 1440 Loss: nan\n",
      "Iteration: 1441 Loss: nan\n",
      "Iteration: 1442 Loss: nan\n",
      "Iteration: 1443 Loss: nan\n",
      "Iteration: 1444 Loss: nan\n",
      "Iteration: 1445 Loss: nan\n",
      "Iteration: 1446 Loss: nan\n",
      "Iteration: 1447 Loss: nan\n",
      "Iteration: 1448 Loss: nan\n",
      "Iteration: 1449 Loss: nan\n",
      "Iteration: 1450 Loss: nan\n",
      "Iteration: 1451 Loss: nan\n",
      "Iteration: 1452 Loss: nan\n",
      "Iteration: 1453 Loss: nan\n",
      "Iteration: 1454 Loss: nan\n",
      "Iteration: 1455 Loss: nan\n",
      "Iteration: 1456 Loss: nan\n",
      "Iteration: 1457 Loss: nan\n",
      "Iteration: 1458 Loss: nan\n",
      "Iteration: 1459 Loss: nan\n",
      "Iteration: 1460 Loss: nan\n",
      "Iteration: 1461 Loss: nan\n",
      "Iteration: 1462 Loss: nan\n",
      "Iteration: 1463 Loss: nan\n",
      "Iteration: 1464 Loss: nan\n",
      "Iteration: 1465 Loss: nan\n",
      "Iteration: 1466 Loss: nan\n",
      "Iteration: 1467 Loss: nan\n",
      "Iteration: 1468 Loss: nan\n",
      "Iteration: 1469 Loss: nan\n",
      "Iteration: 1470 Loss: nan\n",
      "Iteration: 1471 Loss: nan\n",
      "Iteration: 1472 Loss: nan\n",
      "Iteration: 1473 Loss: nan\n",
      "Iteration: 1474 Loss: nan\n",
      "Iteration: 1475 Loss: nan\n",
      "Iteration: 1476 Loss: nan\n",
      "Iteration: 1477 Loss: nan\n",
      "Iteration: 1478 Loss: nan\n",
      "Iteration: 1479 Loss: nan\n",
      "Iteration: 1480 Loss: nan\n",
      "Iteration: 1481 Loss: nan\n",
      "Iteration: 1482 Loss: nan\n",
      "Iteration: 1483 Loss: nan\n",
      "Iteration: 1484 Loss: nan\n",
      "Iteration: 1485 Loss: nan\n",
      "Iteration: 1486 Loss: nan\n",
      "Iteration: 1487 Loss: nan\n",
      "Iteration: 1488 Loss: nan\n",
      "Iteration: 1489 Loss: nan\n",
      "Iteration: 1490 Loss: nan\n",
      "Iteration: 1491 Loss: nan\n",
      "Iteration: 1492 Loss: nan\n",
      "Iteration: 1493 Loss: nan\n",
      "Iteration: 1494 Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1495 Loss: nan\n",
      "Iteration: 1496 Loss: nan\n",
      "Iteration: 1497 Loss: nan\n",
      "Iteration: 1498 Loss: nan\n",
      "Iteration: 1499 Loss: nan\n",
      "Iteration: 1500 Loss: nan\n",
      "Iteration: 1501 Loss: nan\n",
      "Iteration: 1502 Loss: nan\n",
      "Iteration: 1503 Loss: nan\n",
      "Iteration: 1504 Loss: nan\n",
      "Iteration: 1505 Loss: nan\n",
      "Iteration: 1506 Loss: nan\n",
      "Iteration: 1507 Loss: nan\n",
      "Iteration: 1508 Loss: nan\n",
      "Iteration: 1509 Loss: nan\n",
      "Iteration: 1510 Loss: nan\n",
      "Iteration: 1511 Loss: nan\n",
      "Iteration: 1512 Loss: nan\n",
      "Iteration: 1513 Loss: nan\n",
      "Iteration: 1514 Loss: nan\n",
      "Iteration: 1515 Loss: nan\n",
      "Iteration: 1516 Loss: nan\n",
      "Iteration: 1517 Loss: nan\n",
      "Iteration: 1518 Loss: nan\n",
      "Iteration: 1519 Loss: nan\n",
      "Iteration: 1520 Loss: nan\n",
      "Iteration: 1521 Loss: nan\n",
      "Iteration: 1522 Loss: nan\n",
      "Iteration: 1523 Loss: nan\n",
      "Iteration: 1524 Loss: nan\n",
      "Iteration: 1525 Loss: nan\n",
      "Iteration: 1526 Loss: nan\n",
      "Iteration: 1527 Loss: nan\n",
      "Iteration: 1528 Loss: nan\n",
      "Iteration: 1529 Loss: nan\n",
      "Iteration: 1530 Loss: nan\n",
      "Iteration: 1531 Loss: nan\n",
      "Iteration: 1532 Loss: nan\n",
      "Iteration: 1533 Loss: nan\n",
      "Iteration: 1534 Loss: nan\n",
      "Iteration: 1535 Loss: nan\n",
      "Iteration: 1536 Loss: nan\n",
      "Iteration: 1537 Loss: nan\n",
      "Iteration: 1538 Loss: nan\n",
      "Iteration: 1539 Loss: nan\n",
      "Iteration: 1540 Loss: nan\n",
      "Iteration: 1541 Loss: nan\n",
      "Iteration: 1542 Loss: nan\n",
      "Iteration: 1543 Loss: nan\n",
      "Iteration: 1544 Loss: nan\n",
      "Iteration: 1545 Loss: nan\n",
      "Iteration: 1546 Loss: nan\n",
      "Iteration: 1547 Loss: nan\n",
      "Iteration: 1548 Loss: nan\n",
      "Iteration: 1549 Loss: nan\n",
      "Iteration: 1550 Loss: nan\n",
      "Iteration: 1551 Loss: nan\n",
      "Iteration: 1552 Loss: nan\n",
      "Iteration: 1553 Loss: nan\n",
      "Iteration: 1554 Loss: nan\n",
      "Iteration: 1555 Loss: nan\n",
      "Iteration: 1556 Loss: nan\n",
      "Iteration: 1557 Loss: nan\n",
      "Iteration: 1558 Loss: nan\n",
      "Iteration: 1559 Loss: nan\n",
      "Iteration: 1560 Loss: nan\n",
      "Iteration: 1561 Loss: nan\n",
      "Iteration: 1562 Loss: nan\n",
      "Iteration: 1563 Loss: nan\n",
      "Iteration: 1564 Loss: nan\n",
      "Iteration: 1565 Loss: nan\n",
      "Iteration: 1566 Loss: nan\n",
      "Iteration: 1567 Loss: nan\n",
      "Iteration: 1568 Loss: nan\n",
      "Iteration: 1569 Loss: nan\n",
      "Iteration: 1570 Loss: nan\n",
      "Iteration: 1571 Loss: nan\n",
      "Iteration: 1572 Loss: nan\n",
      "Iteration: 1573 Loss: nan\n",
      "Iteration: 1574 Loss: nan\n",
      "Iteration: 1575 Loss: nan\n",
      "Iteration: 1576 Loss: nan\n",
      "Iteration: 1577 Loss: nan\n",
      "Iteration: 1578 Loss: nan\n",
      "Iteration: 1579 Loss: nan\n",
      "Iteration: 1580 Loss: nan\n",
      "Iteration: 1581 Loss: nan\n",
      "Iteration: 1582 Loss: nan\n",
      "Iteration: 1583 Loss: nan\n",
      "Iteration: 1584 Loss: nan\n",
      "Iteration: 1585 Loss: nan\n",
      "Iteration: 1586 Loss: nan\n",
      "Iteration: 1587 Loss: nan\n",
      "Iteration: 1588 Loss: nan\n",
      "Iteration: 1589 Loss: nan\n",
      "Iteration: 1590 Loss: nan\n",
      "Iteration: 1591 Loss: nan\n",
      "Iteration: 1592 Loss: nan\n",
      "Iteration: 1593 Loss: nan\n",
      "Iteration: 1594 Loss: nan\n",
      "Iteration: 1595 Loss: nan\n",
      "Iteration: 1596 Loss: nan\n",
      "Iteration: 1597 Loss: nan\n",
      "Iteration: 1598 Loss: nan\n",
      "Iteration: 1599 Loss: nan\n",
      "Iteration: 1600 Loss: nan\n",
      "Iteration: 1601 Loss: nan\n",
      "Iteration: 1602 Loss: nan\n",
      "Iteration: 1603 Loss: nan\n",
      "Iteration: 1604 Loss: nan\n",
      "Iteration: 1605 Loss: nan\n",
      "Iteration: 1606 Loss: nan\n",
      "Iteration: 1607 Loss: nan\n",
      "Iteration: 1608 Loss: nan\n",
      "Iteration: 1609 Loss: nan\n",
      "Iteration: 1610 Loss: nan\n",
      "Iteration: 1611 Loss: nan\n",
      "Iteration: 1612 Loss: nan\n",
      "Iteration: 1613 Loss: nan\n",
      "Iteration: 1614 Loss: nan\n",
      "Iteration: 1615 Loss: nan\n",
      "Iteration: 1616 Loss: nan\n",
      "Iteration: 1617 Loss: nan\n",
      "Iteration: 1618 Loss: nan\n",
      "Iteration: 1619 Loss: nan\n",
      "Iteration: 1620 Loss: nan\n",
      "Iteration: 1621 Loss: nan\n",
      "Iteration: 1622 Loss: nan\n",
      "Iteration: 1623 Loss: nan\n",
      "Iteration: 1624 Loss: nan\n",
      "Iteration: 1625 Loss: nan\n",
      "Iteration: 1626 Loss: nan\n",
      "Iteration: 1627 Loss: nan\n",
      "Iteration: 1628 Loss: nan\n",
      "Iteration: 1629 Loss: nan\n",
      "Iteration: 1630 Loss: nan\n",
      "Iteration: 1631 Loss: nan\n",
      "Iteration: 1632 Loss: nan\n",
      "Iteration: 1633 Loss: nan\n",
      "Iteration: 1634 Loss: nan\n",
      "Iteration: 1635 Loss: nan\n",
      "Iteration: 1636 Loss: nan\n",
      "Iteration: 1637 Loss: nan\n",
      "Iteration: 1638 Loss: nan\n",
      "Iteration: 1639 Loss: nan\n",
      "Iteration: 1640 Loss: nan\n",
      "Iteration: 1641 Loss: nan\n",
      "Iteration: 1642 Loss: nan\n",
      "Iteration: 1643 Loss: nan\n",
      "Iteration: 1644 Loss: nan\n",
      "Iteration: 1645 Loss: nan\n",
      "Iteration: 1646 Loss: nan\n",
      "Iteration: 1647 Loss: nan\n",
      "Iteration: 1648 Loss: nan\n",
      "Iteration: 1649 Loss: nan\n",
      "Iteration: 1650 Loss: nan\n",
      "Iteration: 1651 Loss: nan\n",
      "Iteration: 1652 Loss: nan\n",
      "Iteration: 1653 Loss: nan\n",
      "Iteration: 1654 Loss: nan\n",
      "Iteration: 1655 Loss: nan\n",
      "Iteration: 1656 Loss: nan\n",
      "Iteration: 1657 Loss: nan\n",
      "Iteration: 1658 Loss: nan\n",
      "Iteration: 1659 Loss: nan\n",
      "Iteration: 1660 Loss: nan\n",
      "Iteration: 1661 Loss: nan\n",
      "Iteration: 1662 Loss: nan\n",
      "Iteration: 1663 Loss: nan\n",
      "Iteration: 1664 Loss: nan\n",
      "Iteration: 1665 Loss: nan\n",
      "Iteration: 1666 Loss: nan\n",
      "Iteration: 1667 Loss: nan\n",
      "Iteration: 1668 Loss: nan\n",
      "Iteration: 1669 Loss: nan\n",
      "Iteration: 1670 Loss: nan\n",
      "Iteration: 1671 Loss: nan\n",
      "Iteration: 1672 Loss: nan\n",
      "Iteration: 1673 Loss: nan\n",
      "Iteration: 1674 Loss: nan\n",
      "Iteration: 1675 Loss: nan\n",
      "Iteration: 1676 Loss: nan\n",
      "Iteration: 1677 Loss: nan\n",
      "Iteration: 1678 Loss: nan\n",
      "Iteration: 1679 Loss: nan\n",
      "Iteration: 1680 Loss: nan\n",
      "Iteration: 1681 Loss: nan\n",
      "Iteration: 1682 Loss: nan\n",
      "Iteration: 1683 Loss: nan\n",
      "Iteration: 1684 Loss: nan\n",
      "Iteration: 1685 Loss: nan\n",
      "Iteration: 1686 Loss: nan\n",
      "Iteration: 1687 Loss: nan\n",
      "Iteration: 1688 Loss: nan\n",
      "Iteration: 1689 Loss: nan\n",
      "Iteration: 1690 Loss: nan\n",
      "Iteration: 1691 Loss: nan\n",
      "Iteration: 1692 Loss: nan\n",
      "Iteration: 1693 Loss: nan\n",
      "Iteration: 1694 Loss: nan\n",
      "Iteration: 1695 Loss: nan\n",
      "Iteration: 1696 Loss: nan\n",
      "Iteration: 1697 Loss: nan\n",
      "Iteration: 1698 Loss: nan\n",
      "Iteration: 1699 Loss: nan\n",
      "Iteration: 1700 Loss: nan\n",
      "Iteration: 1701 Loss: nan\n",
      "Iteration: 1702 Loss: nan\n",
      "Iteration: 1703 Loss: nan\n",
      "Iteration: 1704 Loss: nan\n",
      "Iteration: 1705 Loss: nan\n",
      "Iteration: 1706 Loss: nan\n",
      "Iteration: 1707 Loss: nan\n",
      "Iteration: 1708 Loss: nan\n",
      "Iteration: 1709 Loss: nan\n",
      "Iteration: 1710 Loss: nan\n",
      "Iteration: 1711 Loss: nan\n",
      "Iteration: 1712 Loss: nan\n",
      "Iteration: 1713 Loss: nan\n",
      "Iteration: 1714 Loss: nan\n",
      "Iteration: 1715 Loss: nan\n",
      "Iteration: 1716 Loss: nan\n",
      "Iteration: 1717 Loss: nan\n",
      "Iteration: 1718 Loss: nan\n",
      "Iteration: 1719 Loss: nan\n",
      "Iteration: 1720 Loss: nan\n",
      "Iteration: 1721 Loss: nan\n",
      "Iteration: 1722 Loss: nan\n",
      "Iteration: 1723 Loss: nan\n",
      "Iteration: 1724 Loss: nan\n",
      "Iteration: 1725 Loss: nan\n",
      "Iteration: 1726 Loss: nan\n",
      "Iteration: 1727 Loss: nan\n",
      "Iteration: 1728 Loss: nan\n",
      "Iteration: 1729 Loss: nan\n",
      "Iteration: 1730 Loss: nan\n",
      "Iteration: 1731 Loss: nan\n",
      "Iteration: 1732 Loss: nan\n",
      "Iteration: 1733 Loss: nan\n",
      "Iteration: 1734 Loss: nan\n",
      "Iteration: 1735 Loss: nan\n",
      "Iteration: 1736 Loss: nan\n",
      "Iteration: 1737 Loss: nan\n",
      "Iteration: 1738 Loss: nan\n",
      "Iteration: 1739 Loss: nan\n",
      "Iteration: 1740 Loss: nan\n",
      "Iteration: 1741 Loss: nan\n",
      "Iteration: 1742 Loss: nan\n",
      "Iteration: 1743 Loss: nan\n",
      "Iteration: 1744 Loss: nan\n",
      "Iteration: 1745 Loss: nan\n",
      "Iteration: 1746 Loss: nan\n",
      "Iteration: 1747 Loss: nan\n",
      "Iteration: 1748 Loss: nan\n",
      "Iteration: 1749 Loss: nan\n",
      "Iteration: 1750 Loss: nan\n",
      "Iteration: 1751 Loss: nan\n",
      "Iteration: 1752 Loss: nan\n",
      "Iteration: 1753 Loss: nan\n",
      "Iteration: 1754 Loss: nan\n",
      "Iteration: 1755 Loss: nan\n",
      "Iteration: 1756 Loss: nan\n",
      "Iteration: 1757 Loss: nan\n",
      "Iteration: 1758 Loss: nan\n",
      "Iteration: 1759 Loss: nan\n",
      "Iteration: 1760 Loss: nan\n",
      "Iteration: 1761 Loss: nan\n",
      "Iteration: 1762 Loss: nan\n",
      "Iteration: 1763 Loss: nan\n",
      "Iteration: 1764 Loss: nan\n",
      "Iteration: 1765 Loss: nan\n",
      "Iteration: 1766 Loss: nan\n",
      "Iteration: 1767 Loss: nan\n",
      "Iteration: 1768 Loss: nan\n",
      "Iteration: 1769 Loss: nan\n",
      "Iteration: 1770 Loss: nan\n",
      "Iteration: 1771 Loss: nan\n",
      "Iteration: 1772 Loss: nan\n",
      "Iteration: 1773 Loss: nan\n",
      "Iteration: 1774 Loss: nan\n",
      "Iteration: 1775 Loss: nan\n",
      "Iteration: 1776 Loss: nan\n",
      "Iteration: 1777 Loss: nan\n",
      "Iteration: 1778 Loss: nan\n",
      "Iteration: 1779 Loss: nan\n",
      "Iteration: 1780 Loss: nan\n",
      "Iteration: 1781 Loss: nan\n",
      "Iteration: 1782 Loss: nan\n",
      "Iteration: 1783 Loss: nan\n",
      "Iteration: 1784 Loss: nan\n",
      "Iteration: 1785 Loss: nan\n",
      "Iteration: 1786 Loss: nan\n",
      "Iteration: 1787 Loss: nan\n",
      "Iteration: 1788 Loss: nan\n",
      "Iteration: 1789 Loss: nan\n",
      "Iteration: 1790 Loss: nan\n",
      "Iteration: 1791 Loss: nan\n",
      "Iteration: 1792 Loss: nan\n",
      "Iteration: 1793 Loss: nan\n",
      "Iteration: 1794 Loss: nan\n",
      "Iteration: 1795 Loss: nan\n",
      "Iteration: 1796 Loss: nan\n",
      "Iteration: 1797 Loss: nan\n",
      "Iteration: 1798 Loss: nan\n",
      "Iteration: 1799 Loss: nan\n",
      "Iteration: 1800 Loss: nan\n",
      "Iteration: 1801 Loss: nan\n",
      "Iteration: 1802 Loss: nan\n",
      "Iteration: 1803 Loss: nan\n",
      "Iteration: 1804 Loss: nan\n",
      "Iteration: 1805 Loss: nan\n",
      "Iteration: 1806 Loss: nan\n",
      "Iteration: 1807 Loss: nan\n",
      "Iteration: 1808 Loss: nan\n",
      "Iteration: 1809 Loss: nan\n",
      "Iteration: 1810 Loss: nan\n",
      "Iteration: 1811 Loss: nan\n",
      "Iteration: 1812 Loss: nan\n",
      "Iteration: 1813 Loss: nan\n",
      "Iteration: 1814 Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1815 Loss: nan\n",
      "Iteration: 1816 Loss: nan\n",
      "Iteration: 1817 Loss: nan\n",
      "Iteration: 1818 Loss: nan\n",
      "Iteration: 1819 Loss: nan\n",
      "Iteration: 1820 Loss: nan\n",
      "Iteration: 1821 Loss: nan\n",
      "Iteration: 1822 Loss: nan\n",
      "Iteration: 1823 Loss: nan\n",
      "Iteration: 1824 Loss: nan\n",
      "Iteration: 1825 Loss: nan\n",
      "Iteration: 1826 Loss: nan\n",
      "Iteration: 1827 Loss: nan\n",
      "Iteration: 1828 Loss: nan\n",
      "Iteration: 1829 Loss: nan\n",
      "Iteration: 1830 Loss: nan\n",
      "Iteration: 1831 Loss: nan\n",
      "Iteration: 1832 Loss: nan\n",
      "Iteration: 1833 Loss: nan\n",
      "Iteration: 1834 Loss: nan\n",
      "Iteration: 1835 Loss: nan\n",
      "Iteration: 1836 Loss: nan\n",
      "Iteration: 1837 Loss: nan\n",
      "Iteration: 1838 Loss: nan\n",
      "Iteration: 1839 Loss: nan\n",
      "Iteration: 1840 Loss: nan\n",
      "Iteration: 1841 Loss: nan\n",
      "Iteration: 1842 Loss: nan\n",
      "Iteration: 1843 Loss: nan\n",
      "Iteration: 1844 Loss: nan\n",
      "Iteration: 1845 Loss: nan\n",
      "Iteration: 1846 Loss: nan\n",
      "Iteration: 1847 Loss: nan\n",
      "Iteration: 1848 Loss: nan\n",
      "Iteration: 1849 Loss: nan\n",
      "Iteration: 1850 Loss: nan\n",
      "Iteration: 1851 Loss: nan\n",
      "Iteration: 1852 Loss: nan\n",
      "Iteration: 1853 Loss: nan\n",
      "Iteration: 1854 Loss: nan\n",
      "Iteration: 1855 Loss: nan\n",
      "Iteration: 1856 Loss: nan\n",
      "Iteration: 1857 Loss: nan\n",
      "Iteration: 1858 Loss: nan\n",
      "Iteration: 1859 Loss: nan\n",
      "Iteration: 1860 Loss: nan\n",
      "Iteration: 1861 Loss: nan\n",
      "Iteration: 1862 Loss: nan\n",
      "Iteration: 1863 Loss: nan\n",
      "Iteration: 1864 Loss: nan\n",
      "Iteration: 1865 Loss: nan\n",
      "Iteration: 1866 Loss: nan\n",
      "Iteration: 1867 Loss: nan\n",
      "Iteration: 1868 Loss: nan\n",
      "Iteration: 1869 Loss: nan\n",
      "Iteration: 1870 Loss: nan\n",
      "Iteration: 1871 Loss: nan\n",
      "Iteration: 1872 Loss: nan\n",
      "Iteration: 1873 Loss: nan\n",
      "Iteration: 1874 Loss: nan\n",
      "Iteration: 1875 Loss: nan\n",
      "Iteration: 1876 Loss: nan\n",
      "Iteration: 1877 Loss: nan\n",
      "Iteration: 1878 Loss: nan\n",
      "Iteration: 1879 Loss: nan\n",
      "Iteration: 1880 Loss: nan\n",
      "Iteration: 1881 Loss: nan\n",
      "Iteration: 1882 Loss: nan\n",
      "Iteration: 1883 Loss: nan\n",
      "Iteration: 1884 Loss: nan\n",
      "Iteration: 1885 Loss: nan\n",
      "Iteration: 1886 Loss: nan\n",
      "Iteration: 1887 Loss: nan\n",
      "Iteration: 1888 Loss: nan\n",
      "Iteration: 1889 Loss: nan\n",
      "Iteration: 1890 Loss: nan\n",
      "Iteration: 1891 Loss: nan\n",
      "Iteration: 1892 Loss: nan\n",
      "Iteration: 1893 Loss: nan\n",
      "Iteration: 1894 Loss: nan\n",
      "Iteration: 1895 Loss: nan\n",
      "Iteration: 1896 Loss: nan\n",
      "Iteration: 1897 Loss: nan\n",
      "Iteration: 1898 Loss: nan\n",
      "Iteration: 1899 Loss: nan\n",
      "Iteration: 1900 Loss: nan\n",
      "Iteration: 1901 Loss: nan\n",
      "Iteration: 1902 Loss: nan\n",
      "Iteration: 1903 Loss: nan\n",
      "Iteration: 1904 Loss: nan\n",
      "Iteration: 1905 Loss: nan\n",
      "Iteration: 1906 Loss: nan\n",
      "Iteration: 1907 Loss: nan\n",
      "Iteration: 1908 Loss: nan\n",
      "Iteration: 1909 Loss: nan\n",
      "Iteration: 1910 Loss: nan\n",
      "Iteration: 1911 Loss: nan\n",
      "Iteration: 1912 Loss: nan\n",
      "Iteration: 1913 Loss: nan\n",
      "Iteration: 1914 Loss: nan\n",
      "Iteration: 1915 Loss: nan\n",
      "Iteration: 1916 Loss: nan\n",
      "Iteration: 1917 Loss: nan\n",
      "Iteration: 1918 Loss: nan\n",
      "Iteration: 1919 Loss: nan\n",
      "Iteration: 1920 Loss: nan\n",
      "Iteration: 1921 Loss: nan\n",
      "Iteration: 1922 Loss: nan\n",
      "Iteration: 1923 Loss: nan\n",
      "Iteration: 1924 Loss: nan\n",
      "Iteration: 1925 Loss: nan\n",
      "Iteration: 1926 Loss: nan\n",
      "Iteration: 1927 Loss: nan\n",
      "Iteration: 1928 Loss: nan\n",
      "Iteration: 1929 Loss: nan\n",
      "Iteration: 1930 Loss: nan\n",
      "Iteration: 1931 Loss: nan\n",
      "Iteration: 1932 Loss: nan\n",
      "Iteration: 1933 Loss: nan\n",
      "Iteration: 1934 Loss: nan\n",
      "Iteration: 1935 Loss: nan\n",
      "Iteration: 1936 Loss: nan\n",
      "Iteration: 1937 Loss: nan\n",
      "Iteration: 1938 Loss: nan\n",
      "Iteration: 1939 Loss: nan\n",
      "Iteration: 1940 Loss: nan\n",
      "Iteration: 1941 Loss: nan\n",
      "Iteration: 1942 Loss: nan\n",
      "Iteration: 1943 Loss: nan\n",
      "Iteration: 1944 Loss: nan\n",
      "Iteration: 1945 Loss: nan\n",
      "Iteration: 1946 Loss: nan\n",
      "Iteration: 1947 Loss: nan\n",
      "Iteration: 1948 Loss: nan\n",
      "Iteration: 1949 Loss: nan\n",
      "Iteration: 1950 Loss: nan\n",
      "Iteration: 1951 Loss: nan\n",
      "Iteration: 1952 Loss: nan\n",
      "Iteration: 1953 Loss: nan\n",
      "Iteration: 1954 Loss: nan\n",
      "Iteration: 1955 Loss: nan\n",
      "Iteration: 1956 Loss: nan\n",
      "Iteration: 1957 Loss: nan\n",
      "Iteration: 1958 Loss: nan\n",
      "Iteration: 1959 Loss: nan\n",
      "Iteration: 1960 Loss: nan\n",
      "Iteration: 1961 Loss: nan\n",
      "Iteration: 1962 Loss: nan\n",
      "Iteration: 1963 Loss: nan\n",
      "Iteration: 1964 Loss: nan\n",
      "Iteration: 1965 Loss: nan\n",
      "Iteration: 1966 Loss: nan\n",
      "Iteration: 1967 Loss: nan\n",
      "Iteration: 1968 Loss: nan\n",
      "Iteration: 1969 Loss: nan\n",
      "Iteration: 1970 Loss: nan\n",
      "Iteration: 1971 Loss: nan\n",
      "Iteration: 1972 Loss: nan\n",
      "Iteration: 1973 Loss: nan\n",
      "Iteration: 1974 Loss: nan\n",
      "Iteration: 1975 Loss: nan\n",
      "Iteration: 1976 Loss: nan\n",
      "Iteration: 1977 Loss: nan\n",
      "Iteration: 1978 Loss: nan\n",
      "Iteration: 1979 Loss: nan\n",
      "Iteration: 1980 Loss: nan\n",
      "Iteration: 1981 Loss: nan\n",
      "Iteration: 1982 Loss: nan\n",
      "Iteration: 1983 Loss: nan\n",
      "Iteration: 1984 Loss: nan\n",
      "Iteration: 1985 Loss: nan\n",
      "Iteration: 1986 Loss: nan\n",
      "Iteration: 1987 Loss: nan\n",
      "Iteration: 1988 Loss: nan\n",
      "Iteration: 1989 Loss: nan\n",
      "Iteration: 1990 Loss: nan\n",
      "Iteration: 1991 Loss: nan\n",
      "Iteration: 1992 Loss: nan\n",
      "Iteration: 1993 Loss: nan\n",
      "Iteration: 1994 Loss: nan\n",
      "Iteration: 1995 Loss: nan\n",
      "Iteration: 1996 Loss: nan\n",
      "Iteration: 1997 Loss: nan\n",
      "Iteration: 1998 Loss: nan\n",
      "Iteration: 1999 Loss: nan\n",
      "Iteration: 2000 Loss: nan\n",
      "Iteration: 2001 Loss: nan\n",
      "Iteration: 2002 Loss: nan\n",
      "Iteration: 2003 Loss: nan\n",
      "Iteration: 2004 Loss: nan\n",
      "Iteration: 2005 Loss: nan\n",
      "Iteration: 2006 Loss: nan\n",
      "Iteration: 2007 Loss: nan\n",
      "Iteration: 2008 Loss: nan\n",
      "Iteration: 2009 Loss: nan\n",
      "Iteration: 2010 Loss: nan\n",
      "Iteration: 2011 Loss: nan\n",
      "Iteration: 2012 Loss: nan\n",
      "Iteration: 2013 Loss: nan\n",
      "Iteration: 2014 Loss: nan\n",
      "Iteration: 2015 Loss: nan\n",
      "Iteration: 2016 Loss: nan\n",
      "Iteration: 2017 Loss: nan\n",
      "Iteration: 2018 Loss: nan\n",
      "Iteration: 2019 Loss: nan\n",
      "Iteration: 2020 Loss: nan\n",
      "Iteration: 2021 Loss: nan\n",
      "Iteration: 2022 Loss: nan\n",
      "Iteration: 2023 Loss: nan\n",
      "Iteration: 2024 Loss: nan\n",
      "Iteration: 2025 Loss: nan\n",
      "Iteration: 2026 Loss: nan\n",
      "Iteration: 2027 Loss: nan\n",
      "Iteration: 2028 Loss: nan\n",
      "Iteration: 2029 Loss: nan\n",
      "Iteration: 2030 Loss: nan\n",
      "Iteration: 2031 Loss: nan\n",
      "Iteration: 2032 Loss: nan\n",
      "Iteration: 2033 Loss: nan\n",
      "Iteration: 2034 Loss: nan\n",
      "Iteration: 2035 Loss: nan\n",
      "Iteration: 2036 Loss: nan\n",
      "Iteration: 2037 Loss: nan\n",
      "Iteration: 2038 Loss: nan\n",
      "Iteration: 2039 Loss: nan\n",
      "Iteration: 2040 Loss: nan\n",
      "Iteration: 2041 Loss: nan\n",
      "Iteration: 2042 Loss: nan\n",
      "Iteration: 2043 Loss: nan\n",
      "Iteration: 2044 Loss: nan\n",
      "Iteration: 2045 Loss: nan\n",
      "Iteration: 2046 Loss: nan\n",
      "Iteration: 2047 Loss: nan\n",
      "Iteration: 2048 Loss: nan\n",
      "Iteration: 2049 Loss: nan\n",
      "Iteration: 2050 Loss: nan\n",
      "Iteration: 2051 Loss: nan\n",
      "Iteration: 2052 Loss: nan\n",
      "Iteration: 2053 Loss: nan\n",
      "Iteration: 2054 Loss: nan\n",
      "Iteration: 2055 Loss: nan\n",
      "Iteration: 2056 Loss: nan\n",
      "Iteration: 2057 Loss: nan\n",
      "Iteration: 2058 Loss: nan\n",
      "Iteration: 2059 Loss: nan\n",
      "Iteration: 2060 Loss: nan\n",
      "Iteration: 2061 Loss: nan\n",
      "Iteration: 2062 Loss: nan\n",
      "Iteration: 2063 Loss: nan\n",
      "Iteration: 2064 Loss: nan\n",
      "Iteration: 2065 Loss: nan\n",
      "Iteration: 2066 Loss: nan\n",
      "Iteration: 2067 Loss: nan\n",
      "Iteration: 2068 Loss: nan\n",
      "Iteration: 2069 Loss: nan\n",
      "Iteration: 2070 Loss: nan\n",
      "Iteration: 2071 Loss: nan\n",
      "Iteration: 2072 Loss: nan\n",
      "Iteration: 2073 Loss: nan\n",
      "Iteration: 2074 Loss: nan\n",
      "Iteration: 2075 Loss: nan\n",
      "Iteration: 2076 Loss: nan\n",
      "Iteration: 2077 Loss: nan\n",
      "Iteration: 2078 Loss: nan\n",
      "Iteration: 2079 Loss: nan\n",
      "Iteration: 2080 Loss: nan\n",
      "Iteration: 2081 Loss: nan\n",
      "Iteration: 2082 Loss: nan\n",
      "Iteration: 2083 Loss: nan\n",
      "Iteration: 2084 Loss: nan\n",
      "Iteration: 2085 Loss: nan\n",
      "Iteration: 2086 Loss: nan\n",
      "Iteration: 2087 Loss: nan\n",
      "Iteration: 2088 Loss: nan\n",
      "Iteration: 2089 Loss: nan\n",
      "Iteration: 2090 Loss: nan\n",
      "Iteration: 2091 Loss: nan\n",
      "Iteration: 2092 Loss: nan\n",
      "Iteration: 2093 Loss: nan\n",
      "Iteration: 2094 Loss: nan\n",
      "Iteration: 2095 Loss: nan\n",
      "Iteration: 2096 Loss: nan\n",
      "Iteration: 2097 Loss: nan\n",
      "Iteration: 2098 Loss: nan\n",
      "Iteration: 2099 Loss: nan\n",
      "Iteration: 2100 Loss: nan\n",
      "Iteration: 2101 Loss: nan\n",
      "Iteration: 2102 Loss: nan\n",
      "Iteration: 2103 Loss: nan\n",
      "Iteration: 2104 Loss: nan\n",
      "Iteration: 2105 Loss: nan\n",
      "Iteration: 2106 Loss: nan\n",
      "Iteration: 2107 Loss: nan\n",
      "Iteration: 2108 Loss: nan\n",
      "Iteration: 2109 Loss: nan\n",
      "Iteration: 2110 Loss: nan\n",
      "Iteration: 2111 Loss: nan\n",
      "Iteration: 2112 Loss: nan\n",
      "Iteration: 2113 Loss: nan\n",
      "Iteration: 2114 Loss: nan\n",
      "Iteration: 2115 Loss: nan\n",
      "Iteration: 2116 Loss: nan\n",
      "Iteration: 2117 Loss: nan\n",
      "Iteration: 2118 Loss: nan\n",
      "Iteration: 2119 Loss: nan\n",
      "Iteration: 2120 Loss: nan\n",
      "Iteration: 2121 Loss: nan\n",
      "Iteration: 2122 Loss: nan\n",
      "Iteration: 2123 Loss: nan\n",
      "Iteration: 2124 Loss: nan\n",
      "Iteration: 2125 Loss: nan\n",
      "Iteration: 2126 Loss: nan\n",
      "Iteration: 2127 Loss: nan\n",
      "Iteration: 2128 Loss: nan\n",
      "Iteration: 2129 Loss: nan\n",
      "Iteration: 2130 Loss: nan\n",
      "Iteration: 2131 Loss: nan\n",
      "Iteration: 2132 Loss: nan\n",
      "Iteration: 2133 Loss: nan\n",
      "Iteration: 2134 Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2135 Loss: nan\n",
      "Iteration: 2136 Loss: nan\n",
      "Iteration: 2137 Loss: nan\n",
      "Iteration: 2138 Loss: nan\n",
      "Iteration: 2139 Loss: nan\n",
      "Iteration: 2140 Loss: nan\n",
      "Iteration: 2141 Loss: nan\n",
      "Iteration: 2142 Loss: nan\n",
      "Iteration: 2143 Loss: nan\n",
      "Iteration: 2144 Loss: nan\n",
      "Iteration: 2145 Loss: nan\n",
      "Iteration: 2146 Loss: nan\n",
      "Iteration: 2147 Loss: nan\n",
      "Iteration: 2148 Loss: nan\n",
      "Iteration: 2149 Loss: nan\n",
      "Iteration: 2150 Loss: nan\n",
      "Iteration: 2151 Loss: nan\n",
      "Iteration: 2152 Loss: nan\n",
      "Iteration: 2153 Loss: nan\n",
      "Iteration: 2154 Loss: nan\n",
      "Iteration: 2155 Loss: nan\n",
      "Iteration: 2156 Loss: nan\n",
      "Iteration: 2157 Loss: nan\n",
      "Iteration: 2158 Loss: nan\n",
      "Iteration: 2159 Loss: nan\n",
      "Iteration: 2160 Loss: nan\n",
      "Iteration: 2161 Loss: nan\n",
      "Iteration: 2162 Loss: nan\n",
      "Iteration: 2163 Loss: nan\n",
      "Iteration: 2164 Loss: nan\n",
      "Iteration: 2165 Loss: nan\n",
      "Iteration: 2166 Loss: nan\n",
      "Iteration: 2167 Loss: nan\n",
      "Iteration: 2168 Loss: nan\n",
      "Iteration: 2169 Loss: nan\n",
      "Iteration: 2170 Loss: nan\n",
      "Iteration: 2171 Loss: nan\n",
      "Iteration: 2172 Loss: nan\n",
      "Iteration: 2173 Loss: nan\n",
      "Iteration: 2174 Loss: nan\n",
      "Iteration: 2175 Loss: nan\n",
      "Iteration: 2176 Loss: nan\n",
      "Iteration: 2177 Loss: nan\n",
      "Iteration: 2178 Loss: nan\n",
      "Iteration: 2179 Loss: nan\n",
      "Iteration: 2180 Loss: nan\n",
      "Iteration: 2181 Loss: nan\n",
      "Iteration: 2182 Loss: nan\n",
      "Iteration: 2183 Loss: nan\n",
      "Iteration: 2184 Loss: nan\n",
      "Iteration: 2185 Loss: nan\n",
      "Iteration: 2186 Loss: nan\n",
      "Iteration: 2187 Loss: nan\n",
      "Iteration: 2188 Loss: nan\n",
      "Iteration: 2189 Loss: nan\n",
      "Iteration: 2190 Loss: nan\n",
      "Iteration: 2191 Loss: nan\n",
      "Iteration: 2192 Loss: nan\n",
      "Iteration: 2193 Loss: nan\n",
      "Iteration: 2194 Loss: nan\n",
      "Iteration: 2195 Loss: nan\n",
      "Iteration: 2196 Loss: nan\n",
      "Iteration: 2197 Loss: nan\n",
      "Iteration: 2198 Loss: nan\n",
      "Iteration: 2199 Loss: nan\n",
      "Iteration: 2200 Loss: nan\n",
      "Iteration: 2201 Loss: nan\n",
      "Iteration: 2202 Loss: nan\n",
      "Iteration: 2203 Loss: nan\n",
      "Iteration: 2204 Loss: nan\n",
      "Iteration: 2205 Loss: nan\n",
      "Iteration: 2206 Loss: nan\n",
      "Iteration: 2207 Loss: nan\n",
      "Iteration: 2208 Loss: nan\n",
      "Iteration: 2209 Loss: nan\n",
      "Iteration: 2210 Loss: nan\n",
      "Iteration: 2211 Loss: nan\n",
      "Iteration: 2212 Loss: nan\n",
      "Iteration: 2213 Loss: nan\n",
      "Iteration: 2214 Loss: nan\n",
      "Iteration: 2215 Loss: nan\n",
      "Iteration: 2216 Loss: nan\n",
      "Iteration: 2217 Loss: nan\n",
      "Iteration: 2218 Loss: nan\n",
      "Iteration: 2219 Loss: nan\n",
      "Iteration: 2220 Loss: nan\n",
      "Iteration: 2221 Loss: nan\n",
      "Iteration: 2222 Loss: nan\n",
      "Iteration: 2223 Loss: nan\n",
      "Iteration: 2224 Loss: nan\n",
      "Iteration: 2225 Loss: nan\n",
      "Iteration: 2226 Loss: nan\n",
      "Iteration: 2227 Loss: nan\n",
      "Iteration: 2228 Loss: nan\n",
      "Iteration: 2229 Loss: nan\n",
      "Iteration: 2230 Loss: nan\n",
      "Iteration: 2231 Loss: nan\n",
      "Iteration: 2232 Loss: nan\n",
      "Iteration: 2233 Loss: nan\n",
      "Iteration: 2234 Loss: nan\n",
      "Iteration: 2235 Loss: nan\n",
      "Iteration: 2236 Loss: nan\n",
      "Iteration: 2237 Loss: nan\n",
      "Iteration: 2238 Loss: nan\n",
      "Iteration: 2239 Loss: nan\n",
      "Iteration: 2240 Loss: nan\n",
      "Iteration: 2241 Loss: nan\n",
      "Iteration: 2242 Loss: nan\n",
      "Iteration: 2243 Loss: nan\n",
      "Iteration: 2244 Loss: nan\n",
      "Iteration: 2245 Loss: nan\n",
      "Iteration: 2246 Loss: nan\n",
      "Iteration: 2247 Loss: nan\n",
      "Iteration: 2248 Loss: nan\n",
      "Iteration: 2249 Loss: nan\n",
      "Iteration: 2250 Loss: nan\n",
      "Iteration: 2251 Loss: nan\n",
      "Iteration: 2252 Loss: nan\n",
      "Iteration: 2253 Loss: nan\n",
      "Iteration: 2254 Loss: nan\n",
      "Iteration: 2255 Loss: nan\n",
      "Iteration: 2256 Loss: nan\n",
      "Iteration: 2257 Loss: nan\n",
      "Iteration: 2258 Loss: nan\n",
      "Iteration: 2259 Loss: nan\n",
      "Iteration: 2260 Loss: nan\n",
      "Iteration: 2261 Loss: nan\n",
      "Iteration: 2262 Loss: nan\n",
      "Iteration: 2263 Loss: nan\n",
      "Iteration: 2264 Loss: nan\n",
      "Iteration: 2265 Loss: nan\n",
      "Iteration: 2266 Loss: nan\n",
      "Iteration: 2267 Loss: nan\n",
      "Iteration: 2268 Loss: nan\n",
      "Iteration: 2269 Loss: nan\n",
      "Iteration: 2270 Loss: nan\n",
      "Iteration: 2271 Loss: nan\n",
      "Iteration: 2272 Loss: nan\n",
      "Iteration: 2273 Loss: nan\n",
      "Iteration: 2274 Loss: nan\n",
      "Iteration: 2275 Loss: nan\n",
      "Iteration: 2276 Loss: nan\n",
      "Iteration: 2277 Loss: nan\n",
      "Iteration: 2278 Loss: nan\n",
      "Iteration: 2279 Loss: nan\n",
      "Iteration: 2280 Loss: nan\n",
      "Iteration: 2281 Loss: nan\n",
      "Iteration: 2282 Loss: nan\n",
      "Iteration: 2283 Loss: nan\n",
      "Iteration: 2284 Loss: nan\n",
      "Iteration: 2285 Loss: nan\n",
      "Iteration: 2286 Loss: nan\n",
      "Iteration: 2287 Loss: nan\n",
      "Iteration: 2288 Loss: nan\n",
      "Iteration: 2289 Loss: nan\n",
      "Iteration: 2290 Loss: nan\n",
      "Iteration: 2291 Loss: nan\n",
      "Iteration: 2292 Loss: nan\n",
      "Iteration: 2293 Loss: nan\n",
      "Iteration: 2294 Loss: nan\n",
      "Iteration: 2295 Loss: nan\n",
      "Iteration: 2296 Loss: nan\n",
      "Iteration: 2297 Loss: nan\n",
      "Iteration: 2298 Loss: nan\n",
      "Iteration: 2299 Loss: nan\n",
      "Iteration: 2300 Loss: nan\n",
      "Iteration: 2301 Loss: nan\n",
      "Iteration: 2302 Loss: nan\n",
      "Iteration: 2303 Loss: nan\n",
      "Iteration: 2304 Loss: nan\n",
      "Iteration: 2305 Loss: nan\n",
      "Iteration: 2306 Loss: nan\n",
      "Iteration: 2307 Loss: nan\n",
      "Iteration: 2308 Loss: nan\n",
      "Iteration: 2309 Loss: nan\n",
      "Iteration: 2310 Loss: nan\n",
      "Iteration: 2311 Loss: nan\n",
      "Iteration: 2312 Loss: nan\n",
      "Iteration: 2313 Loss: nan\n",
      "Iteration: 2314 Loss: nan\n",
      "Iteration: 2315 Loss: nan\n",
      "Iteration: 2316 Loss: nan\n",
      "Iteration: 2317 Loss: nan\n",
      "Iteration: 2318 Loss: nan\n",
      "Iteration: 2319 Loss: nan\n",
      "Iteration: 2320 Loss: nan\n",
      "Iteration: 2321 Loss: nan\n",
      "Iteration: 2322 Loss: nan\n",
      "Iteration: 2323 Loss: nan\n",
      "Iteration: 2324 Loss: nan\n",
      "Iteration: 2325 Loss: nan\n",
      "Iteration: 2326 Loss: nan\n",
      "Iteration: 2327 Loss: nan\n",
      "Iteration: 2328 Loss: nan\n",
      "Iteration: 2329 Loss: nan\n",
      "Iteration: 2330 Loss: nan\n",
      "Iteration: 2331 Loss: nan\n",
      "Iteration: 2332 Loss: nan\n",
      "Iteration: 2333 Loss: nan\n",
      "Iteration: 2334 Loss: nan\n",
      "Iteration: 2335 Loss: nan\n",
      "Iteration: 2336 Loss: nan\n",
      "Iteration: 2337 Loss: nan\n",
      "Iteration: 2338 Loss: nan\n",
      "Iteration: 2339 Loss: nan\n",
      "Iteration: 2340 Loss: nan\n",
      "Iteration: 2341 Loss: nan\n",
      "Iteration: 2342 Loss: nan\n",
      "Iteration: 2343 Loss: nan\n",
      "Iteration: 2344 Loss: nan\n",
      "Iteration: 2345 Loss: nan\n",
      "Iteration: 2346 Loss: nan\n",
      "Iteration: 2347 Loss: nan\n",
      "Iteration: 2348 Loss: nan\n",
      "Iteration: 2349 Loss: nan\n",
      "Iteration: 2350 Loss: nan\n",
      "Iteration: 2351 Loss: nan\n",
      "Iteration: 2352 Loss: nan\n",
      "Iteration: 2353 Loss: nan\n",
      "Iteration: 2354 Loss: nan\n",
      "Iteration: 2355 Loss: nan\n",
      "Iteration: 2356 Loss: nan\n",
      "Iteration: 2357 Loss: nan\n",
      "Iteration: 2358 Loss: nan\n",
      "Iteration: 2359 Loss: nan\n",
      "Iteration: 2360 Loss: nan\n",
      "Iteration: 2361 Loss: nan\n",
      "Iteration: 2362 Loss: nan\n",
      "Iteration: 2363 Loss: nan\n",
      "Iteration: 2364 Loss: nan\n",
      "Iteration: 2365 Loss: nan\n",
      "Iteration: 2366 Loss: nan\n",
      "Iteration: 2367 Loss: nan\n",
      "Iteration: 2368 Loss: nan\n",
      "Iteration: 2369 Loss: nan\n",
      "Iteration: 2370 Loss: nan\n",
      "Iteration: 2371 Loss: nan\n",
      "Iteration: 2372 Loss: nan\n",
      "Iteration: 2373 Loss: nan\n",
      "Iteration: 2374 Loss: nan\n",
      "Iteration: 2375 Loss: nan\n",
      "Iteration: 2376 Loss: nan\n",
      "Iteration: 2377 Loss: nan\n",
      "Iteration: 2378 Loss: nan\n",
      "Iteration: 2379 Loss: nan\n",
      "Iteration: 2380 Loss: nan\n",
      "Iteration: 2381 Loss: nan\n",
      "Iteration: 2382 Loss: nan\n",
      "Iteration: 2383 Loss: nan\n",
      "Iteration: 2384 Loss: nan\n",
      "Iteration: 2385 Loss: nan\n",
      "Iteration: 2386 Loss: nan\n",
      "Iteration: 2387 Loss: nan\n",
      "Iteration: 2388 Loss: nan\n",
      "Iteration: 2389 Loss: nan\n",
      "Iteration: 2390 Loss: nan\n",
      "Iteration: 2391 Loss: nan\n",
      "Iteration: 2392 Loss: nan\n",
      "Iteration: 2393 Loss: nan\n",
      "Iteration: 2394 Loss: nan\n",
      "Iteration: 2395 Loss: nan\n",
      "Iteration: 2396 Loss: nan\n",
      "Iteration: 2397 Loss: nan\n",
      "Iteration: 2398 Loss: nan\n",
      "Iteration: 2399 Loss: nan\n",
      "Iteration: 2400 Loss: nan\n",
      "Iteration: 2401 Loss: nan\n",
      "Iteration: 2402 Loss: nan\n",
      "Iteration: 2403 Loss: nan\n",
      "Iteration: 2404 Loss: nan\n",
      "Iteration: 2405 Loss: nan\n",
      "Iteration: 2406 Loss: nan\n",
      "Iteration: 2407 Loss: nan\n",
      "Iteration: 2408 Loss: nan\n",
      "Iteration: 2409 Loss: nan\n",
      "Iteration: 2410 Loss: nan\n",
      "Iteration: 2411 Loss: nan\n",
      "Iteration: 2412 Loss: nan\n",
      "Iteration: 2413 Loss: nan\n",
      "Iteration: 2414 Loss: nan\n",
      "Iteration: 2415 Loss: nan\n",
      "Iteration: 2416 Loss: nan\n",
      "Iteration: 2417 Loss: nan\n",
      "Iteration: 2418 Loss: nan\n",
      "Iteration: 2419 Loss: nan\n",
      "Iteration: 2420 Loss: nan\n",
      "Iteration: 2421 Loss: nan\n",
      "Iteration: 2422 Loss: nan\n",
      "Iteration: 2423 Loss: nan\n",
      "Iteration: 2424 Loss: nan\n",
      "Iteration: 2425 Loss: nan\n",
      "Iteration: 2426 Loss: nan\n",
      "Iteration: 2427 Loss: nan\n",
      "Iteration: 2428 Loss: nan\n",
      "Iteration: 2429 Loss: nan\n",
      "Iteration: 2430 Loss: nan\n",
      "Iteration: 2431 Loss: nan\n",
      "Iteration: 2432 Loss: nan\n",
      "Iteration: 2433 Loss: nan\n",
      "Iteration: 2434 Loss: nan\n",
      "Iteration: 2435 Loss: nan\n",
      "Iteration: 2436 Loss: nan\n",
      "Iteration: 2437 Loss: nan\n",
      "Iteration: 2438 Loss: nan\n",
      "Iteration: 2439 Loss: nan\n",
      "Iteration: 2440 Loss: nan\n",
      "Iteration: 2441 Loss: nan\n",
      "Iteration: 2442 Loss: nan\n",
      "Iteration: 2443 Loss: nan\n",
      "Iteration: 2444 Loss: nan\n",
      "Iteration: 2445 Loss: nan\n",
      "Iteration: 2446 Loss: nan\n",
      "Iteration: 2447 Loss: nan\n",
      "Iteration: 2448 Loss: nan\n",
      "Iteration: 2449 Loss: nan\n",
      "Iteration: 2450 Loss: nan\n",
      "Iteration: 2451 Loss: nan\n",
      "Iteration: 2452 Loss: nan\n",
      "Iteration: 2453 Loss: nan\n",
      "Iteration: 2454 Loss: nan\n",
      "Iteration: 2455 Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2456 Loss: nan\n",
      "Iteration: 2457 Loss: nan\n",
      "Iteration: 2458 Loss: nan\n",
      "Iteration: 2459 Loss: nan\n",
      "Iteration: 2460 Loss: nan\n",
      "Iteration: 2461 Loss: nan\n",
      "Iteration: 2462 Loss: nan\n",
      "Iteration: 2463 Loss: nan\n",
      "Iteration: 2464 Loss: nan\n",
      "Iteration: 2465 Loss: nan\n",
      "Iteration: 2466 Loss: nan\n",
      "Iteration: 2467 Loss: nan\n",
      "Iteration: 2468 Loss: nan\n",
      "Iteration: 2469 Loss: nan\n",
      "Iteration: 2470 Loss: nan\n",
      "Iteration: 2471 Loss: nan\n",
      "Iteration: 2472 Loss: nan\n",
      "Iteration: 2473 Loss: nan\n",
      "Iteration: 2474 Loss: nan\n",
      "Iteration: 2475 Loss: nan\n",
      "Iteration: 2476 Loss: nan\n",
      "Iteration: 2477 Loss: nan\n",
      "Iteration: 2478 Loss: nan\n",
      "Iteration: 2479 Loss: nan\n",
      "Iteration: 2480 Loss: nan\n",
      "Iteration: 2481 Loss: nan\n",
      "Iteration: 2482 Loss: nan\n",
      "Iteration: 2483 Loss: nan\n",
      "Iteration: 2484 Loss: nan\n",
      "Iteration: 2485 Loss: nan\n",
      "Iteration: 2486 Loss: nan\n",
      "Iteration: 2487 Loss: nan\n",
      "Iteration: 2488 Loss: nan\n",
      "Iteration: 2489 Loss: nan\n",
      "Iteration: 2490 Loss: nan\n",
      "Iteration: 2491 Loss: nan\n",
      "Iteration: 2492 Loss: nan\n",
      "Iteration: 2493 Loss: nan\n",
      "Iteration: 2494 Loss: nan\n",
      "Iteration: 2495 Loss: nan\n",
      "Iteration: 2496 Loss: nan\n",
      "Iteration: 2497 Loss: nan\n",
      "Iteration: 2498 Loss: nan\n",
      "Iteration: 2499 Loss: nan\n",
      "Iteration: 2500 Loss: nan\n",
      "Iteration: 2501 Loss: nan\n",
      "Iteration: 2502 Loss: nan\n",
      "Iteration: 2503 Loss: nan\n",
      "Iteration: 2504 Loss: nan\n",
      "Iteration: 2505 Loss: nan\n",
      "Iteration: 2506 Loss: nan\n",
      "Iteration: 2507 Loss: nan\n",
      "Iteration: 2508 Loss: nan\n",
      "Iteration: 2509 Loss: nan\n",
      "Iteration: 2510 Loss: nan\n",
      "Iteration: 2511 Loss: nan\n",
      "Iteration: 2512 Loss: nan\n",
      "Iteration: 2513 Loss: nan\n",
      "Iteration: 2514 Loss: nan\n",
      "Iteration: 2515 Loss: nan\n",
      "Iteration: 2516 Loss: nan\n",
      "Iteration: 2517 Loss: nan\n",
      "Iteration: 2518 Loss: nan\n",
      "Iteration: 2519 Loss: nan\n",
      "Iteration: 2520 Loss: nan\n",
      "Iteration: 2521 Loss: nan\n",
      "Iteration: 2522 Loss: nan\n",
      "Iteration: 2523 Loss: nan\n",
      "Iteration: 2524 Loss: nan\n",
      "Iteration: 2525 Loss: nan\n",
      "Iteration: 2526 Loss: nan\n",
      "Iteration: 2527 Loss: nan\n",
      "Iteration: 2528 Loss: nan\n",
      "Iteration: 2529 Loss: nan\n",
      "Iteration: 2530 Loss: nan\n",
      "Iteration: 2531 Loss: nan\n",
      "Iteration: 2532 Loss: nan\n",
      "Iteration: 2533 Loss: nan\n",
      "Iteration: 2534 Loss: nan\n",
      "Iteration: 2535 Loss: nan\n",
      "Iteration: 2536 Loss: nan\n",
      "Iteration: 2537 Loss: nan\n",
      "Iteration: 2538 Loss: nan\n",
      "Iteration: 2539 Loss: nan\n",
      "Iteration: 2540 Loss: nan\n",
      "Iteration: 2541 Loss: nan\n",
      "Iteration: 2542 Loss: nan\n",
      "Iteration: 2543 Loss: nan\n",
      "Iteration: 2544 Loss: nan\n",
      "Iteration: 2545 Loss: nan\n",
      "Iteration: 2546 Loss: nan\n",
      "Iteration: 2547 Loss: nan\n",
      "Iteration: 2548 Loss: nan\n",
      "Iteration: 2549 Loss: nan\n",
      "Iteration: 2550 Loss: nan\n",
      "Iteration: 2551 Loss: nan\n",
      "Iteration: 2552 Loss: nan\n",
      "Iteration: 2553 Loss: nan\n",
      "Iteration: 2554 Loss: nan\n",
      "Iteration: 2555 Loss: nan\n",
      "Iteration: 2556 Loss: nan\n",
      "Iteration: 2557 Loss: nan\n",
      "Iteration: 2558 Loss: nan\n",
      "Iteration: 2559 Loss: nan\n",
      "Iteration: 2560 Loss: nan\n",
      "Iteration: 2561 Loss: nan\n",
      "Iteration: 2562 Loss: nan\n",
      "Iteration: 2563 Loss: nan\n",
      "Iteration: 2564 Loss: nan\n",
      "Iteration: 2565 Loss: nan\n",
      "Iteration: 2566 Loss: nan\n",
      "Iteration: 2567 Loss: nan\n",
      "Iteration: 2568 Loss: nan\n",
      "Iteration: 2569 Loss: nan\n",
      "Iteration: 2570 Loss: nan\n",
      "Iteration: 2571 Loss: nan\n",
      "Iteration: 2572 Loss: nan\n",
      "Iteration: 2573 Loss: nan\n",
      "Iteration: 2574 Loss: nan\n",
      "Iteration: 2575 Loss: nan\n",
      "Iteration: 2576 Loss: nan\n",
      "Iteration: 2577 Loss: nan\n",
      "Iteration: 2578 Loss: nan\n",
      "Iteration: 2579 Loss: nan\n",
      "Iteration: 2580 Loss: nan\n",
      "Iteration: 2581 Loss: nan\n",
      "Iteration: 2582 Loss: nan\n",
      "Iteration: 2583 Loss: nan\n",
      "Iteration: 2584 Loss: nan\n",
      "Iteration: 2585 Loss: nan\n",
      "Iteration: 2586 Loss: nan\n",
      "Iteration: 2587 Loss: nan\n",
      "Iteration: 2588 Loss: nan\n",
      "Iteration: 2589 Loss: nan\n",
      "Iteration: 2590 Loss: nan\n",
      "Iteration: 2591 Loss: nan\n",
      "Iteration: 2592 Loss: nan\n",
      "Iteration: 2593 Loss: nan\n",
      "Iteration: 2594 Loss: nan\n",
      "Iteration: 2595 Loss: nan\n",
      "Iteration: 2596 Loss: nan\n",
      "Iteration: 2597 Loss: nan\n",
      "Iteration: 2598 Loss: nan\n",
      "Iteration: 2599 Loss: nan\n",
      "Iteration: 2600 Loss: nan\n",
      "Iteration: 2601 Loss: nan\n",
      "Iteration: 2602 Loss: nan\n",
      "Iteration: 2603 Loss: nan\n",
      "Iteration: 2604 Loss: nan\n",
      "Iteration: 2605 Loss: nan\n",
      "Iteration: 2606 Loss: nan\n",
      "Iteration: 2607 Loss: nan\n",
      "Iteration: 2608 Loss: nan\n",
      "Iteration: 2609 Loss: nan\n",
      "Iteration: 2610 Loss: nan\n",
      "Iteration: 2611 Loss: nan\n",
      "Iteration: 2612 Loss: nan\n",
      "Iteration: 2613 Loss: nan\n",
      "Iteration: 2614 Loss: nan\n",
      "Iteration: 2615 Loss: nan\n",
      "Iteration: 2616 Loss: nan\n",
      "Iteration: 2617 Loss: nan\n",
      "Iteration: 2618 Loss: nan\n",
      "Iteration: 2619 Loss: nan\n",
      "Iteration: 2620 Loss: nan\n",
      "Iteration: 2621 Loss: nan\n",
      "Iteration: 2622 Loss: nan\n",
      "Iteration: 2623 Loss: nan\n",
      "Iteration: 2624 Loss: nan\n",
      "Iteration: 2625 Loss: nan\n",
      "Iteration: 2626 Loss: nan\n",
      "Iteration: 2627 Loss: nan\n",
      "Iteration: 2628 Loss: nan\n",
      "Iteration: 2629 Loss: nan\n",
      "Iteration: 2630 Loss: nan\n",
      "Iteration: 2631 Loss: nan\n",
      "Iteration: 2632 Loss: nan\n",
      "Iteration: 2633 Loss: nan\n",
      "Iteration: 2634 Loss: nan\n",
      "Iteration: 2635 Loss: nan\n",
      "Iteration: 2636 Loss: nan\n",
      "Iteration: 2637 Loss: nan\n",
      "Iteration: 2638 Loss: nan\n",
      "Iteration: 2639 Loss: nan\n",
      "Iteration: 2640 Loss: nan\n",
      "Iteration: 2641 Loss: nan\n",
      "Iteration: 2642 Loss: nan\n",
      "Iteration: 2643 Loss: nan\n",
      "Iteration: 2644 Loss: nan\n",
      "Iteration: 2645 Loss: nan\n",
      "Iteration: 2646 Loss: nan\n",
      "Iteration: 2647 Loss: nan\n",
      "Iteration: 2648 Loss: nan\n",
      "Iteration: 2649 Loss: nan\n",
      "Iteration: 2650 Loss: nan\n",
      "Iteration: 2651 Loss: nan\n",
      "Iteration: 2652 Loss: nan\n",
      "Iteration: 2653 Loss: nan\n",
      "Iteration: 2654 Loss: nan\n",
      "Iteration: 2655 Loss: nan\n",
      "Iteration: 2656 Loss: nan\n",
      "Iteration: 2657 Loss: nan\n",
      "Iteration: 2658 Loss: nan\n",
      "Iteration: 2659 Loss: nan\n",
      "Iteration: 2660 Loss: nan\n",
      "Iteration: 2661 Loss: nan\n",
      "Iteration: 2662 Loss: nan\n",
      "Iteration: 2663 Loss: nan\n",
      "Iteration: 2664 Loss: nan\n",
      "Iteration: 2665 Loss: nan\n",
      "Iteration: 2666 Loss: nan\n",
      "Iteration: 2667 Loss: nan\n",
      "Iteration: 2668 Loss: nan\n",
      "Iteration: 2669 Loss: nan\n",
      "Iteration: 2670 Loss: nan\n",
      "Iteration: 2671 Loss: nan\n",
      "Iteration: 2672 Loss: nan\n",
      "Iteration: 2673 Loss: nan\n",
      "Iteration: 2674 Loss: nan\n",
      "Iteration: 2675 Loss: nan\n",
      "Iteration: 2676 Loss: nan\n",
      "Iteration: 2677 Loss: nan\n",
      "Iteration: 2678 Loss: nan\n",
      "Iteration: 2679 Loss: nan\n",
      "Iteration: 2680 Loss: nan\n",
      "Iteration: 2681 Loss: nan\n",
      "Iteration: 2682 Loss: nan\n",
      "Iteration: 2683 Loss: nan\n",
      "Iteration: 2684 Loss: nan\n",
      "Iteration: 2685 Loss: nan\n",
      "Iteration: 2686 Loss: nan\n",
      "Iteration: 2687 Loss: nan\n",
      "Iteration: 2688 Loss: nan\n",
      "Iteration: 2689 Loss: nan\n",
      "Iteration: 2690 Loss: nan\n",
      "Iteration: 2691 Loss: nan\n",
      "Iteration: 2692 Loss: nan\n",
      "Iteration: 2693 Loss: nan\n",
      "Iteration: 2694 Loss: nan\n",
      "Iteration: 2695 Loss: nan\n",
      "Iteration: 2696 Loss: nan\n",
      "Iteration: 2697 Loss: nan\n",
      "Iteration: 2698 Loss: nan\n",
      "Iteration: 2699 Loss: nan\n",
      "Iteration: 2700 Loss: nan\n",
      "Iteration: 2701 Loss: nan\n",
      "Iteration: 2702 Loss: nan\n",
      "Iteration: 2703 Loss: nan\n",
      "Iteration: 2704 Loss: nan\n",
      "Iteration: 2705 Loss: nan\n",
      "Iteration: 2706 Loss: nan\n",
      "Iteration: 2707 Loss: nan\n",
      "Iteration: 2708 Loss: nan\n",
      "Iteration: 2709 Loss: nan\n",
      "Iteration: 2710 Loss: nan\n",
      "Iteration: 2711 Loss: nan\n",
      "Iteration: 2712 Loss: nan\n",
      "Iteration: 2713 Loss: nan\n",
      "Iteration: 2714 Loss: nan\n",
      "Iteration: 2715 Loss: nan\n",
      "Iteration: 2716 Loss: nan\n",
      "Iteration: 2717 Loss: nan\n",
      "Iteration: 2718 Loss: nan\n",
      "Iteration: 2719 Loss: nan\n",
      "Iteration: 2720 Loss: nan\n",
      "Iteration: 2721 Loss: nan\n",
      "Iteration: 2722 Loss: nan\n",
      "Iteration: 2723 Loss: nan\n",
      "Iteration: 2724 Loss: nan\n",
      "Iteration: 2725 Loss: nan\n",
      "Iteration: 2726 Loss: nan\n",
      "Iteration: 2727 Loss: nan\n",
      "Iteration: 2728 Loss: nan\n",
      "Iteration: 2729 Loss: nan\n",
      "Iteration: 2730 Loss: nan\n",
      "Iteration: 2731 Loss: nan\n",
      "Iteration: 2732 Loss: nan\n",
      "Iteration: 2733 Loss: nan\n",
      "Iteration: 2734 Loss: nan\n",
      "Iteration: 2735 Loss: nan\n",
      "Iteration: 2736 Loss: nan\n",
      "Iteration: 2737 Loss: nan\n",
      "Iteration: 2738 Loss: nan\n",
      "Iteration: 2739 Loss: nan\n",
      "Iteration: 2740 Loss: nan\n",
      "Iteration: 2741 Loss: nan\n",
      "Iteration: 2742 Loss: nan\n",
      "Iteration: 2743 Loss: nan\n",
      "Iteration: 2744 Loss: nan\n",
      "Iteration: 2745 Loss: nan\n",
      "Iteration: 2746 Loss: nan\n",
      "Iteration: 2747 Loss: nan\n",
      "Iteration: 2748 Loss: nan\n",
      "Iteration: 2749 Loss: nan\n",
      "Iteration: 2750 Loss: nan\n",
      "Iteration: 2751 Loss: nan\n",
      "Iteration: 2752 Loss: nan\n",
      "Iteration: 2753 Loss: nan\n",
      "Iteration: 2754 Loss: nan\n",
      "Iteration: 2755 Loss: nan\n",
      "Iteration: 2756 Loss: nan\n",
      "Iteration: 2757 Loss: nan\n",
      "Iteration: 2758 Loss: nan\n",
      "Iteration: 2759 Loss: nan\n",
      "Iteration: 2760 Loss: nan\n",
      "Iteration: 2761 Loss: nan\n",
      "Iteration: 2762 Loss: nan\n",
      "Iteration: 2763 Loss: nan\n",
      "Iteration: 2764 Loss: nan\n",
      "Iteration: 2765 Loss: nan\n",
      "Iteration: 2766 Loss: nan\n",
      "Iteration: 2767 Loss: nan\n",
      "Iteration: 2768 Loss: nan\n",
      "Iteration: 2769 Loss: nan\n",
      "Iteration: 2770 Loss: nan\n",
      "Iteration: 2771 Loss: nan\n",
      "Iteration: 2772 Loss: nan\n",
      "Iteration: 2773 Loss: nan\n",
      "Iteration: 2774 Loss: nan\n",
      "Iteration: 2775 Loss: nan\n",
      "Iteration: 2776 Loss: nan\n",
      "Iteration: 2777 Loss: nan\n",
      "Iteration: 2778 Loss: nan\n",
      "Iteration: 2779 Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2780 Loss: nan\n",
      "Iteration: 2781 Loss: nan\n",
      "Iteration: 2782 Loss: nan\n",
      "Iteration: 2783 Loss: nan\n",
      "Iteration: 2784 Loss: nan\n",
      "Iteration: 2785 Loss: nan\n",
      "Iteration: 2786 Loss: nan\n",
      "Iteration: 2787 Loss: nan\n",
      "Iteration: 2788 Loss: nan\n",
      "Iteration: 2789 Loss: nan\n",
      "Iteration: 2790 Loss: nan\n",
      "Iteration: 2791 Loss: nan\n",
      "Iteration: 2792 Loss: nan\n",
      "Iteration: 2793 Loss: nan\n",
      "Iteration: 2794 Loss: nan\n",
      "Iteration: 2795 Loss: nan\n",
      "Iteration: 2796 Loss: nan\n",
      "Iteration: 2797 Loss: nan\n",
      "Iteration: 2798 Loss: nan\n",
      "Iteration: 2799 Loss: nan\n",
      "Iteration: 2800 Loss: nan\n",
      "Iteration: 2801 Loss: nan\n",
      "Iteration: 2802 Loss: nan\n",
      "Iteration: 2803 Loss: nan\n",
      "Iteration: 2804 Loss: nan\n",
      "Iteration: 2805 Loss: nan\n",
      "Iteration: 2806 Loss: nan\n",
      "Iteration: 2807 Loss: nan\n",
      "Iteration: 2808 Loss: nan\n",
      "Iteration: 2809 Loss: nan\n",
      "Iteration: 2810 Loss: nan\n",
      "Iteration: 2811 Loss: nan\n",
      "Iteration: 2812 Loss: nan\n",
      "Iteration: 2813 Loss: nan\n",
      "Iteration: 2814 Loss: nan\n",
      "Iteration: 2815 Loss: nan\n",
      "Iteration: 2816 Loss: nan\n",
      "Iteration: 2817 Loss: nan\n",
      "Iteration: 2818 Loss: nan\n",
      "Iteration: 2819 Loss: nan\n",
      "Iteration: 2820 Loss: nan\n",
      "Iteration: 2821 Loss: nan\n",
      "Iteration: 2822 Loss: nan\n",
      "Iteration: 2823 Loss: nan\n",
      "Iteration: 2824 Loss: nan\n",
      "Iteration: 2825 Loss: nan\n",
      "Iteration: 2826 Loss: nan\n",
      "Iteration: 2827 Loss: nan\n",
      "Iteration: 2828 Loss: nan\n",
      "Iteration: 2829 Loss: nan\n",
      "Iteration: 2830 Loss: nan\n",
      "Iteration: 2831 Loss: nan\n",
      "Iteration: 2832 Loss: nan\n",
      "Iteration: 2833 Loss: nan\n",
      "Iteration: 2834 Loss: nan\n",
      "Iteration: 2835 Loss: nan\n",
      "Iteration: 2836 Loss: nan\n",
      "Iteration: 2837 Loss: nan\n",
      "Iteration: 2838 Loss: nan\n",
      "Iteration: 2839 Loss: nan\n",
      "Iteration: 2840 Loss: nan\n",
      "Iteration: 2841 Loss: nan\n",
      "Iteration: 2842 Loss: nan\n",
      "Iteration: 2843 Loss: nan\n",
      "Iteration: 2844 Loss: nan\n",
      "Iteration: 2845 Loss: nan\n",
      "Iteration: 2846 Loss: nan\n",
      "Iteration: 2847 Loss: nan\n",
      "Iteration: 2848 Loss: nan\n",
      "Iteration: 2849 Loss: nan\n",
      "Iteration: 2850 Loss: nan\n",
      "Iteration: 2851 Loss: nan\n",
      "Iteration: 2852 Loss: nan\n",
      "Iteration: 2853 Loss: nan\n",
      "Iteration: 2854 Loss: nan\n",
      "Iteration: 2855 Loss: nan\n",
      "Iteration: 2856 Loss: nan\n",
      "Iteration: 2857 Loss: nan\n",
      "Iteration: 2858 Loss: nan\n",
      "Iteration: 2859 Loss: nan\n",
      "Iteration: 2860 Loss: nan\n",
      "Iteration: 2861 Loss: nan\n",
      "Iteration: 2862 Loss: nan\n",
      "Iteration: 2863 Loss: nan\n",
      "Iteration: 2864 Loss: nan\n",
      "Iteration: 2865 Loss: nan\n",
      "Iteration: 2866 Loss: nan\n",
      "Iteration: 2867 Loss: nan\n",
      "Iteration: 2868 Loss: nan\n",
      "Iteration: 2869 Loss: nan\n",
      "Iteration: 2870 Loss: nan\n",
      "Iteration: 2871 Loss: nan\n",
      "Iteration: 2872 Loss: nan\n",
      "Iteration: 2873 Loss: nan\n",
      "Iteration: 2874 Loss: nan\n",
      "Iteration: 2875 Loss: nan\n",
      "Iteration: 2876 Loss: nan\n",
      "Iteration: 2877 Loss: nan\n",
      "Iteration: 2878 Loss: nan\n",
      "Iteration: 2879 Loss: nan\n",
      "Iteration: 2880 Loss: nan\n",
      "Iteration: 2881 Loss: nan\n",
      "Iteration: 2882 Loss: nan\n",
      "Iteration: 2883 Loss: nan\n",
      "Iteration: 2884 Loss: nan\n",
      "Iteration: 2885 Loss: nan\n",
      "Iteration: 2886 Loss: nan\n",
      "Iteration: 2887 Loss: nan\n",
      "Iteration: 2888 Loss: nan\n",
      "Iteration: 2889 Loss: nan\n",
      "Iteration: 2890 Loss: nan\n",
      "Iteration: 2891 Loss: nan\n",
      "Iteration: 2892 Loss: nan\n",
      "Iteration: 2893 Loss: nan\n",
      "Iteration: 2894 Loss: nan\n",
      "Iteration: 2895 Loss: nan\n",
      "Iteration: 2896 Loss: nan\n",
      "Iteration: 2897 Loss: nan\n",
      "Iteration: 2898 Loss: nan\n",
      "Iteration: 2899 Loss: nan\n",
      "Iteration: 2900 Loss: nan\n",
      "Iteration: 2901 Loss: nan\n",
      "Iteration: 2902 Loss: nan\n",
      "Iteration: 2903 Loss: nan\n",
      "Iteration: 2904 Loss: nan\n",
      "Iteration: 2905 Loss: nan\n",
      "Iteration: 2906 Loss: nan\n",
      "Iteration: 2907 Loss: nan\n",
      "Iteration: 2908 Loss: nan\n",
      "Iteration: 2909 Loss: nan\n",
      "Iteration: 2910 Loss: nan\n",
      "Iteration: 2911 Loss: nan\n",
      "Iteration: 2912 Loss: nan\n",
      "Iteration: 2913 Loss: nan\n",
      "Iteration: 2914 Loss: nan\n",
      "Iteration: 2915 Loss: nan\n",
      "Iteration: 2916 Loss: nan\n",
      "Iteration: 2917 Loss: nan\n",
      "Iteration: 2918 Loss: nan\n",
      "Iteration: 2919 Loss: nan\n",
      "Iteration: 2920 Loss: nan\n",
      "Iteration: 2921 Loss: nan\n",
      "Iteration: 2922 Loss: nan\n",
      "Iteration: 2923 Loss: nan\n",
      "Iteration: 2924 Loss: nan\n",
      "Iteration: 2925 Loss: nan\n",
      "Iteration: 2926 Loss: nan\n",
      "Iteration: 2927 Loss: nan\n",
      "Iteration: 2928 Loss: nan\n",
      "Iteration: 2929 Loss: nan\n",
      "Iteration: 2930 Loss: nan\n",
      "Iteration: 2931 Loss: nan\n",
      "Iteration: 2932 Loss: nan\n",
      "Iteration: 2933 Loss: nan\n",
      "Iteration: 2934 Loss: nan\n",
      "Iteration: 2935 Loss: nan\n",
      "Iteration: 2936 Loss: nan\n",
      "Iteration: 2937 Loss: nan\n",
      "Iteration: 2938 Loss: nan\n",
      "Iteration: 2939 Loss: nan\n",
      "Iteration: 2940 Loss: nan\n",
      "Iteration: 2941 Loss: nan\n",
      "Iteration: 2942 Loss: nan\n",
      "Iteration: 2943 Loss: nan\n",
      "Iteration: 2944 Loss: nan\n",
      "Iteration: 2945 Loss: nan\n",
      "Iteration: 2946 Loss: nan\n",
      "Iteration: 2947 Loss: nan\n",
      "Iteration: 2948 Loss: nan\n",
      "Iteration: 2949 Loss: nan\n",
      "Iteration: 2950 Loss: nan\n",
      "Iteration: 2951 Loss: nan\n",
      "Iteration: 2952 Loss: nan\n",
      "Iteration: 2953 Loss: nan\n",
      "Iteration: 2954 Loss: nan\n",
      "Iteration: 2955 Loss: nan\n",
      "Iteration: 2956 Loss: nan\n",
      "Iteration: 2957 Loss: nan\n",
      "Iteration: 2958 Loss: nan\n",
      "Iteration: 2959 Loss: nan\n",
      "Iteration: 2960 Loss: nan\n",
      "Iteration: 2961 Loss: nan\n",
      "Iteration: 2962 Loss: nan\n",
      "Iteration: 2963 Loss: nan\n",
      "Iteration: 2964 Loss: nan\n",
      "Iteration: 2965 Loss: nan\n",
      "Iteration: 2966 Loss: nan\n",
      "Iteration: 2967 Loss: nan\n",
      "Iteration: 2968 Loss: nan\n",
      "Iteration: 2969 Loss: nan\n",
      "Iteration: 2970 Loss: nan\n",
      "Iteration: 2971 Loss: nan\n",
      "Iteration: 2972 Loss: nan\n",
      "Iteration: 2973 Loss: nan\n",
      "Iteration: 2974 Loss: nan\n",
      "Iteration: 2975 Loss: nan\n",
      "Iteration: 2976 Loss: nan\n",
      "Iteration: 2977 Loss: nan\n",
      "Iteration: 2978 Loss: nan\n",
      "Iteration: 2979 Loss: nan\n",
      "Iteration: 2980 Loss: nan\n",
      "Iteration: 2981 Loss: nan\n",
      "Iteration: 2982 Loss: nan\n",
      "Iteration: 2983 Loss: nan\n",
      "Iteration: 2984 Loss: nan\n",
      "Iteration: 2985 Loss: nan\n",
      "Iteration: 2986 Loss: nan\n",
      "Iteration: 2987 Loss: nan\n",
      "Iteration: 2988 Loss: nan\n",
      "Iteration: 2989 Loss: nan\n",
      "Iteration: 2990 Loss: nan\n",
      "Iteration: 2991 Loss: nan\n",
      "Iteration: 2992 Loss: nan\n",
      "Iteration: 2993 Loss: nan\n",
      "Iteration: 2994 Loss: nan\n",
      "Iteration: 2995 Loss: nan\n",
      "Iteration: 2996 Loss: nan\n",
      "Iteration: 2997 Loss: nan\n",
      "Iteration: 2998 Loss: nan\n",
      "Iteration: 2999 Loss: nan\n",
      "Iteration: 3000 Loss: nan\n",
      "Iteration: 3001 Loss: nan\n",
      "Iteration: 3002 Loss: nan\n",
      "Iteration: 3003 Loss: nan\n",
      "Iteration: 3004 Loss: nan\n",
      "Iteration: 3005 Loss: nan\n",
      "Iteration: 3006 Loss: nan\n",
      "Iteration: 3007 Loss: nan\n",
      "Iteration: 3008 Loss: nan\n",
      "Iteration: 3009 Loss: nan\n",
      "Iteration: 3010 Loss: nan\n",
      "Iteration: 3011 Loss: nan\n",
      "Iteration: 3012 Loss: nan\n",
      "Iteration: 3013 Loss: nan\n",
      "Iteration: 3014 Loss: nan\n",
      "Iteration: 3015 Loss: nan\n",
      "Iteration: 3016 Loss: nan\n",
      "Iteration: 3017 Loss: nan\n",
      "Iteration: 3018 Loss: nan\n",
      "Iteration: 3019 Loss: nan\n",
      "Iteration: 3020 Loss: nan\n",
      "Iteration: 3021 Loss: nan\n",
      "Iteration: 3022 Loss: nan\n",
      "Iteration: 3023 Loss: nan\n",
      "Iteration: 3024 Loss: nan\n",
      "Iteration: 3025 Loss: nan\n",
      "Iteration: 3026 Loss: nan\n",
      "Iteration: 3027 Loss: nan\n",
      "Iteration: 3028 Loss: nan\n",
      "Iteration: 3029 Loss: nan\n",
      "Iteration: 3030 Loss: nan\n",
      "Iteration: 3031 Loss: nan\n",
      "Iteration: 3032 Loss: nan\n",
      "Iteration: 3033 Loss: nan\n",
      "Iteration: 3034 Loss: nan\n",
      "Iteration: 3035 Loss: nan\n",
      "Iteration: 3036 Loss: nan\n",
      "Iteration: 3037 Loss: nan\n",
      "Iteration: 3038 Loss: nan\n",
      "Iteration: 3039 Loss: nan\n",
      "Iteration: 3040 Loss: nan\n",
      "Iteration: 3041 Loss: nan\n",
      "Iteration: 3042 Loss: nan\n",
      "Iteration: 3043 Loss: nan\n",
      "Iteration: 3044 Loss: nan\n",
      "Iteration: 3045 Loss: nan\n",
      "Iteration: 3046 Loss: nan\n",
      "Iteration: 3047 Loss: nan\n",
      "Iteration: 3048 Loss: nan\n",
      "Iteration: 3049 Loss: nan\n",
      "Iteration: 3050 Loss: nan\n",
      "Iteration: 3051 Loss: nan\n",
      "Iteration: 3052 Loss: nan\n",
      "Iteration: 3053 Loss: nan\n",
      "Iteration: 3054 Loss: nan\n",
      "Iteration: 3055 Loss: nan\n",
      "Iteration: 3056 Loss: nan\n",
      "Iteration: 3057 Loss: nan\n",
      "Iteration: 3058 Loss: nan\n",
      "Iteration: 3059 Loss: nan\n",
      "Iteration: 3060 Loss: nan\n",
      "Iteration: 3061 Loss: nan\n",
      "Iteration: 3062 Loss: nan\n",
      "Iteration: 3063 Loss: nan\n",
      "Iteration: 3064 Loss: nan\n",
      "Iteration: 3065 Loss: nan\n",
      "Iteration: 3066 Loss: nan\n",
      "Iteration: 3067 Loss: nan\n",
      "Iteration: 3068 Loss: nan\n",
      "Iteration: 3069 Loss: nan\n",
      "Iteration: 3070 Loss: nan\n",
      "Iteration: 3071 Loss: nan\n",
      "Iteration: 3072 Loss: nan\n",
      "Iteration: 3073 Loss: nan\n",
      "Iteration: 3074 Loss: nan\n",
      "Iteration: 3075 Loss: nan\n",
      "Iteration: 3076 Loss: nan\n",
      "Iteration: 3077 Loss: nan\n",
      "Iteration: 3078 Loss: nan\n",
      "Iteration: 3079 Loss: nan\n",
      "Iteration: 3080 Loss: nan\n",
      "Iteration: 3081 Loss: nan\n",
      "Iteration: 3082 Loss: nan\n",
      "Iteration: 3083 Loss: nan\n",
      "Iteration: 3084 Loss: nan\n",
      "Iteration: 3085 Loss: nan\n",
      "Iteration: 3086 Loss: nan\n",
      "Iteration: 3087 Loss: nan\n",
      "Iteration: 3088 Loss: nan\n",
      "Iteration: 3089 Loss: nan\n",
      "Iteration: 3090 Loss: nan\n",
      "Iteration: 3091 Loss: nan\n",
      "Iteration: 3092 Loss: nan\n",
      "Iteration: 3093 Loss: nan\n",
      "Iteration: 3094 Loss: nan\n",
      "Iteration: 3095 Loss: nan\n",
      "Iteration: 3096 Loss: nan\n",
      "Iteration: 3097 Loss: nan\n",
      "Iteration: 3098 Loss: nan\n",
      "Iteration: 3099 Loss: nan\n",
      "Iteration: 3100 Loss: nan\n",
      "Iteration: 3101 Loss: nan\n",
      "Iteration: 3102 Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3103 Loss: nan\n",
      "Iteration: 3104 Loss: nan\n",
      "Iteration: 3105 Loss: nan\n",
      "Iteration: 3106 Loss: nan\n",
      "Iteration: 3107 Loss: nan\n",
      "Iteration: 3108 Loss: nan\n",
      "Iteration: 3109 Loss: nan\n",
      "Iteration: 3110 Loss: nan\n",
      "Iteration: 3111 Loss: nan\n",
      "Iteration: 3112 Loss: nan\n",
      "Iteration: 3113 Loss: nan\n",
      "Iteration: 3114 Loss: nan\n",
      "Iteration: 3115 Loss: nan\n",
      "Iteration: 3116 Loss: nan\n",
      "Iteration: 3117 Loss: nan\n",
      "Iteration: 3118 Loss: nan\n",
      "Iteration: 3119 Loss: nan\n",
      "Iteration: 3120 Loss: nan\n",
      "Iteration: 3121 Loss: nan\n",
      "Iteration: 3122 Loss: nan\n",
      "Iteration: 3123 Loss: nan\n",
      "Iteration: 3124 Loss: nan\n",
      "Iteration: 3125 Loss: nan\n",
      "Iteration: 3126 Loss: nan\n",
      "Iteration: 3127 Loss: nan\n",
      "Iteration: 3128 Loss: nan\n",
      "Iteration: 3129 Loss: nan\n",
      "Iteration: 3130 Loss: nan\n",
      "Iteration: 3131 Loss: nan\n",
      "Iteration: 3132 Loss: nan\n",
      "Iteration: 3133 Loss: nan\n",
      "Iteration: 3134 Loss: nan\n",
      "Iteration: 3135 Loss: nan\n",
      "Iteration: 3136 Loss: nan\n",
      "Iteration: 3137 Loss: nan\n",
      "Iteration: 3138 Loss: nan\n",
      "Iteration: 3139 Loss: nan\n",
      "Iteration: 3140 Loss: nan\n",
      "Iteration: 3141 Loss: nan\n",
      "Iteration: 3142 Loss: nan\n",
      "Iteration: 3143 Loss: nan\n",
      "Iteration: 3144 Loss: nan\n",
      "Iteration: 3145 Loss: nan\n",
      "Iteration: 3146 Loss: nan\n",
      "Iteration: 3147 Loss: nan\n",
      "Iteration: 3148 Loss: nan\n",
      "Iteration: 3149 Loss: nan\n",
      "Iteration: 3150 Loss: nan\n",
      "Iteration: 3151 Loss: nan\n",
      "Iteration: 3152 Loss: nan\n",
      "Iteration: 3153 Loss: nan\n",
      "Iteration: 3154 Loss: nan\n",
      "Iteration: 3155 Loss: nan\n",
      "Iteration: 3156 Loss: nan\n",
      "Iteration: 3157 Loss: nan\n",
      "Iteration: 3158 Loss: nan\n",
      "Iteration: 3159 Loss: nan\n",
      "Iteration: 3160 Loss: nan\n",
      "Iteration: 3161 Loss: nan\n",
      "Iteration: 3162 Loss: nan\n",
      "Iteration: 3163 Loss: nan\n",
      "Iteration: 3164 Loss: nan\n",
      "Iteration: 3165 Loss: nan\n",
      "Iteration: 3166 Loss: nan\n",
      "Iteration: 3167 Loss: nan\n",
      "Iteration: 3168 Loss: nan\n",
      "Iteration: 3169 Loss: nan\n",
      "Iteration: 3170 Loss: nan\n",
      "Iteration: 3171 Loss: nan\n",
      "Iteration: 3172 Loss: nan\n",
      "Iteration: 3173 Loss: nan\n",
      "Iteration: 3174 Loss: nan\n",
      "Iteration: 3175 Loss: nan\n",
      "Iteration: 3176 Loss: nan\n",
      "Iteration: 3177 Loss: nan\n",
      "Iteration: 3178 Loss: nan\n",
      "Iteration: 3179 Loss: nan\n",
      "Iteration: 3180 Loss: nan\n",
      "Iteration: 3181 Loss: nan\n",
      "Iteration: 3182 Loss: nan\n",
      "Iteration: 3183 Loss: nan\n",
      "Iteration: 3184 Loss: nan\n",
      "Iteration: 3185 Loss: nan\n",
      "Iteration: 3186 Loss: nan\n",
      "Iteration: 3187 Loss: nan\n",
      "Iteration: 3188 Loss: nan\n",
      "Iteration: 3189 Loss: nan\n",
      "Iteration: 3190 Loss: nan\n",
      "Iteration: 3191 Loss: nan\n",
      "Iteration: 3192 Loss: nan\n",
      "Iteration: 3193 Loss: nan\n",
      "Iteration: 3194 Loss: nan\n",
      "Iteration: 3195 Loss: nan\n",
      "Iteration: 3196 Loss: nan\n",
      "Iteration: 3197 Loss: nan\n",
      "Iteration: 3198 Loss: nan\n",
      "Iteration: 3199 Loss: nan\n",
      "Iteration: 3200 Loss: nan\n",
      "Iteration: 3201 Loss: nan\n",
      "Iteration: 3202 Loss: nan\n",
      "Iteration: 3203 Loss: nan\n",
      "Iteration: 3204 Loss: nan\n",
      "Iteration: 3205 Loss: nan\n",
      "Iteration: 3206 Loss: nan\n",
      "Iteration: 3207 Loss: nan\n",
      "Iteration: 3208 Loss: nan\n",
      "Iteration: 3209 Loss: nan\n",
      "Iteration: 3210 Loss: nan\n",
      "Iteration: 3211 Loss: nan\n",
      "Iteration: 3212 Loss: nan\n",
      "Iteration: 3213 Loss: nan\n",
      "Iteration: 3214 Loss: nan\n",
      "Iteration: 3215 Loss: nan\n",
      "Iteration: 3216 Loss: nan\n",
      "Iteration: 3217 Loss: nan\n",
      "Iteration: 3218 Loss: nan\n",
      "Iteration: 3219 Loss: nan\n",
      "Iteration: 3220 Loss: nan\n",
      "Iteration: 3221 Loss: nan\n",
      "Iteration: 3222 Loss: nan\n",
      "Iteration: 3223 Loss: nan\n",
      "Iteration: 3224 Loss: nan\n",
      "Iteration: 3225 Loss: nan\n",
      "Iteration: 3226 Loss: nan\n",
      "Iteration: 3227 Loss: nan\n",
      "Iteration: 3228 Loss: nan\n",
      "Iteration: 3229 Loss: nan\n",
      "Iteration: 3230 Loss: nan\n",
      "Iteration: 3231 Loss: nan\n",
      "Iteration: 3232 Loss: nan\n",
      "Iteration: 3233 Loss: nan\n",
      "Iteration: 3234 Loss: nan\n",
      "Iteration: 3235 Loss: nan\n",
      "Iteration: 3236 Loss: nan\n",
      "Iteration: 3237 Loss: nan\n",
      "Iteration: 3238 Loss: nan\n",
      "Iteration: 3239 Loss: nan\n",
      "Iteration: 3240 Loss: nan\n",
      "Iteration: 3241 Loss: nan\n",
      "Iteration: 3242 Loss: nan\n",
      "Iteration: 3243 Loss: nan\n",
      "Iteration: 3244 Loss: nan\n",
      "Iteration: 3245 Loss: nan\n",
      "Iteration: 3246 Loss: nan\n",
      "Iteration: 3247 Loss: nan\n",
      "Iteration: 3248 Loss: nan\n",
      "Iteration: 3249 Loss: nan\n",
      "Iteration: 3250 Loss: nan\n",
      "Iteration: 3251 Loss: nan\n",
      "Iteration: 3252 Loss: nan\n",
      "Iteration: 3253 Loss: nan\n",
      "Iteration: 3254 Loss: nan\n",
      "Iteration: 3255 Loss: nan\n",
      "Iteration: 3256 Loss: nan\n",
      "Iteration: 3257 Loss: nan\n",
      "Iteration: 3258 Loss: nan\n",
      "Iteration: 3259 Loss: nan\n",
      "Iteration: 3260 Loss: nan\n",
      "Iteration: 3261 Loss: nan\n",
      "Iteration: 3262 Loss: nan\n",
      "Iteration: 3263 Loss: nan\n",
      "Iteration: 3264 Loss: nan\n",
      "Iteration: 3265 Loss: nan\n",
      "Iteration: 3266 Loss: nan\n",
      "Iteration: 3267 Loss: nan\n",
      "Iteration: 3268 Loss: nan\n",
      "Iteration: 3269 Loss: nan\n",
      "Iteration: 3270 Loss: nan\n",
      "Iteration: 3271 Loss: nan\n",
      "Iteration: 3272 Loss: nan\n",
      "Iteration: 3273 Loss: nan\n",
      "Iteration: 3274 Loss: nan\n",
      "Iteration: 3275 Loss: nan\n",
      "Iteration: 3276 Loss: nan\n",
      "Iteration: 3277 Loss: nan\n",
      "Iteration: 3278 Loss: nan\n",
      "Iteration: 3279 Loss: nan\n",
      "Iteration: 3280 Loss: nan\n",
      "Iteration: 3281 Loss: nan\n",
      "Iteration: 3282 Loss: nan\n",
      "Iteration: 3283 Loss: nan\n",
      "Iteration: 3284 Loss: nan\n",
      "Iteration: 3285 Loss: nan\n",
      "Iteration: 3286 Loss: nan\n",
      "Iteration: 3287 Loss: nan\n",
      "Iteration: 3288 Loss: nan\n",
      "Iteration: 3289 Loss: nan\n",
      "Iteration: 3290 Loss: nan\n",
      "Iteration: 3291 Loss: nan\n",
      "Iteration: 3292 Loss: nan\n",
      "Iteration: 3293 Loss: nan\n",
      "Iteration: 3294 Loss: nan\n",
      "Iteration: 3295 Loss: nan\n",
      "Iteration: 3296 Loss: nan\n",
      "Iteration: 3297 Loss: nan\n",
      "Iteration: 3298 Loss: nan\n",
      "Iteration: 3299 Loss: nan\n",
      "Iteration: 3300 Loss: nan\n",
      "Iteration: 3301 Loss: nan\n",
      "Iteration: 3302 Loss: nan\n",
      "Iteration: 3303 Loss: nan\n",
      "Iteration: 3304 Loss: nan\n",
      "Iteration: 3305 Loss: nan\n",
      "Iteration: 3306 Loss: nan\n",
      "Iteration: 3307 Loss: nan\n",
      "Iteration: 3308 Loss: nan\n",
      "Iteration: 3309 Loss: nan\n",
      "Iteration: 3310 Loss: nan\n",
      "Iteration: 3311 Loss: nan\n",
      "Iteration: 3312 Loss: nan\n",
      "Iteration: 3313 Loss: nan\n",
      "Iteration: 3314 Loss: nan\n",
      "Iteration: 3315 Loss: nan\n",
      "Iteration: 3316 Loss: nan\n",
      "Iteration: 3317 Loss: nan\n",
      "Iteration: 3318 Loss: nan\n",
      "Iteration: 3319 Loss: nan\n",
      "Iteration: 3320 Loss: nan\n",
      "Iteration: 3321 Loss: nan\n",
      "Iteration: 3322 Loss: nan\n",
      "Iteration: 3323 Loss: nan\n",
      "Iteration: 3324 Loss: nan\n",
      "Iteration: 3325 Loss: nan\n",
      "Iteration: 3326 Loss: nan\n",
      "Iteration: 3327 Loss: nan\n",
      "Iteration: 3328 Loss: nan\n",
      "Iteration: 3329 Loss: nan\n",
      "Iteration: 3330 Loss: nan\n",
      "Iteration: 3331 Loss: nan\n",
      "Iteration: 3332 Loss: nan\n",
      "Iteration: 3333 Loss: nan\n",
      "Iteration: 3334 Loss: nan\n",
      "Iteration: 3335 Loss: nan\n",
      "Iteration: 3336 Loss: nan\n",
      "Iteration: 3337 Loss: nan\n",
      "Iteration: 3338 Loss: nan\n",
      "Iteration: 3339 Loss: nan\n",
      "Iteration: 3340 Loss: nan\n",
      "Iteration: 3341 Loss: nan\n",
      "Iteration: 3342 Loss: nan\n",
      "Iteration: 3343 Loss: nan\n",
      "Iteration: 3344 Loss: nan\n",
      "Iteration: 3345 Loss: nan\n",
      "Iteration: 3346 Loss: nan\n",
      "Iteration: 3347 Loss: nan\n",
      "Iteration: 3348 Loss: nan\n",
      "Iteration: 3349 Loss: nan\n",
      "Iteration: 3350 Loss: nan\n",
      "Iteration: 3351 Loss: nan\n",
      "Iteration: 3352 Loss: nan\n",
      "Iteration: 3353 Loss: nan\n",
      "Iteration: 3354 Loss: nan\n",
      "Iteration: 3355 Loss: nan\n",
      "Iteration: 3356 Loss: nan\n",
      "Iteration: 3357 Loss: nan\n",
      "Iteration: 3358 Loss: nan\n",
      "Iteration: 3359 Loss: nan\n",
      "Iteration: 3360 Loss: nan\n",
      "Iteration: 3361 Loss: nan\n",
      "Iteration: 3362 Loss: nan\n",
      "Iteration: 3363 Loss: nan\n",
      "Iteration: 3364 Loss: nan\n",
      "Iteration: 3365 Loss: nan\n",
      "Iteration: 3366 Loss: nan\n",
      "Iteration: 3367 Loss: nan\n",
      "Iteration: 3368 Loss: nan\n",
      "Iteration: 3369 Loss: nan\n",
      "Iteration: 3370 Loss: nan\n",
      "Iteration: 3371 Loss: nan\n",
      "Iteration: 3372 Loss: nan\n",
      "Iteration: 3373 Loss: nan\n",
      "Iteration: 3374 Loss: nan\n",
      "Iteration: 3375 Loss: nan\n",
      "Iteration: 3376 Loss: nan\n",
      "Iteration: 3377 Loss: nan\n",
      "Iteration: 3378 Loss: nan\n",
      "Iteration: 3379 Loss: nan\n",
      "Iteration: 3380 Loss: nan\n",
      "Iteration: 3381 Loss: nan\n",
      "Iteration: 3382 Loss: nan\n",
      "Iteration: 3383 Loss: nan\n",
      "Iteration: 3384 Loss: nan\n",
      "Iteration: 3385 Loss: nan\n",
      "Iteration: 3386 Loss: nan\n",
      "Iteration: 3387 Loss: nan\n",
      "Iteration: 3388 Loss: nan\n",
      "Iteration: 3389 Loss: nan\n",
      "Iteration: 3390 Loss: nan\n",
      "Iteration: 3391 Loss: nan\n",
      "Iteration: 3392 Loss: nan\n",
      "Iteration: 3393 Loss: nan\n",
      "Iteration: 3394 Loss: nan\n",
      "Iteration: 3395 Loss: nan\n",
      "Iteration: 3396 Loss: nan\n",
      "Iteration: 3397 Loss: nan\n",
      "Iteration: 3398 Loss: nan\n",
      "Iteration: 3399 Loss: nan\n",
      "Iteration: 3400 Loss: nan\n",
      "Iteration: 3401 Loss: nan\n",
      "Iteration: 3402 Loss: nan\n",
      "Iteration: 3403 Loss: nan\n",
      "Iteration: 3404 Loss: nan\n",
      "Iteration: 3405 Loss: nan\n",
      "Iteration: 3406 Loss: nan\n",
      "Iteration: 3407 Loss: nan\n",
      "Iteration: 3408 Loss: nan\n",
      "Iteration: 3409 Loss: nan\n",
      "Iteration: 3410 Loss: nan\n",
      "Iteration: 3411 Loss: nan\n",
      "Iteration: 3412 Loss: nan\n",
      "Iteration: 3413 Loss: nan\n",
      "Iteration: 3414 Loss: nan\n",
      "Iteration: 3415 Loss: nan\n",
      "Iteration: 3416 Loss: nan\n",
      "Iteration: 3417 Loss: nan\n",
      "Iteration: 3418 Loss: nan\n",
      "Iteration: 3419 Loss: nan\n",
      "Iteration: 3420 Loss: nan\n",
      "Iteration: 3421 Loss: nan\n",
      "Iteration: 3422 Loss: nan\n",
      "Iteration: 3423 Loss: nan\n",
      "Iteration: 3424 Loss: nan\n",
      "Iteration: 3425 Loss: nan\n",
      "Iteration: 3426 Loss: nan\n",
      "Iteration: 3427 Loss: nan\n",
      "Iteration: 3428 Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3429 Loss: nan\n",
      "Iteration: 3430 Loss: nan\n",
      "Iteration: 3431 Loss: nan\n",
      "Iteration: 3432 Loss: nan\n",
      "Iteration: 3433 Loss: nan\n",
      "Iteration: 3434 Loss: nan\n",
      "Iteration: 3435 Loss: nan\n",
      "Iteration: 3436 Loss: nan\n",
      "Iteration: 3437 Loss: nan\n",
      "Iteration: 3438 Loss: nan\n",
      "Iteration: 3439 Loss: nan\n",
      "Iteration: 3440 Loss: nan\n",
      "Iteration: 3441 Loss: nan\n",
      "Iteration: 3442 Loss: nan\n",
      "Iteration: 3443 Loss: nan\n",
      "Iteration: 3444 Loss: nan\n",
      "Iteration: 3445 Loss: nan\n",
      "Iteration: 3446 Loss: nan\n",
      "Iteration: 3447 Loss: nan\n",
      "Iteration: 3448 Loss: nan\n",
      "Iteration: 3449 Loss: nan\n",
      "Iteration: 3450 Loss: nan\n",
      "Iteration: 3451 Loss: nan\n",
      "Iteration: 3452 Loss: nan\n",
      "Iteration: 3453 Loss: nan\n",
      "Iteration: 3454 Loss: nan\n",
      "Iteration: 3455 Loss: nan\n",
      "Iteration: 3456 Loss: nan\n",
      "Iteration: 3457 Loss: nan\n",
      "Iteration: 3458 Loss: nan\n",
      "Iteration: 3459 Loss: nan\n",
      "Iteration: 3460 Loss: nan\n",
      "Iteration: 3461 Loss: nan\n",
      "Iteration: 3462 Loss: nan\n",
      "Iteration: 3463 Loss: nan\n",
      "Iteration: 3464 Loss: nan\n",
      "Iteration: 3465 Loss: nan\n",
      "Iteration: 3466 Loss: nan\n",
      "Iteration: 3467 Loss: nan\n",
      "Iteration: 3468 Loss: nan\n",
      "Iteration: 3469 Loss: nan\n",
      "Iteration: 3470 Loss: nan\n",
      "Iteration: 3471 Loss: nan\n",
      "Iteration: 3472 Loss: nan\n",
      "Iteration: 3473 Loss: nan\n",
      "Iteration: 3474 Loss: nan\n",
      "Iteration: 3475 Loss: nan\n",
      "Iteration: 3476 Loss: nan\n",
      "Iteration: 3477 Loss: nan\n",
      "Iteration: 3478 Loss: nan\n",
      "Iteration: 3479 Loss: nan\n",
      "Iteration: 3480 Loss: nan\n",
      "Iteration: 3481 Loss: nan\n",
      "Iteration: 3482 Loss: nan\n",
      "Iteration: 3483 Loss: nan\n",
      "Iteration: 3484 Loss: nan\n",
      "Iteration: 3485 Loss: nan\n",
      "Iteration: 3486 Loss: nan\n",
      "Iteration: 3487 Loss: nan\n",
      "Iteration: 3488 Loss: nan\n",
      "Iteration: 3489 Loss: nan\n",
      "Iteration: 3490 Loss: nan\n",
      "Iteration: 3491 Loss: nan\n",
      "Iteration: 3492 Loss: nan\n",
      "Iteration: 3493 Loss: nan\n",
      "Iteration: 3494 Loss: nan\n",
      "Iteration: 3495 Loss: nan\n",
      "Iteration: 3496 Loss: nan\n",
      "Iteration: 3497 Loss: nan\n",
      "Iteration: 3498 Loss: nan\n",
      "Iteration: 3499 Loss: nan\n",
      "Iteration: 3500 Loss: nan\n",
      "Iteration: 3501 Loss: nan\n",
      "Iteration: 3502 Loss: nan\n",
      "Iteration: 3503 Loss: nan\n",
      "Iteration: 3504 Loss: nan\n",
      "Iteration: 3505 Loss: nan\n",
      "Iteration: 3506 Loss: nan\n",
      "Iteration: 3507 Loss: nan\n",
      "Iteration: 3508 Loss: nan\n",
      "Iteration: 3509 Loss: nan\n",
      "Iteration: 3510 Loss: nan\n",
      "Iteration: 3511 Loss: nan\n",
      "Iteration: 3512 Loss: nan\n",
      "Iteration: 3513 Loss: nan\n",
      "Iteration: 3514 Loss: nan\n",
      "Iteration: 3515 Loss: nan\n",
      "Iteration: 3516 Loss: nan\n",
      "Iteration: 3517 Loss: nan\n",
      "Iteration: 3518 Loss: nan\n",
      "Iteration: 3519 Loss: nan\n",
      "Iteration: 3520 Loss: nan\n",
      "Iteration: 3521 Loss: nan\n",
      "Iteration: 3522 Loss: nan\n",
      "Iteration: 3523 Loss: nan\n",
      "Iteration: 3524 Loss: nan\n",
      "Iteration: 3525 Loss: nan\n",
      "Iteration: 3526 Loss: nan\n",
      "Iteration: 3527 Loss: nan\n",
      "Iteration: 3528 Loss: nan\n",
      "Iteration: 3529 Loss: nan\n",
      "Iteration: 3530 Loss: nan\n",
      "Iteration: 3531 Loss: nan\n",
      "Iteration: 3532 Loss: nan\n",
      "Iteration: 3533 Loss: nan\n",
      "Iteration: 3534 Loss: nan\n",
      "Iteration: 3535 Loss: nan\n",
      "Iteration: 3536 Loss: nan\n",
      "Iteration: 3537 Loss: nan\n",
      "Iteration: 3538 Loss: nan\n",
      "Iteration: 3539 Loss: nan\n",
      "Iteration: 3540 Loss: nan\n",
      "Iteration: 3541 Loss: nan\n",
      "Iteration: 3542 Loss: nan\n",
      "Iteration: 3543 Loss: nan\n",
      "Iteration: 3544 Loss: nan\n",
      "Iteration: 3545 Loss: nan\n",
      "Iteration: 3546 Loss: nan\n",
      "Iteration: 3547 Loss: nan\n",
      "Iteration: 3548 Loss: nan\n",
      "Iteration: 3549 Loss: nan\n",
      "Iteration: 3550 Loss: nan\n",
      "Iteration: 3551 Loss: nan\n",
      "Iteration: 3552 Loss: nan\n",
      "Iteration: 3553 Loss: nan\n",
      "Iteration: 3554 Loss: nan\n",
      "Iteration: 3555 Loss: nan\n",
      "Iteration: 3556 Loss: nan\n",
      "Iteration: 3557 Loss: nan\n",
      "Iteration: 3558 Loss: nan\n",
      "Iteration: 3559 Loss: nan\n",
      "Iteration: 3560 Loss: nan\n",
      "Iteration: 3561 Loss: nan\n",
      "Iteration: 3562 Loss: nan\n",
      "Iteration: 3563 Loss: nan\n",
      "Iteration: 3564 Loss: nan\n",
      "Iteration: 3565 Loss: nan\n",
      "Iteration: 3566 Loss: nan\n",
      "Iteration: 3567 Loss: nan\n",
      "Iteration: 3568 Loss: nan\n",
      "Iteration: 3569 Loss: nan\n",
      "Iteration: 3570 Loss: nan\n",
      "Iteration: 3571 Loss: nan\n",
      "Iteration: 3572 Loss: nan\n",
      "Iteration: 3573 Loss: nan\n",
      "Iteration: 3574 Loss: nan\n",
      "Iteration: 3575 Loss: nan\n",
      "Iteration: 3576 Loss: nan\n",
      "Iteration: 3577 Loss: nan\n",
      "Iteration: 3578 Loss: nan\n",
      "Iteration: 3579 Loss: nan\n",
      "Iteration: 3580 Loss: nan\n",
      "Iteration: 3581 Loss: nan\n",
      "Iteration: 3582 Loss: nan\n",
      "Iteration: 3583 Loss: nan\n",
      "Iteration: 3584 Loss: nan\n",
      "Iteration: 3585 Loss: nan\n",
      "Iteration: 3586 Loss: nan\n",
      "Iteration: 3587 Loss: nan\n",
      "Iteration: 3588 Loss: nan\n",
      "Iteration: 3589 Loss: nan\n",
      "Iteration: 3590 Loss: nan\n",
      "Iteration: 3591 Loss: nan\n",
      "Iteration: 3592 Loss: nan\n",
      "Iteration: 3593 Loss: nan\n",
      "Iteration: 3594 Loss: nan\n",
      "Iteration: 3595 Loss: nan\n",
      "Iteration: 3596 Loss: nan\n",
      "Iteration: 3597 Loss: nan\n",
      "Iteration: 3598 Loss: nan\n",
      "Iteration: 3599 Loss: nan\n",
      "Iteration: 3600 Loss: nan\n",
      "Iteration: 3601 Loss: nan\n",
      "Iteration: 3602 Loss: nan\n",
      "Iteration: 3603 Loss: nan\n",
      "Iteration: 3604 Loss: nan\n",
      "Iteration: 3605 Loss: nan\n",
      "Iteration: 3606 Loss: nan\n",
      "Iteration: 3607 Loss: nan\n",
      "Iteration: 3608 Loss: nan\n",
      "Iteration: 3609 Loss: nan\n",
      "Iteration: 3610 Loss: nan\n",
      "Iteration: 3611 Loss: nan\n",
      "Iteration: 3612 Loss: nan\n",
      "Iteration: 3613 Loss: nan\n",
      "Iteration: 3614 Loss: nan\n",
      "Iteration: 3615 Loss: nan\n",
      "Iteration: 3616 Loss: nan\n",
      "Iteration: 3617 Loss: nan\n",
      "Iteration: 3618 Loss: nan\n",
      "Iteration: 3619 Loss: nan\n",
      "Iteration: 3620 Loss: nan\n",
      "Iteration: 3621 Loss: nan\n",
      "Iteration: 3622 Loss: nan\n",
      "Iteration: 3623 Loss: nan\n",
      "Iteration: 3624 Loss: nan\n",
      "Iteration: 3625 Loss: nan\n",
      "Iteration: 3626 Loss: nan\n",
      "Iteration: 3627 Loss: nan\n",
      "Iteration: 3628 Loss: nan\n",
      "Iteration: 3629 Loss: nan\n",
      "Iteration: 3630 Loss: nan\n",
      "Iteration: 3631 Loss: nan\n",
      "Iteration: 3632 Loss: nan\n",
      "Iteration: 3633 Loss: nan\n",
      "Iteration: 3634 Loss: nan\n",
      "Iteration: 3635 Loss: nan\n",
      "Iteration: 3636 Loss: nan\n",
      "Iteration: 3637 Loss: nan\n",
      "Iteration: 3638 Loss: nan\n",
      "Iteration: 3639 Loss: nan\n",
      "Iteration: 3640 Loss: nan\n",
      "Iteration: 3641 Loss: nan\n",
      "Iteration: 3642 Loss: nan\n",
      "Iteration: 3643 Loss: nan\n",
      "Iteration: 3644 Loss: nan\n",
      "Iteration: 3645 Loss: nan\n",
      "Iteration: 3646 Loss: nan\n",
      "Iteration: 3647 Loss: nan\n",
      "Iteration: 3648 Loss: nan\n",
      "Iteration: 3649 Loss: nan\n",
      "Iteration: 3650 Loss: nan\n",
      "Iteration: 3651 Loss: nan\n",
      "Iteration: 3652 Loss: nan\n",
      "Iteration: 3653 Loss: nan\n",
      "Iteration: 3654 Loss: nan\n",
      "Iteration: 3655 Loss: nan\n",
      "Iteration: 3656 Loss: nan\n",
      "Iteration: 3657 Loss: nan\n",
      "Iteration: 3658 Loss: nan\n",
      "Iteration: 3659 Loss: nan\n",
      "Iteration: 3660 Loss: nan\n",
      "Iteration: 3661 Loss: nan\n",
      "Iteration: 3662 Loss: nan\n",
      "Iteration: 3663 Loss: nan\n",
      "Iteration: 3664 Loss: nan\n",
      "Iteration: 3665 Loss: nan\n",
      "Iteration: 3666 Loss: nan\n",
      "Iteration: 3667 Loss: nan\n",
      "Iteration: 3668 Loss: nan\n",
      "Iteration: 3669 Loss: nan\n",
      "Iteration: 3670 Loss: nan\n",
      "Iteration: 3671 Loss: nan\n",
      "Iteration: 3672 Loss: nan\n",
      "Iteration: 3673 Loss: nan\n",
      "Iteration: 3674 Loss: nan\n",
      "Iteration: 3675 Loss: nan\n",
      "Iteration: 3676 Loss: nan\n",
      "Iteration: 3677 Loss: nan\n",
      "Iteration: 3678 Loss: nan\n",
      "Iteration: 3679 Loss: nan\n",
      "Iteration: 3680 Loss: nan\n",
      "Iteration: 3681 Loss: nan\n",
      "Iteration: 3682 Loss: nan\n",
      "Iteration: 3683 Loss: nan\n",
      "Iteration: 3684 Loss: nan\n",
      "Iteration: 3685 Loss: nan\n",
      "Iteration: 3686 Loss: nan\n",
      "Iteration: 3687 Loss: nan\n",
      "Iteration: 3688 Loss: nan\n",
      "Iteration: 3689 Loss: nan\n",
      "Iteration: 3690 Loss: nan\n",
      "Iteration: 3691 Loss: nan\n",
      "Iteration: 3692 Loss: nan\n",
      "Iteration: 3693 Loss: nan\n",
      "Iteration: 3694 Loss: nan\n",
      "Iteration: 3695 Loss: nan\n",
      "Iteration: 3696 Loss: nan\n",
      "Iteration: 3697 Loss: nan\n",
      "Iteration: 3698 Loss: nan\n",
      "Iteration: 3699 Loss: nan\n",
      "Iteration: 3700 Loss: nan\n",
      "Iteration: 3701 Loss: nan\n",
      "Iteration: 3702 Loss: nan\n",
      "Iteration: 3703 Loss: nan\n",
      "Iteration: 3704 Loss: nan\n",
      "Iteration: 3705 Loss: nan\n",
      "Iteration: 3706 Loss: nan\n",
      "Iteration: 3707 Loss: nan\n",
      "Iteration: 3708 Loss: nan\n",
      "Iteration: 3709 Loss: nan\n",
      "Iteration: 3710 Loss: nan\n",
      "Iteration: 3711 Loss: nan\n",
      "Iteration: 3712 Loss: nan\n",
      "Iteration: 3713 Loss: nan\n",
      "Iteration: 3714 Loss: nan\n",
      "Iteration: 3715 Loss: nan\n",
      "Iteration: 3716 Loss: nan\n",
      "Iteration: 3717 Loss: nan\n",
      "Iteration: 3718 Loss: nan\n",
      "Iteration: 3719 Loss: nan\n",
      "Iteration: 3720 Loss: nan\n",
      "Iteration: 3721 Loss: nan\n",
      "Iteration: 3722 Loss: nan\n",
      "Iteration: 3723 Loss: nan\n",
      "Iteration: 3724 Loss: nan\n",
      "Iteration: 3725 Loss: nan\n",
      "Iteration: 3726 Loss: nan\n",
      "Iteration: 3727 Loss: nan\n",
      "Iteration: 3728 Loss: nan\n",
      "Iteration: 3729 Loss: nan\n",
      "Iteration: 3730 Loss: nan\n",
      "Iteration: 3731 Loss: nan\n",
      "Iteration: 3732 Loss: nan\n",
      "Iteration: 3733 Loss: nan\n",
      "Iteration: 3734 Loss: nan\n",
      "Iteration: 3735 Loss: nan\n",
      "Iteration: 3736 Loss: nan\n",
      "Iteration: 3737 Loss: nan\n",
      "Iteration: 3738 Loss: nan\n",
      "Iteration: 3739 Loss: nan\n",
      "Iteration: 3740 Loss: nan\n",
      "Iteration: 3741 Loss: nan\n",
      "Iteration: 3742 Loss: nan\n",
      "Iteration: 3743 Loss: nan\n",
      "Iteration: 3744 Loss: nan\n",
      "Iteration: 3745 Loss: nan\n",
      "Iteration: 3746 Loss: nan\n",
      "Iteration: 3747 Loss: nan\n",
      "Iteration: 3748 Loss: nan\n",
      "Iteration: 3749 Loss: nan\n",
      "Iteration: 3750 Loss: nan\n",
      "Iteration: 3751 Loss: nan\n",
      "Iteration: 3752 Loss: nan\n",
      "Iteration: 3753 Loss: nan\n",
      "Iteration: 3754 Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3755 Loss: nan\n",
      "Iteration: 3756 Loss: nan\n",
      "Iteration: 3757 Loss: nan\n",
      "Iteration: 3758 Loss: nan\n",
      "Iteration: 3759 Loss: nan\n",
      "Iteration: 3760 Loss: nan\n",
      "Iteration: 3761 Loss: nan\n",
      "Iteration: 3762 Loss: nan\n",
      "Iteration: 3763 Loss: nan\n",
      "Iteration: 3764 Loss: nan\n",
      "Iteration: 3765 Loss: nan\n",
      "Iteration: 3766 Loss: nan\n",
      "Iteration: 3767 Loss: nan\n",
      "Iteration: 3768 Loss: nan\n",
      "Iteration: 3769 Loss: nan\n",
      "Iteration: 3770 Loss: nan\n",
      "Iteration: 3771 Loss: nan\n",
      "Iteration: 3772 Loss: nan\n",
      "Iteration: 3773 Loss: nan\n",
      "Iteration: 3774 Loss: nan\n",
      "Iteration: 3775 Loss: nan\n",
      "Iteration: 3776 Loss: nan\n",
      "Iteration: 3777 Loss: nan\n",
      "Iteration: 3778 Loss: nan\n",
      "Iteration: 3779 Loss: nan\n",
      "Iteration: 3780 Loss: nan\n",
      "Iteration: 3781 Loss: nan\n",
      "Iteration: 3782 Loss: nan\n",
      "Iteration: 3783 Loss: nan\n",
      "Iteration: 3784 Loss: nan\n",
      "Iteration: 3785 Loss: nan\n",
      "Iteration: 3786 Loss: nan\n",
      "Iteration: 3787 Loss: nan\n",
      "Iteration: 3788 Loss: nan\n",
      "Iteration: 3789 Loss: nan\n",
      "Iteration: 3790 Loss: nan\n",
      "Iteration: 3791 Loss: nan\n",
      "Iteration: 3792 Loss: nan\n",
      "Iteration: 3793 Loss: nan\n",
      "Iteration: 3794 Loss: nan\n",
      "Iteration: 3795 Loss: nan\n",
      "Iteration: 3796 Loss: nan\n",
      "Iteration: 3797 Loss: nan\n",
      "Iteration: 3798 Loss: nan\n",
      "Iteration: 3799 Loss: nan\n",
      "Iteration: 3800 Loss: nan\n",
      "Iteration: 3801 Loss: nan\n",
      "Iteration: 3802 Loss: nan\n",
      "Iteration: 3803 Loss: nan\n",
      "Iteration: 3804 Loss: nan\n",
      "Iteration: 3805 Loss: nan\n",
      "Iteration: 3806 Loss: nan\n",
      "Iteration: 3807 Loss: nan\n",
      "Iteration: 3808 Loss: nan\n",
      "Iteration: 3809 Loss: nan\n",
      "Iteration: 3810 Loss: nan\n",
      "Iteration: 3811 Loss: nan\n",
      "Iteration: 3812 Loss: nan\n",
      "Iteration: 3813 Loss: nan\n",
      "Iteration: 3814 Loss: nan\n",
      "Iteration: 3815 Loss: nan\n",
      "Iteration: 3816 Loss: nan\n",
      "Iteration: 3817 Loss: nan\n",
      "Iteration: 3818 Loss: nan\n",
      "Iteration: 3819 Loss: nan\n",
      "Iteration: 3820 Loss: nan\n",
      "Iteration: 3821 Loss: nan\n",
      "Iteration: 3822 Loss: nan\n",
      "Iteration: 3823 Loss: nan\n",
      "Iteration: 3824 Loss: nan\n",
      "Iteration: 3825 Loss: nan\n",
      "Iteration: 3826 Loss: nan\n",
      "Iteration: 3827 Loss: nan\n",
      "Iteration: 3828 Loss: nan\n",
      "Iteration: 3829 Loss: nan\n",
      "Iteration: 3830 Loss: nan\n",
      "Iteration: 3831 Loss: nan\n",
      "Iteration: 3832 Loss: nan\n",
      "Iteration: 3833 Loss: nan\n",
      "Iteration: 3834 Loss: nan\n",
      "Iteration: 3835 Loss: nan\n",
      "Iteration: 3836 Loss: nan\n",
      "Iteration: 3837 Loss: nan\n",
      "Iteration: 3838 Loss: nan\n",
      "Iteration: 3839 Loss: nan\n",
      "Iteration: 3840 Loss: nan\n",
      "Iteration: 3841 Loss: nan\n",
      "Iteration: 3842 Loss: nan\n",
      "Iteration: 3843 Loss: nan\n",
      "Iteration: 3844 Loss: nan\n",
      "Iteration: 3845 Loss: nan\n",
      "Iteration: 3846 Loss: nan\n",
      "Iteration: 3847 Loss: nan\n",
      "Iteration: 3848 Loss: nan\n",
      "Iteration: 3849 Loss: nan\n",
      "Iteration: 3850 Loss: nan\n",
      "Iteration: 3851 Loss: nan\n",
      "Iteration: 3852 Loss: nan\n",
      "Iteration: 3853 Loss: nan\n",
      "Iteration: 3854 Loss: nan\n",
      "Iteration: 3855 Loss: nan\n",
      "Iteration: 3856 Loss: nan\n",
      "Iteration: 3857 Loss: nan\n",
      "Iteration: 3858 Loss: nan\n",
      "Iteration: 3859 Loss: nan\n",
      "Iteration: 3860 Loss: nan\n",
      "Iteration: 3861 Loss: nan\n",
      "Iteration: 3862 Loss: nan\n",
      "Iteration: 3863 Loss: nan\n",
      "Iteration: 3864 Loss: nan\n",
      "Iteration: 3865 Loss: nan\n",
      "Iteration: 3866 Loss: nan\n",
      "Iteration: 3867 Loss: nan\n",
      "Iteration: 3868 Loss: nan\n",
      "Iteration: 3869 Loss: nan\n",
      "Iteration: 3870 Loss: nan\n",
      "Iteration: 3871 Loss: nan\n",
      "Iteration: 3872 Loss: nan\n",
      "Iteration: 3873 Loss: nan\n",
      "Iteration: 3874 Loss: nan\n",
      "Iteration: 3875 Loss: nan\n",
      "Iteration: 3876 Loss: nan\n",
      "Iteration: 3877 Loss: nan\n",
      "Iteration: 3878 Loss: nan\n",
      "Iteration: 3879 Loss: nan\n",
      "Iteration: 3880 Loss: nan\n",
      "Iteration: 3881 Loss: nan\n",
      "Iteration: 3882 Loss: nan\n",
      "Iteration: 3883 Loss: nan\n",
      "Iteration: 3884 Loss: nan\n",
      "Iteration: 3885 Loss: nan\n",
      "Iteration: 3886 Loss: nan\n",
      "Iteration: 3887 Loss: nan\n",
      "Iteration: 3888 Loss: nan\n",
      "Iteration: 3889 Loss: nan\n",
      "Iteration: 3890 Loss: nan\n",
      "Iteration: 3891 Loss: nan\n",
      "Iteration: 3892 Loss: nan\n",
      "Iteration: 3893 Loss: nan\n",
      "Iteration: 3894 Loss: nan\n",
      "Iteration: 3895 Loss: nan\n",
      "Iteration: 3896 Loss: nan\n",
      "Iteration: 3897 Loss: nan\n",
      "Iteration: 3898 Loss: nan\n",
      "Iteration: 3899 Loss: nan\n",
      "Iteration: 3900 Loss: nan\n",
      "Iteration: 3901 Loss: nan\n",
      "Iteration: 3902 Loss: nan\n",
      "Iteration: 3903 Loss: nan\n",
      "Iteration: 3904 Loss: nan\n",
      "Iteration: 3905 Loss: nan\n",
      "Iteration: 3906 Loss: nan\n",
      "Iteration: 3907 Loss: nan\n",
      "Iteration: 3908 Loss: nan\n",
      "Iteration: 3909 Loss: nan\n",
      "Iteration: 3910 Loss: nan\n",
      "Iteration: 3911 Loss: nan\n",
      "Iteration: 3912 Loss: nan\n",
      "Iteration: 3913 Loss: nan\n",
      "Iteration: 3914 Loss: nan\n",
      "Iteration: 3915 Loss: nan\n",
      "Iteration: 3916 Loss: nan\n",
      "Iteration: 3917 Loss: nan\n",
      "Iteration: 3918 Loss: nan\n",
      "Iteration: 3919 Loss: nan\n",
      "Iteration: 3920 Loss: nan\n",
      "Iteration: 3921 Loss: nan\n",
      "Iteration: 3922 Loss: nan\n",
      "Iteration: 3923 Loss: nan\n",
      "Iteration: 3924 Loss: nan\n",
      "Iteration: 3925 Loss: nan\n",
      "Iteration: 3926 Loss: nan\n",
      "Iteration: 3927 Loss: nan\n",
      "Iteration: 3928 Loss: nan\n",
      "Iteration: 3929 Loss: nan\n",
      "Iteration: 3930 Loss: nan\n",
      "Iteration: 3931 Loss: nan\n",
      "Iteration: 3932 Loss: nan\n",
      "Iteration: 3933 Loss: nan\n",
      "Iteration: 3934 Loss: nan\n",
      "Iteration: 3935 Loss: nan\n",
      "Iteration: 3936 Loss: nan\n",
      "Iteration: 3937 Loss: nan\n",
      "Iteration: 3938 Loss: nan\n",
      "Iteration: 3939 Loss: nan\n",
      "Iteration: 3940 Loss: nan\n",
      "Iteration: 3941 Loss: nan\n",
      "Iteration: 3942 Loss: nan\n",
      "Iteration: 3943 Loss: nan\n",
      "Iteration: 3944 Loss: nan\n",
      "Iteration: 3945 Loss: nan\n",
      "Iteration: 3946 Loss: nan\n",
      "Iteration: 3947 Loss: nan\n",
      "Iteration: 3948 Loss: nan\n",
      "Iteration: 3949 Loss: nan\n",
      "Iteration: 3950 Loss: nan\n",
      "Iteration: 3951 Loss: nan\n",
      "Iteration: 3952 Loss: nan\n",
      "Iteration: 3953 Loss: nan\n",
      "Iteration: 3954 Loss: nan\n",
      "Iteration: 3955 Loss: nan\n",
      "Iteration: 3956 Loss: nan\n",
      "Iteration: 3957 Loss: nan\n",
      "Iteration: 3958 Loss: nan\n",
      "Iteration: 3959 Loss: nan\n",
      "Iteration: 3960 Loss: nan\n",
      "Iteration: 3961 Loss: nan\n",
      "Iteration: 3962 Loss: nan\n",
      "Iteration: 3963 Loss: nan\n",
      "Iteration: 3964 Loss: nan\n",
      "Iteration: 3965 Loss: nan\n",
      "Iteration: 3966 Loss: nan\n",
      "Iteration: 3967 Loss: nan\n",
      "Iteration: 3968 Loss: nan\n",
      "Iteration: 3969 Loss: nan\n",
      "Iteration: 3970 Loss: nan\n",
      "Iteration: 3971 Loss: nan\n",
      "Iteration: 3972 Loss: nan\n",
      "Iteration: 3973 Loss: nan\n",
      "Iteration: 3974 Loss: nan\n",
      "Iteration: 3975 Loss: nan\n",
      "Iteration: 3976 Loss: nan\n",
      "Iteration: 3977 Loss: nan\n",
      "Iteration: 3978 Loss: nan\n",
      "Iteration: 3979 Loss: nan\n",
      "Iteration: 3980 Loss: nan\n",
      "Iteration: 3981 Loss: nan\n",
      "Iteration: 3982 Loss: nan\n",
      "Iteration: 3983 Loss: nan\n",
      "Iteration: 3984 Loss: nan\n",
      "Iteration: 3985 Loss: nan\n",
      "Iteration: 3986 Loss: nan\n",
      "Iteration: 3987 Loss: nan\n",
      "Iteration: 3988 Loss: nan\n",
      "Iteration: 3989 Loss: nan\n",
      "Iteration: 3990 Loss: nan\n",
      "Iteration: 3991 Loss: nan\n",
      "Iteration: 3992 Loss: nan\n",
      "Iteration: 3993 Loss: nan\n",
      "Iteration: 3994 Loss: nan\n",
      "Iteration: 3995 Loss: nan\n",
      "Iteration: 3996 Loss: nan\n",
      "Iteration: 3997 Loss: nan\n",
      "Iteration: 3998 Loss: nan\n",
      "Iteration: 3999 Loss: nan\n",
      "Iteration: 4000 Loss: nan\n",
      "Iteration: 4001 Loss: nan\n",
      "Iteration: 4002 Loss: nan\n",
      "Iteration: 4003 Loss: nan\n",
      "Iteration: 4004 Loss: nan\n",
      "Iteration: 4005 Loss: nan\n",
      "Iteration: 4006 Loss: nan\n",
      "Iteration: 4007 Loss: nan\n",
      "Iteration: 4008 Loss: nan\n",
      "Iteration: 4009 Loss: nan\n",
      "Iteration: 4010 Loss: nan\n",
      "Iteration: 4011 Loss: nan\n",
      "Iteration: 4012 Loss: nan\n",
      "Iteration: 4013 Loss: nan\n",
      "Iteration: 4014 Loss: nan\n",
      "Iteration: 4015 Loss: nan\n",
      "Iteration: 4016 Loss: nan\n",
      "Iteration: 4017 Loss: nan\n",
      "Iteration: 4018 Loss: nan\n",
      "Iteration: 4019 Loss: nan\n",
      "Iteration: 4020 Loss: nan\n",
      "Iteration: 4021 Loss: nan\n",
      "Iteration: 4022 Loss: nan\n",
      "Iteration: 4023 Loss: nan\n",
      "Iteration: 4024 Loss: nan\n",
      "Iteration: 4025 Loss: nan\n",
      "Iteration: 4026 Loss: nan\n",
      "Iteration: 4027 Loss: nan\n",
      "Iteration: 4028 Loss: nan\n",
      "Iteration: 4029 Loss: nan\n",
      "Iteration: 4030 Loss: nan\n",
      "Iteration: 4031 Loss: nan\n",
      "Iteration: 4032 Loss: nan\n",
      "Iteration: 4033 Loss: nan\n",
      "Iteration: 4034 Loss: nan\n",
      "Iteration: 4035 Loss: nan\n",
      "Iteration: 4036 Loss: nan\n",
      "Iteration: 4037 Loss: nan\n",
      "Iteration: 4038 Loss: nan\n",
      "Iteration: 4039 Loss: nan\n",
      "Iteration: 4040 Loss: nan\n",
      "Iteration: 4041 Loss: nan\n",
      "Iteration: 4042 Loss: nan\n",
      "Iteration: 4043 Loss: nan\n",
      "Iteration: 4044 Loss: nan\n",
      "Iteration: 4045 Loss: nan\n",
      "Iteration: 4046 Loss: nan\n",
      "Iteration: 4047 Loss: nan\n",
      "Iteration: 4048 Loss: nan\n",
      "Iteration: 4049 Loss: nan\n",
      "Iteration: 4050 Loss: nan\n",
      "Iteration: 4051 Loss: nan\n",
      "Iteration: 4052 Loss: nan\n",
      "Iteration: 4053 Loss: nan\n",
      "Iteration: 4054 Loss: nan\n",
      "Iteration: 4055 Loss: nan\n",
      "Iteration: 4056 Loss: nan\n",
      "Iteration: 4057 Loss: nan\n",
      "Iteration: 4058 Loss: nan\n",
      "Iteration: 4059 Loss: nan\n",
      "Iteration: 4060 Loss: nan\n",
      "Iteration: 4061 Loss: nan\n",
      "Iteration: 4062 Loss: nan\n",
      "Iteration: 4063 Loss: nan\n",
      "Iteration: 4064 Loss: nan\n",
      "Iteration: 4065 Loss: nan\n",
      "Iteration: 4066 Loss: nan\n",
      "Iteration: 4067 Loss: nan\n",
      "Iteration: 4068 Loss: nan\n",
      "Iteration: 4069 Loss: nan\n",
      "Iteration: 4070 Loss: nan\n",
      "Iteration: 4071 Loss: nan\n",
      "Iteration: 4072 Loss: nan\n",
      "Iteration: 4073 Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4074 Loss: nan\n",
      "Iteration: 4075 Loss: nan\n",
      "Iteration: 4076 Loss: nan\n",
      "Iteration: 4077 Loss: nan\n",
      "Iteration: 4078 Loss: nan\n",
      "Iteration: 4079 Loss: nan\n",
      "Iteration: 4080 Loss: nan\n",
      "Iteration: 4081 Loss: nan\n",
      "Iteration: 4082 Loss: nan\n",
      "Iteration: 4083 Loss: nan\n",
      "Iteration: 4084 Loss: nan\n",
      "Iteration: 4085 Loss: nan\n",
      "Iteration: 4086 Loss: nan\n",
      "Iteration: 4087 Loss: nan\n",
      "Iteration: 4088 Loss: nan\n",
      "Iteration: 4089 Loss: nan\n",
      "Iteration: 4090 Loss: nan\n",
      "Iteration: 4091 Loss: nan\n",
      "Iteration: 4092 Loss: nan\n",
      "Iteration: 4093 Loss: nan\n",
      "Iteration: 4094 Loss: nan\n",
      "Iteration: 4095 Loss: nan\n",
      "Iteration: 4096 Loss: nan\n",
      "Iteration: 4097 Loss: nan\n",
      "Iteration: 4098 Loss: nan\n",
      "Iteration: 4099 Loss: nan\n",
      "Iteration: 4100 Loss: nan\n",
      "Iteration: 4101 Loss: nan\n",
      "Iteration: 4102 Loss: nan\n",
      "Iteration: 4103 Loss: nan\n",
      "Iteration: 4104 Loss: nan\n",
      "Iteration: 4105 Loss: nan\n",
      "Iteration: 4106 Loss: nan\n",
      "Iteration: 4107 Loss: nan\n",
      "Iteration: 4108 Loss: nan\n",
      "Iteration: 4109 Loss: nan\n",
      "Iteration: 4110 Loss: nan\n",
      "Iteration: 4111 Loss: nan\n",
      "Iteration: 4112 Loss: nan\n",
      "Iteration: 4113 Loss: nan\n",
      "Iteration: 4114 Loss: nan\n",
      "Iteration: 4115 Loss: nan\n",
      "Iteration: 4116 Loss: nan\n",
      "Iteration: 4117 Loss: nan\n",
      "Iteration: 4118 Loss: nan\n",
      "Iteration: 4119 Loss: nan\n",
      "Iteration: 4120 Loss: nan\n",
      "Iteration: 4121 Loss: nan\n",
      "Iteration: 4122 Loss: nan\n",
      "Iteration: 4123 Loss: nan\n",
      "Iteration: 4124 Loss: nan\n",
      "Iteration: 4125 Loss: nan\n",
      "Iteration: 4126 Loss: nan\n",
      "Iteration: 4127 Loss: nan\n",
      "Iteration: 4128 Loss: nan\n",
      "Iteration: 4129 Loss: nan\n",
      "Iteration: 4130 Loss: nan\n",
      "Iteration: 4131 Loss: nan\n",
      "Iteration: 4132 Loss: nan\n",
      "Iteration: 4133 Loss: nan\n",
      "Iteration: 4134 Loss: nan\n",
      "Iteration: 4135 Loss: nan\n",
      "Iteration: 4136 Loss: nan\n",
      "Iteration: 4137 Loss: nan\n",
      "Iteration: 4138 Loss: nan\n",
      "Iteration: 4139 Loss: nan\n",
      "Iteration: 4140 Loss: nan\n",
      "Iteration: 4141 Loss: nan\n",
      "Iteration: 4142 Loss: nan\n",
      "Iteration: 4143 Loss: nan\n",
      "Iteration: 4144 Loss: nan\n",
      "Iteration: 4145 Loss: nan\n",
      "Iteration: 4146 Loss: nan\n",
      "Iteration: 4147 Loss: nan\n",
      "Iteration: 4148 Loss: nan\n",
      "Iteration: 4149 Loss: nan\n",
      "Iteration: 4150 Loss: nan\n",
      "Iteration: 4151 Loss: nan\n",
      "Iteration: 4152 Loss: nan\n",
      "Iteration: 4153 Loss: nan\n",
      "Iteration: 4154 Loss: nan\n",
      "Iteration: 4155 Loss: nan\n",
      "Iteration: 4156 Loss: nan\n",
      "Iteration: 4157 Loss: nan\n",
      "Iteration: 4158 Loss: nan\n",
      "Iteration: 4159 Loss: nan\n",
      "Iteration: 4160 Loss: nan\n",
      "Iteration: 4161 Loss: nan\n",
      "Iteration: 4162 Loss: nan\n",
      "Iteration: 4163 Loss: nan\n",
      "Iteration: 4164 Loss: nan\n",
      "Iteration: 4165 Loss: nan\n",
      "Iteration: 4166 Loss: nan\n",
      "Iteration: 4167 Loss: nan\n",
      "Iteration: 4168 Loss: nan\n",
      "Iteration: 4169 Loss: nan\n",
      "Iteration: 4170 Loss: nan\n",
      "Iteration: 4171 Loss: nan\n",
      "Iteration: 4172 Loss: nan\n",
      "Iteration: 4173 Loss: nan\n",
      "Iteration: 4174 Loss: nan\n",
      "Iteration: 4175 Loss: nan\n",
      "Iteration: 4176 Loss: nan\n",
      "Iteration: 4177 Loss: nan\n",
      "Iteration: 4178 Loss: nan\n",
      "Iteration: 4179 Loss: nan\n",
      "Iteration: 4180 Loss: nan\n",
      "Iteration: 4181 Loss: nan\n",
      "Iteration: 4182 Loss: nan\n",
      "Iteration: 4183 Loss: nan\n",
      "Iteration: 4184 Loss: nan\n",
      "Iteration: 4185 Loss: nan\n",
      "Iteration: 4186 Loss: nan\n",
      "Iteration: 4187 Loss: nan\n",
      "Iteration: 4188 Loss: nan\n",
      "Iteration: 4189 Loss: nan\n",
      "Iteration: 4190 Loss: nan\n",
      "Iteration: 4191 Loss: nan\n",
      "Iteration: 4192 Loss: nan\n",
      "Iteration: 4193 Loss: nan\n",
      "Iteration: 4194 Loss: nan\n",
      "Iteration: 4195 Loss: nan\n",
      "Iteration: 4196 Loss: nan\n",
      "Iteration: 4197 Loss: nan\n",
      "Iteration: 4198 Loss: nan\n",
      "Iteration: 4199 Loss: nan\n",
      "Iteration: 4200 Loss: nan\n",
      "Iteration: 4201 Loss: nan\n",
      "Iteration: 4202 Loss: nan\n",
      "Iteration: 4203 Loss: nan\n",
      "Iteration: 4204 Loss: nan\n",
      "Iteration: 4205 Loss: nan\n",
      "Iteration: 4206 Loss: nan\n",
      "Iteration: 4207 Loss: nan\n",
      "Iteration: 4208 Loss: nan\n",
      "Iteration: 4209 Loss: nan\n",
      "Iteration: 4210 Loss: nan\n",
      "Iteration: 4211 Loss: nan\n",
      "Iteration: 4212 Loss: nan\n",
      "Iteration: 4213 Loss: nan\n",
      "Iteration: 4214 Loss: nan\n",
      "Iteration: 4215 Loss: nan\n",
      "Iteration: 4216 Loss: nan\n",
      "Iteration: 4217 Loss: nan\n",
      "Iteration: 4218 Loss: nan\n",
      "Iteration: 4219 Loss: nan\n",
      "Iteration: 4220 Loss: nan\n",
      "Iteration: 4221 Loss: nan\n",
      "Iteration: 4222 Loss: nan\n",
      "Iteration: 4223 Loss: nan\n",
      "Iteration: 4224 Loss: nan\n",
      "Iteration: 4225 Loss: nan\n",
      "Iteration: 4226 Loss: nan\n",
      "Iteration: 4227 Loss: nan\n",
      "Iteration: 4228 Loss: nan\n",
      "Iteration: 4229 Loss: nan\n",
      "Iteration: 4230 Loss: nan\n",
      "Iteration: 4231 Loss: nan\n",
      "Iteration: 4232 Loss: nan\n",
      "Iteration: 4233 Loss: nan\n",
      "Iteration: 4234 Loss: nan\n",
      "Iteration: 4235 Loss: nan\n",
      "Iteration: 4236 Loss: nan\n",
      "Iteration: 4237 Loss: nan\n",
      "Iteration: 4238 Loss: nan\n",
      "Iteration: 4239 Loss: nan\n",
      "Iteration: 4240 Loss: nan\n",
      "Iteration: 4241 Loss: nan\n",
      "Iteration: 4242 Loss: nan\n",
      "Iteration: 4243 Loss: nan\n",
      "Iteration: 4244 Loss: nan\n",
      "Iteration: 4245 Loss: nan\n",
      "Iteration: 4246 Loss: nan\n",
      "Iteration: 4247 Loss: nan\n",
      "Iteration: 4248 Loss: nan\n",
      "Iteration: 4249 Loss: nan\n",
      "Iteration: 4250 Loss: nan\n",
      "Iteration: 4251 Loss: nan\n",
      "Iteration: 4252 Loss: nan\n",
      "Iteration: 4253 Loss: nan\n",
      "Iteration: 4254 Loss: nan\n",
      "Iteration: 4255 Loss: nan\n",
      "Iteration: 4256 Loss: nan\n",
      "Iteration: 4257 Loss: nan\n",
      "Iteration: 4258 Loss: nan\n",
      "Iteration: 4259 Loss: nan\n",
      "Iteration: 4260 Loss: nan\n",
      "Iteration: 4261 Loss: nan\n",
      "Iteration: 4262 Loss: nan\n",
      "Iteration: 4263 Loss: nan\n",
      "Iteration: 4264 Loss: nan\n",
      "Iteration: 4265 Loss: nan\n",
      "Iteration: 4266 Loss: nan\n",
      "Iteration: 4267 Loss: nan\n",
      "Iteration: 4268 Loss: nan\n",
      "Iteration: 4269 Loss: nan\n",
      "Iteration: 4270 Loss: nan\n",
      "Iteration: 4271 Loss: nan\n",
      "Iteration: 4272 Loss: nan\n",
      "Iteration: 4273 Loss: nan\n",
      "Iteration: 4274 Loss: nan\n",
      "Iteration: 4275 Loss: nan\n",
      "Iteration: 4276 Loss: nan\n",
      "Iteration: 4277 Loss: nan\n",
      "Iteration: 4278 Loss: nan\n",
      "Iteration: 4279 Loss: nan\n",
      "Iteration: 4280 Loss: nan\n",
      "Iteration: 4281 Loss: nan\n",
      "Iteration: 4282 Loss: nan\n",
      "Iteration: 4283 Loss: nan\n",
      "Iteration: 4284 Loss: nan\n",
      "Iteration: 4285 Loss: nan\n",
      "Iteration: 4286 Loss: nan\n",
      "Iteration: 4287 Loss: nan\n",
      "Iteration: 4288 Loss: nan\n",
      "Iteration: 4289 Loss: nan\n",
      "Iteration: 4290 Loss: nan\n",
      "Iteration: 4291 Loss: nan\n",
      "Iteration: 4292 Loss: nan\n",
      "Iteration: 4293 Loss: nan\n",
      "Iteration: 4294 Loss: nan\n",
      "Iteration: 4295 Loss: nan\n",
      "Iteration: 4296 Loss: nan\n",
      "Iteration: 4297 Loss: nan\n",
      "Iteration: 4298 Loss: nan\n",
      "Iteration: 4299 Loss: nan\n",
      "Iteration: 4300 Loss: nan\n",
      "Iteration: 4301 Loss: nan\n",
      "Iteration: 4302 Loss: nan\n",
      "Iteration: 4303 Loss: nan\n",
      "Iteration: 4304 Loss: nan\n",
      "Iteration: 4305 Loss: nan\n",
      "Iteration: 4306 Loss: nan\n",
      "Iteration: 4307 Loss: nan\n",
      "Iteration: 4308 Loss: nan\n",
      "Iteration: 4309 Loss: nan\n",
      "Iteration: 4310 Loss: nan\n",
      "Iteration: 4311 Loss: nan\n",
      "Iteration: 4312 Loss: nan\n",
      "Iteration: 4313 Loss: nan\n",
      "Iteration: 4314 Loss: nan\n",
      "Iteration: 4315 Loss: nan\n",
      "Iteration: 4316 Loss: nan\n",
      "Iteration: 4317 Loss: nan\n",
      "Iteration: 4318 Loss: nan\n",
      "Iteration: 4319 Loss: nan\n",
      "Iteration: 4320 Loss: nan\n",
      "Iteration: 4321 Loss: nan\n",
      "Iteration: 4322 Loss: nan\n",
      "Iteration: 4323 Loss: nan\n",
      "Iteration: 4324 Loss: nan\n",
      "Iteration: 4325 Loss: nan\n",
      "Iteration: 4326 Loss: nan\n",
      "Iteration: 4327 Loss: nan\n",
      "Iteration: 4328 Loss: nan\n",
      "Iteration: 4329 Loss: nan\n",
      "Iteration: 4330 Loss: nan\n",
      "Iteration: 4331 Loss: nan\n",
      "Iteration: 4332 Loss: nan\n",
      "Iteration: 4333 Loss: nan\n",
      "Iteration: 4334 Loss: nan\n",
      "Iteration: 4335 Loss: nan\n",
      "Iteration: 4336 Loss: nan\n",
      "Iteration: 4337 Loss: nan\n",
      "Iteration: 4338 Loss: nan\n",
      "Iteration: 4339 Loss: nan\n",
      "Iteration: 4340 Loss: nan\n",
      "Iteration: 4341 Loss: nan\n",
      "Iteration: 4342 Loss: nan\n",
      "Iteration: 4343 Loss: nan\n",
      "Iteration: 4344 Loss: nan\n",
      "Iteration: 4345 Loss: nan\n",
      "Iteration: 4346 Loss: nan\n",
      "Iteration: 4347 Loss: nan\n",
      "Iteration: 4348 Loss: nan\n",
      "Iteration: 4349 Loss: nan\n",
      "Iteration: 4350 Loss: nan\n",
      "Iteration: 4351 Loss: nan\n",
      "Iteration: 4352 Loss: nan\n",
      "Iteration: 4353 Loss: nan\n",
      "Iteration: 4354 Loss: nan\n",
      "Iteration: 4355 Loss: nan\n",
      "Iteration: 4356 Loss: nan\n",
      "Iteration: 4357 Loss: nan\n",
      "Iteration: 4358 Loss: nan\n",
      "Iteration: 4359 Loss: nan\n",
      "Iteration: 4360 Loss: nan\n",
      "Iteration: 4361 Loss: nan\n",
      "Iteration: 4362 Loss: nan\n",
      "Iteration: 4363 Loss: nan\n",
      "Iteration: 4364 Loss: nan\n",
      "Iteration: 4365 Loss: nan\n",
      "Iteration: 4366 Loss: nan\n",
      "Iteration: 4367 Loss: nan\n",
      "Iteration: 4368 Loss: nan\n",
      "Iteration: 4369 Loss: nan\n",
      "Iteration: 4370 Loss: nan\n",
      "Iteration: 4371 Loss: nan\n",
      "Iteration: 4372 Loss: nan\n",
      "Iteration: 4373 Loss: nan\n",
      "Iteration: 4374 Loss: nan\n",
      "Iteration: 4375 Loss: nan\n",
      "Iteration: 4376 Loss: nan\n",
      "Iteration: 4377 Loss: nan\n",
      "Iteration: 4378 Loss: nan\n",
      "Iteration: 4379 Loss: nan\n",
      "Iteration: 4380 Loss: nan\n",
      "Iteration: 4381 Loss: nan\n",
      "Iteration: 4382 Loss: nan\n",
      "Iteration: 4383 Loss: nan\n",
      "Iteration: 4384 Loss: nan\n",
      "Iteration: 4385 Loss: nan\n",
      "Iteration: 4386 Loss: nan\n",
      "Iteration: 4387 Loss: nan\n",
      "Iteration: 4388 Loss: nan\n",
      "Iteration: 4389 Loss: nan\n",
      "Iteration: 4390 Loss: nan\n",
      "Iteration: 4391 Loss: nan\n",
      "Iteration: 4392 Loss: nan\n",
      "Iteration: 4393 Loss: nan\n",
      "Iteration: 4394 Loss: nan\n",
      "Iteration: 4395 Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4396 Loss: nan\n",
      "Iteration: 4397 Loss: nan\n",
      "Iteration: 4398 Loss: nan\n",
      "Iteration: 4399 Loss: nan\n",
      "Iteration: 4400 Loss: nan\n",
      "Iteration: 4401 Loss: nan\n",
      "Iteration: 4402 Loss: nan\n",
      "Iteration: 4403 Loss: nan\n",
      "Iteration: 4404 Loss: nan\n",
      "Iteration: 4405 Loss: nan\n",
      "Iteration: 4406 Loss: nan\n",
      "Iteration: 4407 Loss: nan\n",
      "Iteration: 4408 Loss: nan\n",
      "Iteration: 4409 Loss: nan\n",
      "Iteration: 4410 Loss: nan\n",
      "Iteration: 4411 Loss: nan\n",
      "Iteration: 4412 Loss: nan\n",
      "Iteration: 4413 Loss: nan\n",
      "Iteration: 4414 Loss: nan\n",
      "Iteration: 4415 Loss: nan\n",
      "Iteration: 4416 Loss: nan\n",
      "Iteration: 4417 Loss: nan\n",
      "Iteration: 4418 Loss: nan\n",
      "Iteration: 4419 Loss: nan\n",
      "Iteration: 4420 Loss: nan\n",
      "Iteration: 4421 Loss: nan\n",
      "Iteration: 4422 Loss: nan\n",
      "Iteration: 4423 Loss: nan\n",
      "Iteration: 4424 Loss: nan\n",
      "Iteration: 4425 Loss: nan\n",
      "Iteration: 4426 Loss: nan\n",
      "Iteration: 4427 Loss: nan\n",
      "Iteration: 4428 Loss: nan\n",
      "Iteration: 4429 Loss: nan\n",
      "Iteration: 4430 Loss: nan\n",
      "Iteration: 4431 Loss: nan\n",
      "Iteration: 4432 Loss: nan\n",
      "Iteration: 4433 Loss: nan\n",
      "Iteration: 4434 Loss: nan\n",
      "Iteration: 4435 Loss: nan\n",
      "Iteration: 4436 Loss: nan\n",
      "Iteration: 4437 Loss: nan\n",
      "Iteration: 4438 Loss: nan\n",
      "Iteration: 4439 Loss: nan\n",
      "Iteration: 4440 Loss: nan\n",
      "Iteration: 4441 Loss: nan\n",
      "Iteration: 4442 Loss: nan\n",
      "Iteration: 4443 Loss: nan\n",
      "Iteration: 4444 Loss: nan\n",
      "Iteration: 4445 Loss: nan\n",
      "Iteration: 4446 Loss: nan\n",
      "Iteration: 4447 Loss: nan\n",
      "Iteration: 4448 Loss: nan\n",
      "Iteration: 4449 Loss: nan\n",
      "Iteration: 4450 Loss: nan\n",
      "Iteration: 4451 Loss: nan\n",
      "Iteration: 4452 Loss: nan\n",
      "Iteration: 4453 Loss: nan\n",
      "Iteration: 4454 Loss: nan\n",
      "Iteration: 4455 Loss: nan\n",
      "Iteration: 4456 Loss: nan\n",
      "Iteration: 4457 Loss: nan\n",
      "Iteration: 4458 Loss: nan\n",
      "Iteration: 4459 Loss: nan\n",
      "Iteration: 4460 Loss: nan\n",
      "Iteration: 4461 Loss: nan\n",
      "Iteration: 4462 Loss: nan\n",
      "Iteration: 4463 Loss: nan\n",
      "Iteration: 4464 Loss: nan\n",
      "Iteration: 4465 Loss: nan\n",
      "Iteration: 4466 Loss: nan\n",
      "Iteration: 4467 Loss: nan\n",
      "Iteration: 4468 Loss: nan\n",
      "Iteration: 4469 Loss: nan\n",
      "Iteration: 4470 Loss: nan\n",
      "Iteration: 4471 Loss: nan\n",
      "Iteration: 4472 Loss: nan\n",
      "Iteration: 4473 Loss: nan\n",
      "Iteration: 4474 Loss: nan\n",
      "Iteration: 4475 Loss: nan\n",
      "Iteration: 4476 Loss: nan\n",
      "Iteration: 4477 Loss: nan\n",
      "Iteration: 4478 Loss: nan\n",
      "Iteration: 4479 Loss: nan\n",
      "Iteration: 4480 Loss: nan\n",
      "Iteration: 4481 Loss: nan\n",
      "Iteration: 4482 Loss: nan\n",
      "Iteration: 4483 Loss: nan\n",
      "Iteration: 4484 Loss: nan\n",
      "Iteration: 4485 Loss: nan\n",
      "Iteration: 4486 Loss: nan\n",
      "Iteration: 4487 Loss: nan\n",
      "Iteration: 4488 Loss: nan\n",
      "Iteration: 4489 Loss: nan\n",
      "Iteration: 4490 Loss: nan\n",
      "Iteration: 4491 Loss: nan\n",
      "Iteration: 4492 Loss: nan\n",
      "Iteration: 4493 Loss: nan\n",
      "Iteration: 4494 Loss: nan\n",
      "Iteration: 4495 Loss: nan\n",
      "Iteration: 4496 Loss: nan\n",
      "Iteration: 4497 Loss: nan\n",
      "Iteration: 4498 Loss: nan\n",
      "Iteration: 4499 Loss: nan\n",
      "Iteration: 4500 Loss: nan\n",
      "Iteration: 4501 Loss: nan\n",
      "Iteration: 4502 Loss: nan\n",
      "Iteration: 4503 Loss: nan\n",
      "Iteration: 4504 Loss: nan\n",
      "Iteration: 4505 Loss: nan\n",
      "Iteration: 4506 Loss: nan\n",
      "Iteration: 4507 Loss: nan\n",
      "Iteration: 4508 Loss: nan\n",
      "Iteration: 4509 Loss: nan\n",
      "Iteration: 4510 Loss: nan\n",
      "Iteration: 4511 Loss: nan\n",
      "Iteration: 4512 Loss: nan\n",
      "Iteration: 4513 Loss: nan\n",
      "Iteration: 4514 Loss: nan\n",
      "Iteration: 4515 Loss: nan\n",
      "Iteration: 4516 Loss: nan\n",
      "Iteration: 4517 Loss: nan\n",
      "Iteration: 4518 Loss: nan\n",
      "Iteration: 4519 Loss: nan\n",
      "Iteration: 4520 Loss: nan\n",
      "Iteration: 4521 Loss: nan\n",
      "Iteration: 4522 Loss: nan\n",
      "Iteration: 4523 Loss: nan\n",
      "Iteration: 4524 Loss: nan\n",
      "Iteration: 4525 Loss: nan\n",
      "Iteration: 4526 Loss: nan\n",
      "Iteration: 4527 Loss: nan\n",
      "Iteration: 4528 Loss: nan\n",
      "Iteration: 4529 Loss: nan\n",
      "Iteration: 4530 Loss: nan\n",
      "Iteration: 4531 Loss: nan\n",
      "Iteration: 4532 Loss: nan\n",
      "Iteration: 4533 Loss: nan\n",
      "Iteration: 4534 Loss: nan\n",
      "Iteration: 4535 Loss: nan\n",
      "Iteration: 4536 Loss: nan\n",
      "Iteration: 4537 Loss: nan\n",
      "Iteration: 4538 Loss: nan\n",
      "Iteration: 4539 Loss: nan\n",
      "Iteration: 4540 Loss: nan\n",
      "Iteration: 4541 Loss: nan\n",
      "Iteration: 4542 Loss: nan\n",
      "Iteration: 4543 Loss: nan\n",
      "Iteration: 4544 Loss: nan\n",
      "Iteration: 4545 Loss: nan\n",
      "Iteration: 4546 Loss: nan\n",
      "Iteration: 4547 Loss: nan\n",
      "Iteration: 4548 Loss: nan\n",
      "Iteration: 4549 Loss: nan\n",
      "Iteration: 4550 Loss: nan\n",
      "Iteration: 4551 Loss: nan\n",
      "Iteration: 4552 Loss: nan\n",
      "Iteration: 4553 Loss: nan\n",
      "Iteration: 4554 Loss: nan\n",
      "Iteration: 4555 Loss: nan\n",
      "Iteration: 4556 Loss: nan\n",
      "Iteration: 4557 Loss: nan\n",
      "Iteration: 4558 Loss: nan\n",
      "Iteration: 4559 Loss: nan\n",
      "Iteration: 4560 Loss: nan\n",
      "Iteration: 4561 Loss: nan\n",
      "Iteration: 4562 Loss: nan\n",
      "Iteration: 4563 Loss: nan\n",
      "Iteration: 4564 Loss: nan\n",
      "Iteration: 4565 Loss: nan\n",
      "Iteration: 4566 Loss: nan\n",
      "Iteration: 4567 Loss: nan\n",
      "Iteration: 4568 Loss: nan\n",
      "Iteration: 4569 Loss: nan\n",
      "Iteration: 4570 Loss: nan\n",
      "Iteration: 4571 Loss: nan\n",
      "Iteration: 4572 Loss: nan\n",
      "Iteration: 4573 Loss: nan\n",
      "Iteration: 4574 Loss: nan\n",
      "Iteration: 4575 Loss: nan\n",
      "Iteration: 4576 Loss: nan\n",
      "Iteration: 4577 Loss: nan\n",
      "Iteration: 4578 Loss: nan\n",
      "Iteration: 4579 Loss: nan\n",
      "Iteration: 4580 Loss: nan\n",
      "Iteration: 4581 Loss: nan\n",
      "Iteration: 4582 Loss: nan\n",
      "Iteration: 4583 Loss: nan\n",
      "Iteration: 4584 Loss: nan\n",
      "Iteration: 4585 Loss: nan\n",
      "Iteration: 4586 Loss: nan\n",
      "Iteration: 4587 Loss: nan\n",
      "Iteration: 4588 Loss: nan\n",
      "Iteration: 4589 Loss: nan\n",
      "Iteration: 4590 Loss: nan\n",
      "Iteration: 4591 Loss: nan\n",
      "Iteration: 4592 Loss: nan\n",
      "Iteration: 4593 Loss: nan\n",
      "Iteration: 4594 Loss: nan\n",
      "Iteration: 4595 Loss: nan\n",
      "Iteration: 4596 Loss: nan\n",
      "Iteration: 4597 Loss: nan\n",
      "Iteration: 4598 Loss: nan\n",
      "Iteration: 4599 Loss: nan\n",
      "Iteration: 4600 Loss: nan\n",
      "Iteration: 4601 Loss: nan\n",
      "Iteration: 4602 Loss: nan\n",
      "Iteration: 4603 Loss: nan\n",
      "Iteration: 4604 Loss: nan\n",
      "Iteration: 4605 Loss: nan\n",
      "Iteration: 4606 Loss: nan\n",
      "Iteration: 4607 Loss: nan\n",
      "Iteration: 4608 Loss: nan\n",
      "Iteration: 4609 Loss: nan\n",
      "Iteration: 4610 Loss: nan\n",
      "Iteration: 4611 Loss: nan\n",
      "Iteration: 4612 Loss: nan\n",
      "Iteration: 4613 Loss: nan\n",
      "Iteration: 4614 Loss: nan\n",
      "Iteration: 4615 Loss: nan\n",
      "Iteration: 4616 Loss: nan\n",
      "Iteration: 4617 Loss: nan\n",
      "Iteration: 4618 Loss: nan\n",
      "Iteration: 4619 Loss: nan\n",
      "Iteration: 4620 Loss: nan\n",
      "Iteration: 4621 Loss: nan\n",
      "Iteration: 4622 Loss: nan\n",
      "Iteration: 4623 Loss: nan\n",
      "Iteration: 4624 Loss: nan\n",
      "Iteration: 4625 Loss: nan\n",
      "Iteration: 4626 Loss: nan\n",
      "Iteration: 4627 Loss: nan\n",
      "Iteration: 4628 Loss: nan\n",
      "Iteration: 4629 Loss: nan\n",
      "Iteration: 4630 Loss: nan\n",
      "Iteration: 4631 Loss: nan\n",
      "Iteration: 4632 Loss: nan\n",
      "Iteration: 4633 Loss: nan\n",
      "Iteration: 4634 Loss: nan\n",
      "Iteration: 4635 Loss: nan\n",
      "Iteration: 4636 Loss: nan\n",
      "Iteration: 4637 Loss: nan\n",
      "Iteration: 4638 Loss: nan\n",
      "Iteration: 4639 Loss: nan\n",
      "Iteration: 4640 Loss: nan\n",
      "Iteration: 4641 Loss: nan\n",
      "Iteration: 4642 Loss: nan\n",
      "Iteration: 4643 Loss: nan\n",
      "Iteration: 4644 Loss: nan\n",
      "Iteration: 4645 Loss: nan\n",
      "Iteration: 4646 Loss: nan\n",
      "Iteration: 4647 Loss: nan\n",
      "Iteration: 4648 Loss: nan\n",
      "Iteration: 4649 Loss: nan\n",
      "Iteration: 4650 Loss: nan\n",
      "Iteration: 4651 Loss: nan\n",
      "Iteration: 4652 Loss: nan\n",
      "Iteration: 4653 Loss: nan\n",
      "Iteration: 4654 Loss: nan\n",
      "Iteration: 4655 Loss: nan\n",
      "Iteration: 4656 Loss: nan\n",
      "Iteration: 4657 Loss: nan\n",
      "Iteration: 4658 Loss: nan\n",
      "Iteration: 4659 Loss: nan\n",
      "Iteration: 4660 Loss: nan\n",
      "Iteration: 4661 Loss: nan\n",
      "Iteration: 4662 Loss: nan\n",
      "Iteration: 4663 Loss: nan\n",
      "Iteration: 4664 Loss: nan\n",
      "Iteration: 4665 Loss: nan\n",
      "Iteration: 4666 Loss: nan\n",
      "Iteration: 4667 Loss: nan\n",
      "Iteration: 4668 Loss: nan\n",
      "Iteration: 4669 Loss: nan\n",
      "Iteration: 4670 Loss: nan\n",
      "Iteration: 4671 Loss: nan\n",
      "Iteration: 4672 Loss: nan\n",
      "Iteration: 4673 Loss: nan\n",
      "Iteration: 4674 Loss: nan\n",
      "Iteration: 4675 Loss: nan\n",
      "Iteration: 4676 Loss: nan\n",
      "Iteration: 4677 Loss: nan\n",
      "Iteration: 4678 Loss: nan\n",
      "Iteration: 4679 Loss: nan\n",
      "Iteration: 4680 Loss: nan\n",
      "Iteration: 4681 Loss: nan\n",
      "Iteration: 4682 Loss: nan\n",
      "Iteration: 4683 Loss: nan\n",
      "Iteration: 4684 Loss: nan\n",
      "Iteration: 4685 Loss: nan\n",
      "Iteration: 4686 Loss: nan\n",
      "Iteration: 4687 Loss: nan\n",
      "Iteration: 4688 Loss: nan\n",
      "Iteration: 4689 Loss: nan\n",
      "Iteration: 4690 Loss: nan\n",
      "Iteration: 4691 Loss: nan\n",
      "Iteration: 4692 Loss: nan\n",
      "Iteration: 4693 Loss: nan\n",
      "Iteration: 4694 Loss: nan\n",
      "Iteration: 4695 Loss: nan\n",
      "Iteration: 4696 Loss: nan\n",
      "Iteration: 4697 Loss: nan\n",
      "Iteration: 4698 Loss: nan\n",
      "Iteration: 4699 Loss: nan\n",
      "Iteration: 4700 Loss: nan\n",
      "Iteration: 4701 Loss: nan\n",
      "Iteration: 4702 Loss: nan\n",
      "Iteration: 4703 Loss: nan\n",
      "Iteration: 4704 Loss: nan\n",
      "Iteration: 4705 Loss: nan\n",
      "Iteration: 4706 Loss: nan\n",
      "Iteration: 4707 Loss: nan\n",
      "Iteration: 4708 Loss: nan\n",
      "Iteration: 4709 Loss: nan\n",
      "Iteration: 4710 Loss: nan\n",
      "Iteration: 4711 Loss: nan\n",
      "Iteration: 4712 Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4713 Loss: nan\n",
      "Iteration: 4714 Loss: nan\n",
      "Iteration: 4715 Loss: nan\n",
      "Iteration: 4716 Loss: nan\n",
      "Iteration: 4717 Loss: nan\n",
      "Iteration: 4718 Loss: nan\n",
      "Iteration: 4719 Loss: nan\n",
      "Iteration: 4720 Loss: nan\n",
      "Iteration: 4721 Loss: nan\n",
      "Iteration: 4722 Loss: nan\n",
      "Iteration: 4723 Loss: nan\n",
      "Iteration: 4724 Loss: nan\n",
      "Iteration: 4725 Loss: nan\n",
      "Iteration: 4726 Loss: nan\n",
      "Iteration: 4727 Loss: nan\n",
      "Iteration: 4728 Loss: nan\n",
      "Iteration: 4729 Loss: nan\n",
      "Iteration: 4730 Loss: nan\n",
      "Iteration: 4731 Loss: nan\n",
      "Iteration: 4732 Loss: nan\n",
      "Iteration: 4733 Loss: nan\n",
      "Iteration: 4734 Loss: nan\n",
      "Iteration: 4735 Loss: nan\n",
      "Iteration: 4736 Loss: nan\n",
      "Iteration: 4737 Loss: nan\n",
      "Iteration: 4738 Loss: nan\n",
      "Iteration: 4739 Loss: nan\n",
      "Iteration: 4740 Loss: nan\n",
      "Iteration: 4741 Loss: nan\n",
      "Iteration: 4742 Loss: nan\n",
      "Iteration: 4743 Loss: nan\n",
      "Iteration: 4744 Loss: nan\n",
      "Iteration: 4745 Loss: nan\n",
      "Iteration: 4746 Loss: nan\n",
      "Iteration: 4747 Loss: nan\n",
      "Iteration: 4748 Loss: nan\n",
      "Iteration: 4749 Loss: nan\n",
      "Iteration: 4750 Loss: nan\n",
      "Iteration: 4751 Loss: nan\n",
      "Iteration: 4752 Loss: nan\n",
      "Iteration: 4753 Loss: nan\n",
      "Iteration: 4754 Loss: nan\n",
      "Iteration: 4755 Loss: nan\n",
      "Iteration: 4756 Loss: nan\n",
      "Iteration: 4757 Loss: nan\n",
      "Iteration: 4758 Loss: nan\n",
      "Iteration: 4759 Loss: nan\n",
      "Iteration: 4760 Loss: nan\n",
      "Iteration: 4761 Loss: nan\n",
      "Iteration: 4762 Loss: nan\n",
      "Iteration: 4763 Loss: nan\n",
      "Iteration: 4764 Loss: nan\n",
      "Iteration: 4765 Loss: nan\n",
      "Iteration: 4766 Loss: nan\n",
      "Iteration: 4767 Loss: nan\n",
      "Iteration: 4768 Loss: nan\n",
      "Iteration: 4769 Loss: nan\n",
      "Iteration: 4770 Loss: nan\n",
      "Iteration: 4771 Loss: nan\n",
      "Iteration: 4772 Loss: nan\n",
      "Iteration: 4773 Loss: nan\n",
      "Iteration: 4774 Loss: nan\n",
      "Iteration: 4775 Loss: nan\n",
      "Iteration: 4776 Loss: nan\n",
      "Iteration: 4777 Loss: nan\n",
      "Iteration: 4778 Loss: nan\n",
      "Iteration: 4779 Loss: nan\n",
      "Iteration: 4780 Loss: nan\n",
      "Iteration: 4781 Loss: nan\n",
      "Iteration: 4782 Loss: nan\n",
      "Iteration: 4783 Loss: nan\n",
      "Iteration: 4784 Loss: nan\n",
      "Iteration: 4785 Loss: nan\n",
      "Iteration: 4786 Loss: nan\n",
      "Iteration: 4787 Loss: nan\n",
      "Iteration: 4788 Loss: nan\n",
      "Iteration: 4789 Loss: nan\n",
      "Iteration: 4790 Loss: nan\n",
      "Iteration: 4791 Loss: nan\n",
      "Iteration: 4792 Loss: nan\n",
      "Iteration: 4793 Loss: nan\n",
      "Iteration: 4794 Loss: nan\n",
      "Iteration: 4795 Loss: nan\n",
      "Iteration: 4796 Loss: nan\n",
      "Iteration: 4797 Loss: nan\n",
      "Iteration: 4798 Loss: nan\n",
      "Iteration: 4799 Loss: nan\n",
      "Iteration: 4800 Loss: nan\n",
      "Iteration: 4801 Loss: nan\n",
      "Iteration: 4802 Loss: nan\n",
      "Iteration: 4803 Loss: nan\n",
      "Iteration: 4804 Loss: nan\n",
      "Iteration: 4805 Loss: nan\n",
      "Iteration: 4806 Loss: nan\n",
      "Iteration: 4807 Loss: nan\n",
      "Iteration: 4808 Loss: nan\n",
      "Iteration: 4809 Loss: nan\n",
      "Iteration: 4810 Loss: nan\n",
      "Iteration: 4811 Loss: nan\n",
      "Iteration: 4812 Loss: nan\n",
      "Iteration: 4813 Loss: nan\n",
      "Iteration: 4814 Loss: nan\n",
      "Iteration: 4815 Loss: nan\n",
      "Iteration: 4816 Loss: nan\n",
      "Iteration: 4817 Loss: nan\n",
      "Iteration: 4818 Loss: nan\n",
      "Iteration: 4819 Loss: nan\n",
      "Iteration: 4820 Loss: nan\n",
      "Iteration: 4821 Loss: nan\n",
      "Iteration: 4822 Loss: nan\n",
      "Iteration: 4823 Loss: nan\n",
      "Iteration: 4824 Loss: nan\n",
      "Iteration: 4825 Loss: nan\n",
      "Iteration: 4826 Loss: nan\n",
      "Iteration: 4827 Loss: nan\n",
      "Iteration: 4828 Loss: nan\n",
      "Iteration: 4829 Loss: nan\n",
      "Iteration: 4830 Loss: nan\n",
      "Iteration: 4831 Loss: nan\n",
      "Iteration: 4832 Loss: nan\n",
      "Iteration: 4833 Loss: nan\n",
      "Iteration: 4834 Loss: nan\n",
      "Iteration: 4835 Loss: nan\n",
      "Iteration: 4836 Loss: nan\n",
      "Iteration: 4837 Loss: nan\n",
      "Iteration: 4838 Loss: nan\n",
      "Iteration: 4839 Loss: nan\n",
      "Iteration: 4840 Loss: nan\n",
      "Iteration: 4841 Loss: nan\n",
      "Iteration: 4842 Loss: nan\n",
      "Iteration: 4843 Loss: nan\n",
      "Iteration: 4844 Loss: nan\n",
      "Iteration: 4845 Loss: nan\n",
      "Iteration: 4846 Loss: nan\n",
      "Iteration: 4847 Loss: nan\n",
      "Iteration: 4848 Loss: nan\n",
      "Iteration: 4849 Loss: nan\n",
      "Iteration: 4850 Loss: nan\n",
      "Iteration: 4851 Loss: nan\n",
      "Iteration: 4852 Loss: nan\n",
      "Iteration: 4853 Loss: nan\n",
      "Iteration: 4854 Loss: nan\n",
      "Iteration: 4855 Loss: nan\n",
      "Iteration: 4856 Loss: nan\n",
      "Iteration: 4857 Loss: nan\n",
      "Iteration: 4858 Loss: nan\n",
      "Iteration: 4859 Loss: nan\n",
      "Iteration: 4860 Loss: nan\n",
      "Iteration: 4861 Loss: nan\n",
      "Iteration: 4862 Loss: nan\n",
      "Iteration: 4863 Loss: nan\n",
      "Iteration: 4864 Loss: nan\n",
      "Iteration: 4865 Loss: nan\n",
      "Iteration: 4866 Loss: nan\n",
      "Iteration: 4867 Loss: nan\n",
      "Iteration: 4868 Loss: nan\n",
      "Iteration: 4869 Loss: nan\n",
      "Iteration: 4870 Loss: nan\n",
      "Iteration: 4871 Loss: nan\n",
      "Iteration: 4872 Loss: nan\n",
      "Iteration: 4873 Loss: nan\n",
      "Iteration: 4874 Loss: nan\n",
      "Iteration: 4875 Loss: nan\n",
      "Iteration: 4876 Loss: nan\n",
      "Iteration: 4877 Loss: nan\n",
      "Iteration: 4878 Loss: nan\n",
      "Iteration: 4879 Loss: nan\n",
      "Iteration: 4880 Loss: nan\n",
      "Iteration: 4881 Loss: nan\n",
      "Iteration: 4882 Loss: nan\n",
      "Iteration: 4883 Loss: nan\n",
      "Iteration: 4884 Loss: nan\n",
      "Iteration: 4885 Loss: nan\n",
      "Iteration: 4886 Loss: nan\n",
      "Iteration: 4887 Loss: nan\n",
      "Iteration: 4888 Loss: nan\n",
      "Iteration: 4889 Loss: nan\n",
      "Iteration: 4890 Loss: nan\n",
      "Iteration: 4891 Loss: nan\n",
      "Iteration: 4892 Loss: nan\n",
      "Iteration: 4893 Loss: nan\n",
      "Iteration: 4894 Loss: nan\n",
      "Iteration: 4895 Loss: nan\n",
      "Iteration: 4896 Loss: nan\n",
      "Iteration: 4897 Loss: nan\n",
      "Iteration: 4898 Loss: nan\n",
      "Iteration: 4899 Loss: nan\n",
      "Iteration: 4900 Loss: nan\n",
      "Iteration: 4901 Loss: nan\n",
      "Iteration: 4902 Loss: nan\n",
      "Iteration: 4903 Loss: nan\n",
      "Iteration: 4904 Loss: nan\n",
      "Iteration: 4905 Loss: nan\n",
      "Iteration: 4906 Loss: nan\n",
      "Iteration: 4907 Loss: nan\n",
      "Iteration: 4908 Loss: nan\n",
      "Iteration: 4909 Loss: nan\n",
      "Iteration: 4910 Loss: nan\n",
      "Iteration: 4911 Loss: nan\n",
      "Iteration: 4912 Loss: nan\n",
      "Iteration: 4913 Loss: nan\n",
      "Iteration: 4914 Loss: nan\n",
      "Iteration: 4915 Loss: nan\n",
      "Iteration: 4916 Loss: nan\n",
      "Iteration: 4917 Loss: nan\n",
      "Iteration: 4918 Loss: nan\n",
      "Iteration: 4919 Loss: nan\n",
      "Iteration: 4920 Loss: nan\n",
      "Iteration: 4921 Loss: nan\n",
      "Iteration: 4922 Loss: nan\n",
      "Iteration: 4923 Loss: nan\n",
      "Iteration: 4924 Loss: nan\n",
      "Iteration: 4925 Loss: nan\n",
      "Iteration: 4926 Loss: nan\n",
      "Iteration: 4927 Loss: nan\n",
      "Iteration: 4928 Loss: nan\n",
      "Iteration: 4929 Loss: nan\n",
      "Iteration: 4930 Loss: nan\n",
      "Iteration: 4931 Loss: nan\n",
      "Iteration: 4932 Loss: nan\n",
      "Iteration: 4933 Loss: nan\n",
      "Iteration: 4934 Loss: nan\n",
      "Iteration: 4935 Loss: nan\n",
      "Iteration: 4936 Loss: nan\n",
      "Iteration: 4937 Loss: nan\n",
      "Iteration: 4938 Loss: nan\n",
      "Iteration: 4939 Loss: nan\n",
      "Iteration: 4940 Loss: nan\n",
      "Iteration: 4941 Loss: nan\n",
      "Iteration: 4942 Loss: nan\n",
      "Iteration: 4943 Loss: nan\n",
      "Iteration: 4944 Loss: nan\n",
      "Iteration: 4945 Loss: nan\n",
      "Iteration: 4946 Loss: nan\n",
      "Iteration: 4947 Loss: nan\n",
      "Iteration: 4948 Loss: nan\n",
      "Iteration: 4949 Loss: nan\n",
      "Iteration: 4950 Loss: nan\n",
      "Iteration: 4951 Loss: nan\n",
      "Iteration: 4952 Loss: nan\n",
      "Iteration: 4953 Loss: nan\n",
      "Iteration: 4954 Loss: nan\n",
      "Iteration: 4955 Loss: nan\n",
      "Iteration: 4956 Loss: nan\n",
      "Iteration: 4957 Loss: nan\n",
      "Iteration: 4958 Loss: nan\n",
      "Iteration: 4959 Loss: nan\n",
      "Iteration: 4960 Loss: nan\n",
      "Iteration: 4961 Loss: nan\n",
      "Iteration: 4962 Loss: nan\n",
      "Iteration: 4963 Loss: nan\n",
      "Iteration: 4964 Loss: nan\n",
      "Iteration: 4965 Loss: nan\n",
      "Iteration: 4966 Loss: nan\n",
      "Iteration: 4967 Loss: nan\n",
      "Iteration: 4968 Loss: nan\n",
      "Iteration: 4969 Loss: nan\n",
      "Iteration: 4970 Loss: nan\n",
      "Iteration: 4971 Loss: nan\n",
      "Iteration: 4972 Loss: nan\n",
      "Iteration: 4973 Loss: nan\n",
      "Iteration: 4974 Loss: nan\n",
      "Iteration: 4975 Loss: nan\n",
      "Iteration: 4976 Loss: nan\n",
      "Iteration: 4977 Loss: nan\n",
      "Iteration: 4978 Loss: nan\n",
      "Iteration: 4979 Loss: nan\n",
      "Iteration: 4980 Loss: nan\n",
      "Iteration: 4981 Loss: nan\n",
      "Iteration: 4982 Loss: nan\n",
      "Iteration: 4983 Loss: nan\n",
      "Iteration: 4984 Loss: nan\n",
      "Iteration: 4985 Loss: nan\n",
      "Iteration: 4986 Loss: nan\n",
      "Iteration: 4987 Loss: nan\n",
      "Iteration: 4988 Loss: nan\n",
      "Iteration: 4989 Loss: nan\n",
      "Iteration: 4990 Loss: nan\n",
      "Iteration: 4991 Loss: nan\n",
      "Iteration: 4992 Loss: nan\n",
      "Iteration: 4993 Loss: nan\n",
      "Iteration: 4994 Loss: nan\n",
      "Iteration: 4995 Loss: nan\n",
      "Iteration: 4996 Loss: nan\n",
      "Iteration: 4997 Loss: nan\n",
      "Iteration: 4998 Loss: nan\n",
      "Iteration: 4999 Loss: nan\n",
      "Iteration: 5000 Loss: nan\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEDCAYAAADOc0QpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f56a420c310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "var_model = MLP_20(Vocab_size,20)\n",
    "optimizer = torch.optim.SGD(var_model.parameters(),lr=0.2)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "model, losses = train_5000(var_model,train_loader,optimizer,loss_func)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 7]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
